# ============================= HIGH PERFORMANCE CONFIG ============================= #
# Swin Transformer + Hard Augmentation + Mixup + WandB
# Based on 0.934 F1 score notebook

# ----------------------------- 기본 메타 설정 ----------------------------- #
project:
  run_name: v094-swin-highperf           # 실험 이름/ID (고성능 버전)
  seed: 42                               # 랜덤 시드 (재현성 확보)
  date_format: "%Y%m%d"                  # 날짜 포맷 (예: 20250905)
  time_format: "%H%M"                    # 시간 포맷 (예: 1530)
  num_workers: 4                         # DataLoader 워커 수
  device: "cuda"                         # 실행 디바이스
  verbose: true                          # 콘솔 로그 출력 여부
  log_prefix: "train_highperf"           # 로그 파일 접두사

# ------------------------------- 데이터 설정 ------------------------------ #
data:
  root: "../data"                        # 데이터 루트 디렉토리
  train_csv: "../data/raw/train.csv"     # 학습용 CSV 경로
  sample_csv: "../data/raw/sample_submission.csv"  # 제출용 샘플 CSV 경로
  image_dir_train: "../data/raw/train"   # 학습 이미지 디렉토리
  image_dir_test:  "../data/raw/test"    # 테스트/추론 이미지 디렉토리
  image_ext: ".jpg"                      # 이미지 확장자
  id_col: "ID"                           # ID 컬럼 이름
  target_col: "target"                   # 타깃 컬럼 이름
  num_classes: 17                        # 분류 클래스 개수
  folds: 5                               # 폴드 수 (교차 검증)
  valid_fold: all                        # 유효 폴드 (all=전 폴드 학습)
  stratify: true                         # 층화 샘플링 여부

# ------------------------------- 학습 설정 -------------------------------- #
train:
  img_size: 384                          # 입력 이미지 크기 (384x384, 고해상도)
  batch_size: 32                         # 배치 크기 (384 해상도에 맞춰 감소)
  epochs: 15                             # 학습 epoch 수 (더 긴 학습)
  lr: 0.0001                             # 학습률 (더 작은 LR로 안정적 학습)
  weight_decay: 0.01                     # weight decay (정규화 강화)
  optimizer: "adamw"                     # 옵티마이저 (AdamW 사용)
  scheduler: "cosine"                    # 학습률 스케줄러
  mixed_precision: true                  # AMP 사용 여부
  max_grad_norm: 1.0                     # gradient clipping
  label_smoothing: 0.1                   # 라벨 스무딩 (overfitting 방지)
  
  # ===== HIGH PERFORMANCE FEATURES ===== #
  use_mixup: true                        # Mixup 데이터 증강 사용
  mixup_alpha: 1.0                       # Mixup alpha 파라미터
  hard_augmentation: true                # Hard Augmentation 사용
  log_interval: 50                       # 로그 출력 간격

# ------------------------------- 모델 설정 -------------------------------- #
model:
  name: "swin_base_384"                  # Swin Transformer Base (384 해상도)
  pretrained: true                       # 사전학습 가중치 사용
  drop_rate: 0.1                         # dropout 비율 (정규화)
  drop_path_rate: 0.1                    # stochastic depth 비율
  pooling: "avg"                         # global pooling 방식

# ------------------------------- 출력/저장 설정 ---------------------------- #
output:
  log_dir: "logs/train"                  # 로그 저장 디렉토리
  exp_dir: "experiments/train"           # 실험 결과 저장 루트
  snapshots: true                        # config 스냅샷 저장 여부

# ------------------------------- WandB 설정 -------------------------------- #
wandb:
  project_name: "document-classification-team"  # WandB 프로젝트명
  entity: null                           # WandB entity (null=개인계정)
  experiment_name: "swin-highperf"       # 실험명
  tags: ["swin", "high-performance", "mixup", "hard-aug"]  # 태그

# ============================= ALTERNATIVE CONFIGS ============================= #
# ConvNext 버전을 사용하려면 아래 설정으로 변경:
# model:
#   name: "convnext_base_384"            # ConvNext Base (384 해상도)
#   
# EfficientNet 버전을 사용하려면:
# model:
#   name: "efficientnet_b3"              # EfficientNet-B3
# train:
#   img_size: 300                        # EfficientNet에 맞는 해상도
