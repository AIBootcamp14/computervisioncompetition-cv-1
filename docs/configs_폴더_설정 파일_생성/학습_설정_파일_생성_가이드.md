# 학습 설정 파일 생성 가이드

이 문서는 configs 폴더 내 학습 관련 설정 파일의 각 항목에 대한 설명과 사용 가능한 값, 추천 조합을 안내합니다.

## 주요 설정 항목 예시
- model: 사용할 모델 이름 (예: resnet50, efficientnet_b3 등)
- optimizer: 최적화 알고리즘 (예: Adam, SGD, AdamW)
- learning_rate: 학습률 (float, 예: 0.001 ~ 0.1)
- batch_size: 배치 크기 (int, 예: 8 ~ 128)
- epochs: 학습 에폭 수 (int, 예: 10 ~ 200)
- scheduler: 러닝레이트 스케줄러 (예: CosineAnnealing, StepLR 등)
- augmentation: 데이터 증강 타입 (basic, advanced, team_normal, team_hard)
- loss: 손실 함수 (예: CrossEntropy, FocalLoss 등)
- seed: 랜덤 시드 (int)
- device: 학습 디바이스 (cpu, cuda)
- early_stopping: 조기 종료 여부 (True/False)
- save_best_only: 최적 모델만 저장 (True/False)

## 각 설정 값 설명 및 사용 가능 값
- model: 학습에 사용할 네트워크 구조. 프로젝트 내 models 모듈 참고.
- optimizer: 학습 최적화 방법. Adam은 빠른 수렴, SGD는 일반적, AdamW는 weight decay 적용.
- learning_rate: 너무 크면 발산, 너무 작으면 느린 학습. 일반적으로 0.001~0.01 추천.
- batch_size: GPU 메모리 상황에 따라 조정. 크면 빠른 학습, 작으면 일반화에 도움.
- epochs: 데이터/모델에 따라 다름. 과적합 방지 위해 early_stopping과 병행 추천.
- scheduler: 학습률 조정 방식. CosineAnnealing은 점진적 감소, StepLR은 일정 에폭마다 감소.
- augmentation: src/data/transforms.py 참고. 다양한 증강 조합 가능.
- loss: 분류/회귀/다중클래스에 따라 선택.
- seed: 재현성 확보용.
- device: GPU 사용 시 'cuda'로 설정.
- early_stopping: 검증 성능 정체 시 학습 중단.
- save_best_only: 검증 최고 성능 모델만 저장.

## 예시
```yaml
model: resnet50
optimizer: Adam
learning_rate: 0.001
batch_size: 32
epochs: 50
scheduler: CosineAnnealing
augmentation: advanced
loss: CrossEntropy
seed: 42
device: cuda
early_stopping: True
save_best_only: True
```

## 참고
- configs/train.yaml, train_highperf.yaml 등에서 실제 예시 확인 가능
- src/models, src/training, src/optimization 등 모듈과 연계됨
