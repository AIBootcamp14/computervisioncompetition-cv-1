# 🎓 학습 파이프라인 완전 가이드

## 🏗️ 학습 파이프라인 전체 아키텍처

```mermaid
flowchart TB
    subgraph CLI["🚀 CLI 진입점"]
        A["src/training/train_main.py<br>통합 학습 인터페이스<br>CLI 옵션 파싱 및 모드 선택"]
    end
    
    subgraph MODES["📋 학습 모드 분기"]
        B["📚 basic 모드<br>src/training/train.py<br>단일/K-fold + 기본 설정"]
        C["⚡ highperf 모드<br>src/training/train_highperf.py<br>고성능 최적화 기법"]
        D["🔄 full-pipeline 모드<br>src/pipeline/full_pipeline.py<br>학습→추론 통합 실행"]
        E["🔍 optimize 모드<br>src/optimization/optuna_optimize.py<br>하이퍼파라미터 자동 튜닝"]
    end
    
    subgraph VALIDATION["🎯 검증 전략"]
        F["📍 단일 폴드 (Single Fold)<br>valid_fold: 0~4<br>빠른 프로토타입"]
        G["🔀 K-fold 교차검증<br>valid_fold: all<br>5-fold 안정성 확보"]
        H["⚖️ 계층적 분할<br>stratify: true<br>클래스 균형 유지"]
    end
    
    subgraph MODELS["🏗️ 모델 아키텍처"]
        I["📊 EfficientNet B3<br>efficientnet_b3<br>12M 파라미터, 224px"]
        J["🏆 ConvNeXt Base 384<br>convnext_base_384<br>89M 파라미터, 384px"]
        K["🎯 Swin Transformer<br>swin_base_patch4_window12_384<br>88M 파라미터, 384px"]
        L["🎨 다중 모델 앙상블<br>train_multi_model_ensemble.yaml<br>여러 아키텍처 조합"]
    end
    
    subgraph TECHS["⚡ Team 고성능 기법"]
        M["💪 Hard Augmentation<br>src/data/transforms.py<br>동적 확률 스케줄링"]
        N["🎲 Mixup & CutMix<br>Alpha 최적화<br>데이터 증강 강화"]
        O["🌡️ Temperature Scaling<br>src/calibration/calibrate.py<br>확률 보정"]
        P["⚡ Auto Batch Size<br>src/utils/gpu_optimization/<br>GPU 메모리 최적화"]
    end
    
    A --> B & C & D & E
    B --> F & G
    C --> G & H
    F & G --> I & J & K & L
    I & J & K & L --> M & N & O & P
    
    style A fill:#e1f5fe, color:#000000
    style C fill:#e8f5e8, color:#000000
    style E fill:#fff3e0, color:#000000
    style G fill:#f3e5f5, color:#000000
    style L fill:#ffcdd2, color:#000000
    style M fill:#fce4ec, color:#000000
```

## 🔀 학습 파이프라인 상세 흐름도

### 📍 단일 폴드 학습 흐름
```mermaid
flowchart LR
    subgraph SINGLE["🎯 단일 폴드 모드"]
        A1["📁 데이터 준비<br/>data/raw/train/<br/>data/raw/train.csv<br/>이미지 + 라벨"]
        B1["🔪 단일 분할<br/>valid_fold: 0-4<br/>특정 폴드만 선택<br/>빠른 학습"]
        C1["🚀 모델 학습<br/>선택된 폴드<br/>1개 모델 집중 학습<br/>GPU 최적화"]
        D1["📊 검증<br/>F1-Score<br/>단일 성능 평가<br/>빠른 피드백"]
        E1["💾 결과 저장<br/>best_fold_N.pth<br/>1개 파일<br/>최적 가중치"]
        
        A1 --> B1 --> C1 --> D1 --> E1
    end
    
    subgraph INFO1["📋 단일 폴드 정보"]
        F1["⚡ 속도: 매우 빠름<br/>🎯 목적: 프로토타입<br/>📈 성능: 0.92-0.95"]
        G1["⚙️ 설정: valid_fold: 0<br/>🔧 에포크: 5-15<br/>💾 메모리: 8GB"]
    end
    
    style A1 fill:#e1f5fe, color:#000000
    style C1 fill:#e8f5e8, color:#000000
    style E1 fill:#fff3e0, color:#000000
    style F1 fill:#f3e5f5, color:#000000
```

### 🔀 K-Fold 교차검증 흐름
```mermaid
flowchart LR
    subgraph KFOLD["🔄 K-Fold 모드"]
        A2["📁 데이터 준비<br/>전체 학습 데이터<br/>계층적 분할 준비"]
        B2["🔀 5-Fold 분할<br/>KFold n=5<br/>stratified=true<br/>클래스 균형 유지"]
        C2["🎯 순차 학습<br/>Fold 0→1→2→3→4<br/>각각 독립 학습<br/>교차 검증"]
        D2["📊 폴드별 검증<br/>각 Fold 성능<br/>F1-Score 계산<br/>안정성 확보"]
        E2["💾 전체 모델 저장<br/>5개 모델 파일<br/>평균 성능 계산<br/>앙상블 준비"]
        
        A2 --> B2 --> C2 --> D2 --> E2
        C2 --> C2
    end
    
    subgraph INFO2["📋 K-Fold 정보"]
        F2["⏱️ 속도: 느림 (5배)<br/>🎯 목적: 안정성<br/>📈 성능: 0.95-0.98"]
        G2["⚙️ 설정: valid_fold: all<br/>🔧 에포크: 30-50<br/>💾 메모리: 16GB+"]
    end
    
    style A2 fill:#e1f5fe, color:#000000
    style C2 fill:#e8f5e8, color:#000000
    style E2 fill:#fff3e0, color:#000000
    style F2 fill:#f3e5f5, color:#000000
```

### 🎨 다중 모델 앙상블 학습 흐름
```mermaid
flowchart TB
    subgraph MULTI["🎭 다중 모델 앙상블"]
        A3["⚙️ 다중 설정 준비<br/>configs/train_multi_model_ensemble.yaml<br/>여러 모델 아키텍처 정의"]
        
        B3["🏗️ 모델 1: ConvNeXt<br/>convnext_base_384<br/>고성능 기본 모델"]
        C3["🏗️ 모델 2: Swin<br/>swin_base_384<br/>Transformer 기반"]
        D3["🏗️ 모델 3: EfficientNet<br/>efficientnet_v2_b3<br/>효율성 중심"]
        
        E3["🔄 순차/병렬 학습<br/>각 모델 독립 학습<br/>동일 데이터 다른 구조"]
        F3["📊 개별 성능 평가<br/>모델별 F1-Score<br/>강점 분석"]
        G3["💾 모델별 저장<br/>서로 다른 경로<br/>독립적 체크포인트"]
        
        A3 --> B3 & C3 & D3
        B3 & C3 & D3 --> E3
        E3 --> F3 --> G3
    end
    
    subgraph INFO3["📋 다중 모델 정보"]
        H3["🎯 목적: 다양성 확보<br/>📈 성능: 개별 최적화<br/>🔄 앙상블 준비"]
        I3["💾 저장: 각각 다른 폴더<br/>📊 비교: 모델간 성능<br/>⚡ 최적: GPU별 모델 할당"]
    end
    
    style A3 fill:#e1f5fe, color:#000000
    style E3 fill:#e8f5e8, color:#000000
    style G3 fill:#fff3e0, color:#000000
    style H3 fill:#f3e5f5, color:#000000
```

## 📁 파일 간 의존 관계 및 데이터 흐름 다이어그램

### 🎯 전체 시스템 의존 관계
```mermaid
graph TB
    subgraph CONFIGS["⚙️ 설정 파일 계층"]
        CONFIG1["configs/train.yaml<br/>📍 단일폴드 + 기본설정<br/>valid_fold: 0-4"]
        CONFIG2["configs/train_highperf.yaml<br/>🔀 K-fold + 고성능<br/>valid_fold: all"]
        CONFIG3["configs/train_multi_model_ensemble.yaml<br/>🎭 다중모델 앙상블<br/>여러 아키텍처"]
        CONFIG4["configs/optuna_config.yaml<br/>🔍 하이퍼파라미터 튜닝<br/>베이지안 최적화"]
    end
    
    subgraph CORE["🧠 핵심 실행 시스템"]
        MAIN["src/training/train_main.py<br/>🚀 메인 CLI 인터페이스<br/>모드 선택 및 옵션 처리"]
        TRAIN_BASIC["src/training/train.py<br/>📚 기본 학습 로직<br/>단일/K-fold 지원"]
        TRAIN_HIGH["src/training/train_highperf.py<br/>⚡ 고성능 학습 로직<br/>Team 최적화 기법"]
        PIPELINE["src/pipeline/full_pipeline.py<br/>🔄 통합 파이프라인<br/>학습→추론 자동화"]
        OPTUNA["src/optimization/optuna_optimize.py<br/>🔍 하이퍼파라미터 최적화<br/>자동 튜닝"]
    end
    
    subgraph SUPPORT["🔧 지원 시스템"]
        MODELS["src/models/build.py<br/>🏗️ 모델 빌더<br/>동적 아키텍처 생성"]
        DATA["src/data/dataset.py<br/>📊 데이터 로더<br/>증강 및 전처리"]
        TRANSFORMS["src/data/transforms.py<br/>🎨 데이터 증강<br/>Hard Aug + TTA"]
        CALIBRATION["src/calibration/calibrate.py<br/>🌡️ 모델 보정<br/>Temperature Scaling"]
        GPU_OPT["src/utils/gpu_optimization/<br/>⚡ GPU 최적화<br/>배치 크기 자동 조정"]
    end
    
    subgraph INPUT["📥 입력 데이터"]
        RAW_IMGS["data/raw/train/<br/>📸 원본 학습 이미지<br/>17클래스 분류"]
        TRAIN_CSV["data/raw/train.csv<br/>📋 라벨 정보<br/>이미지-클래스 매핑"]
        SAMPLE_CSV["data/raw/sample_submission.csv<br/>📄 제출 형식<br/>추론 결과 템플릿"]
    end
    
    subgraph OUTPUT["📤 출력 결과"]
        EXP_SINGLE["experiments/train/YYYYMMDD/<br/>📍 단일 폴드 결과<br/>best_fold_N.pth"]
        EXP_KFOLD["experiments/train/YYYYMMDD/<br/>🔀 K-fold 결과<br/>best_fold_0-4.pth"]
        EXP_MULTI["experiments/train/YYYYMMDD/<br/>🎭 다중 모델 결과<br/>모델별 분리 저장"]
        FOLD_RESULTS["fold_results.yaml<br/>📊 폴드 성능 요약<br/>추론용 메타데이터"]
        LOGS["logs/YYYYMMDD/train/<br/>📝 학습 로그<br/>성능 추적"]
    end
    
    %% 설정 파일 → 메인 시스템 연결
    CONFIG1 --> MAIN
    CONFIG2 --> MAIN  
    CONFIG3 --> MAIN
    CONFIG4 --> OPTUNA
    
    %% 메인 시스템 → 실행 로직 분기
    MAIN --> TRAIN_BASIC
    MAIN --> TRAIN_HIGH
    MAIN --> PIPELINE
    MAIN --> OPTUNA
    
    %% 실행 로직 → 지원 시스템 의존
    TRAIN_BASIC --> MODELS & DATA & TRANSFORMS
    TRAIN_HIGH --> MODELS & DATA & TRANSFORMS & CALIBRATION & GPU_OPT
    PIPELINE --> TRAIN_HIGH
    OPTUNA --> TRAIN_BASIC
    
    %% 입력 데이터 → 데이터 처리
    RAW_IMGS --> DATA
    TRAIN_CSV --> DATA
    SAMPLE_CSV --> PIPELINE
    
    %% 실행 → 출력 생성
    TRAIN_BASIC --> EXP_SINGLE & LOGS
    TRAIN_HIGH --> EXP_KFOLD & FOLD_RESULTS & LOGS
    PIPELINE --> EXP_MULTI & LOGS
    
    style CONFIG2 fill:#e8f5e8, color:#000000
    style CONFIG3 fill:#f3e5f5, color:#000000
    style MAIN fill:#fff3e0, color:#000000
    style TRAIN_HIGH fill:#ffcdd2, color:#000000
    style EXP_KFOLD fill:#e1f5fe, color:#000000
    style FOLD_RESULTS fill:#fce4ec, color:#000000
```

### 🔄 학습 모드별 데이터 흐름
```mermaid
flowchart LR
    subgraph MODE1["📍 단일 폴드 모드"]
        A1["train.yaml<br/>valid_fold: N"]
        B1["train.py<br/>특정 폴드 선택"]
        C1["1개 모델 학습<br/>빠른 결과"]
        D1["best_fold_N.pth<br/>단일 결과"]
        
        A1 --> B1 --> C1 --> D1
    end
    
    subgraph MODE2["🔀 K-fold 모드"]
        A2["train_highperf.yaml<br/>valid_fold: all"]
        B2["train_highperf.py<br/>전체 폴드 순회"]
        C2["5개 모델 학습<br/>교차 검증"]
        D2["5개 .pth 파일<br/>fold_results.yaml"]
        
        A2 --> B2 --> C2 --> D2
    end
    
    subgraph MODE3["🎭 다중 모델 모드"]
        A3["train_multi_model<br/>_ensemble.yaml"]
        B3["여러 아키텍처<br/>동시 학습"]
        C3["ConvNeXt+Swin<br/>+EfficientNet"]
        D3["모델별 분리<br/>저장 및 관리"]
        
        A3 --> B3 --> C3 --> D3
    end
    
    style A1 fill:#e1f5fe, color:#000000
    style A2 fill:#e8f5e8, color:#000000
    style A3 fill:#f3e5f5, color:#000000
    style C1 fill:#fff3e0, color:#000000
    style C2 fill:#ffcdd2, color:#000000
    style C3 fill:#fce4ec, color:#000000
```

### 📂 디렉토리 구조 및 파일 상세 설명

```text
🎓 학습 파이프라인 파일 구조
├── src/training/
│   ├── train_main.py                          # 🚀 메인 실행 인터페이스
│   ├── train.py                               # 🧠 핵심 학습 로직
│   ├── train_highperf.py                      # 🏆 고성능 학습 로직
│   └── __init__.py
│
├── src/models/
│   ├── build.py                               # 🏗️ 모델 아키텍처 빌더
│   └── __init__.py
│
├── src/data/
│   ├── dataset.py                             # 📊 데이터셋 및 로더
│   ├── transforms.py                          # 🔄 데이터 증강
│   └── __init__.py
│
├── src/metrics/
│   ├── __init__.py
│   └── f1.cpython-311.py                     # 
│
├── src/utils/
│   ├── core/                                  # 🔧 핵심 유틸리티
│   │   └── common.py                          # 공통 함수 (파일/YAML/로깅)
│   ├── config/                                # ⚙️ 설정 관리
│   │   ├── seed.py                            # 시드 설정
│   │   └── update_config_dates.py             # 설정 날짜 업데이트
│   ├── gpu_optimization/                      # � GPU 최적화
│   │   ├── auto_batch_size.py                 # 동적 배치 크기 결정
│   │   └── team_gpu_check.py                  # GPU 환경 분석
│   ├── code_management/                       # 📋 코드 관리
│   │   └── reorganize_experiments.py          # 실험 정리
│   ├── visualizations/                        # 📊 시각화 시스템
│   │   ├── base_visualizer.py                 # 기본 시각화 엔진
│   │   ├── training_viz.py                    # 학습 시각화
│   │   ├── inference_viz.py                   # 추론 시각화
│   │   ├── optimization_viz.py                # 최적화 시각화
│   │   └── output_manager.py                  # 출력 관리
│   └── __init__.py
│
├── src/optimization/
│   ├── optuna_optimize.py                     # 🔍 하이퍼파라미터 튜닝
│   └── __init__.py
│
├── src/calibration/
│   ├── calibrate.py                           # 📏 모델 보정
│   └── __init__.py
│
├── src/logging/
│   ├── logger.py                              # � 로깅 시스템
│   └── __init__.py
│
├── configs/
│   ├── train.yaml                             # ⚙️ 기본 학습 설정
│   ├── train_highperf.yaml                    # 🏆 고성능 학습 설정
│   ├── train_fast_optimized.yaml              # ⚡ 빠른 학습 설정
│   ├── optuna_config.yaml                     # 🔍 하이퍼파라미터 튜닝 설정
│   └── optuna_fast_config.yaml                # ⚡ 빠른 튜닝 설정
│
├── data/raw/
│   ├── train/                                 # 📁 학습 이미지 폴더
│   ├── meta.csv                               # 📋 라벨 및 메타데이터
│   └── sample_submission.csv                  # 📄 제출 형식 예시
│
├── experiments/
│   ├── train/YYYYMMDD/                        # 📁 날짜별 학습 실험
│   │   └── model_name_YYYYMMDD_HHMM/
│   │       ├── ckpt/                          # 💾 체크포인트 저장소
│   │       │   ├── best_fold0.pth
│   │       │   ├── best_fold1.pth
│   │       │   ├── best_fold2.pth
│   │       │   ├── best_fold3.pth
│   │       │   ├── best_fold4.pth
│   │       │   └── last.pth                   # 마지막 에포크 모델
│   │       ├── config.yaml                    # 실험 설정 백업
│   │       └── metrics.json                   # 성능 지표 기록
│   └── optimization/                          # 📁 최적화 실험 결과
│
├── logs/YYYYMMDD/
│   └── train/                                 # 📁 학습 로그
│       ├── train_HHMM.log                     # 학습 진행 로그
│       └── metrics_HHMM.json                  # 성능 지표 로그
│
├── scripts/
│   ├── run_fast_training.sh                   # ⚡ 빠른 학습 실행 스크립트
│   ├── run_highperf_training.sh               # 🏆 고성능 학습 실행 스크립트
│   └── monitor_training.sh                    # 📊 학습 모니터링 스크립트
│
└── wandb/                                     # 📊 실험 추적 데이터
    └── run-*/                                 # WandB 실행 기록
```

#### 🔍 핵심 파일 상세 기능

**1. src/training/train_main.py**
- **주요 기능**: 명령행 인터페이스 제공 및 학습 파이프라인 통합 관리
- **핵심 역할**: 
  - argparse를 통한 사용자 입력 처리
  - 설정 파일 로딩 및 검증
  - 학습 모드 선택 (기본/고성능/빠른 모드)
  - GPU 자동 감지 및 분산 학습 설정
- **의존성**: train.py, config 파일들과 직접 연결

**2. src/training/train.py**
- **주요 기능**: 5-Fold Cross Validation 및 핵심 학습 로직 구현
- **핵심 역할**:
  - KFold 데이터 분할 (Stratified 방식)
  - 각 Fold별 모델 학습 관리
  - Early Stopping 및 Learning Rate Scheduling
  - 검증 성능 평가 (F1-Score, Accuracy)
  - 최적 모델 체크포인트 저장
- **의존성**: models/build.py, data/dataset.py와 긴밀한 연결

**3. src/models/build.py**
- **주요 기능**: 모델 아키텍처 동적 빌딩 및 백본 네트워크 관리
- **핵심 역할**:
  - EfficientNet, ResNet, Swin Transformer 등 백본 선택
  - 사전 훈련 가중치 로딩
  - 분류 헤드 커스터마이징 (17개 클래스 대응)
  - 모델 파라미터 초기화
- **의존성**: timm, torchvision 라이브러리 활용

**4. src/data/dataset.py**
- **주요 기능**: 커스텀 데이터셋 클래스 및 데이터 로더 구현
- **핵심 역할**:
  - 이미지 파일 로딩 및 전처리 파이프라인
  - 라벨 인코딩 및 클래스 매핑
  - 데이터 증강 적용 (Albumentations)
  - 배치 생성 및 GPU 메모리 최적화
- **의존성**: transforms.py와 연동하여 증강 기법 적용

**5. configs/train_*.yaml**
- **train.yaml**: 일반적인 실험용 기본 설정 (EfficientNet-B3, 224px)
- **train_highperf.yaml**: 최고 성능 추구 설정 (Swin-Base, 384px, 낮은 LR)
- **train_fast_optimized.yaml**: 빠른 프로토타입 설정 (작은 모델, 높은 LR)

## ⚙️ 설정 파일 및 실행 명령어

### 1. 📍 단일 폴드 학습 모드 (Single Fold Training)

**목적**: 빠른 프로토타입 및 기본 성능 확인
**특징**: 특정 1개 폴드만 선택하여 학습, 빠른 결과 피드백
**기대 성능**: F1 Score 0.92-0.95

#### 설정 파일: `configs/train.yaml` (단일 폴드)
```yaml
# 단일 폴드 핵심 설정
model:
  name: efficientnet_b3        # 단일 모델 선택
  img_size: 224
  num_classes: 17

train:
  epochs: 10                   # 빠른 학습용
  batch_size: 32
  learning_rate: 0.001
  
data:
  folds: 5                     # 전체 폴드 수
  valid_fold: 0                # 특정 폴드 선택 (0-4)
  train_csv: data/raw/train.csv
  image_dir_train: data/raw/train
```

#### 단일 폴드 실행 명령어
```bash
# Fold 0으로 빠른 학습
python src/training/train_main.py --config configs/train.yaml

# 특정 폴드 지정 (예: Fold 2)
# configs/train.yaml에서 valid_fold: 2로 수정 후 실행
python src/training/train_main.py --config configs/train.yaml --mode basic

# 단일 폴드 + Optuna 최적화
python src/training/train_main.py --config configs/train.yaml --optimize --n-trials 10
```

### 2. 🔀 K-Fold 교차검증 학습 모드 (K-Fold Cross Validation)

**목적**: 안정적이고 신뢰할 수 있는 성능 달성
**특징**: 5개 폴드 모두 학습하여 교차검증, 앙상블 기반 구축
**기대 성능**: F1 Score 0.95-0.98

### 3. 🎭 다중 모델 앙상블 학습 모드 (Multi-Model Ensemble)

**목적**: 다양한 아키텍처를 통한 성능 극대화
**특징**: ConvNeXt, Swin, EfficientNet 등 여러 모델 동시 학습
**기대 성능**: 개별 모델 최적화 + 앙상블 효과

#### 설정 파일: `configs/train_highperf.yaml` (K-fold)
```yaml
# K-fold 교차검증 설정
model:
  name: "convnext_base_384_in22ft1k"  # Team 최고 성능 모델
  img_size: 384
  num_classes: 17
  drop_rate: 0.05
  drop_path_rate: 0.1

train:
  epochs: 50                        # 안정적 성능 달성
  batch_size: 48
  lr: 0.0001
  use_advanced_augmentation: true   # Hard Augmentation
  use_mixup: true
  mixup_alpha: 1.0
  label_smoothing: 0.05
  
data:
  folds: 5
  valid_fold: all                   # 모든 폴드 학습
  stratify: true                    # 계층적 분할
```

#### K-fold 실행 명령어
```bash
# 전체 5-fold 교차검증
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# K-fold + Temperature Scaling 보정
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf --use-calibration

# K-fold + 최적화 + 자동 진행 (Team 최고 성능)
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf --optimize --n-trials 30 --use-calibration --auto-continue
```

#### 설정 파일: `configs/train_multi_model_ensemble.yaml` (다중 모델)
```yaml
# 다중 모델 앙상블 설정
project:
  run_name: multi-model-ensemble
  
models:
  - name: convnext_base_384
    img_size: 384
    epochs: 30
    lr: 0.0001
  - name: swin_base_patch4_window12_384  
    img_size: 384
    epochs: 35
    lr: 0.00008
  - name: efficientnet_v2_b3
    img_size: 320
    epochs: 25
    lr: 0.0002
    
data:
  valid_fold: all                   # 모든 모델에 K-fold 적용
  use_advanced_augmentation: true
```

#### 다중 모델 실행 명령어
```bash
# 다중 모델 동시 학습
python src/training/train_main.py --config configs/train_multi_model_ensemble.yaml --mode highperf

# 모델별 병렬 학습 (GPU 여러 개 사용 시)
CUDA_VISIBLE_DEVICES=0 python src/training/train_main.py --config configs/convnext_config.yaml &
CUDA_VISIBLE_DEVICES=1 python src/training/train_main.py --config configs/swin_config.yaml &
CUDA_VISIBLE_DEVICES=2 python src/training/train_main.py --config configs/efficientnet_config.yaml &
wait
```

## 🚀 실행 옵션 완전 가이드

### 기본 명령어 구조
```bash
python src/training/train_main.py [필수옵션] [선택옵션]
```

### 📊 CLI 옵션 완전 가이드

| 옵션 | 타입 | 필수/선택 | 기본값 | choices | 설명 |
|------|------|----------|--------|---------|------|
| `--config` | str | 필수 | - | - | 설정 YAML 파일 경로 |
| `--mode` | str | 선택 | "full-pipeline" | ["basic", "highperf", "full-pipeline"] | 실행 모드 선택 |
| `--skip-training` | flag | 선택 | False | - | 학습 건너뛰고 추론만 실행 |
| `--optimize` | flag | 선택 | False | - | Optuna 하이퍼파라미터 최적화 |
| `--n-trials` | int | 선택 | 20 | - | Optuna 시도 횟수 |
| `--optuna-config` | str | 선택 | None | - | Optuna 설정 파일 경로 |
| `--use-calibration` | flag | 선택 | False | - | Temperature Scaling 사용 |
| `--auto-continue` | flag | 선택 | False | - | 최적화 후 자동 진행 |

### 선택 옵션 상세 설명

#### 1. `--mode <모드>` - 학습 전략 선택
**기능**: 학습 파이프라인의 실행 모드 선택

**모드별 비교**:
| 모드 | 속도 | 성능 | 목적 | 추천 상황 |
|------|------|------|------|----------|
| `basic` | ⚡ 빠름 (30분) | 0.92-0.94 | 프로토타입 | 빠른 검증, 단일 폴드 |
| `highperf` | 🕰️ 느림 (1-3시간) | 0.95-0.98 | 최고 성능 | K-fold, 최종 제출 |
| `full-pipeline` | 🔄 중간 (1-2시간) | 0.93-0.96 | 전체 자동화 | 학습+추론 통합 |

```bash
# 단일 폴드 빠른 학습 (valid_fold: 0-4)
python src/training/train_main.py --config configs/train.yaml --mode basic

# K-fold 교차검증 고성능 학습 (valid_fold: all)
python src/training/train_main.py --config configs/train_highperf.yaml --mode highperf

# 학습부터 추론까지 자동 완성
python src/training/train_main.py --config configs/train.yaml --mode full-pipeline
```

#### 2. `--optimize`
**기능**: Optuna 하이퍼파라미터 자동 최적화 활성화
**효과**: learning_rate, batch_size, augmentation 파라미터 자동 튜닝

```bash
# 최적화 없이 학습 (기본 설정 사용)
python src/training/train_main.py --config configs/train.yaml

# 최적화와 함께 학습 (성능 향상 기대)
python src/training/train_main.py --config configs/train.yaml --optimize
```

#### 3. `--n-trials <숫자>`
**기능**: Optuna 최적화 시도 횟수 설정 (--optimize와 함께 사용)
**기본값**: 20
**권장값**: 
- 빠른 테스트: 5-10
- 일반 최적화: 20
- 완전 최적화: 50+

```bash
# 빠른 최적화 (5번 시도)
python src/training/train_main.py --config configs/train.yaml --optimize --n-trials 5

# 표준 최적화 (20번 시도)
python src/training/train_main.py --config configs/train.yaml --optimize --n-trials 20

# 완전 최적화 (50번 시도)
python src/training/train_main.py --config configs/train.yaml --optimize --n-trials 50
```

#### 4. `--use-calibration`
**기능**: Temperature Scaling 캘리브레이션 활성화
**효과**: 모델 신뢰도(confidence) 향상, 더 정확한 확률 예측

```bash
# 캘리브레이션 없이
python src/training/train_main.py --config configs/train.yaml

# 캘리브레이션 포함 (추천)
python src/training/train_main.py --config configs/train.yaml --use-calibration
```

#### 5. `--auto-continue`
**기능**: 최적화 완료 후 자동으로 전체 학습 진행 (--optimize와 함께 사용)

```bash
# 최적화만 실행 (수동으로 다음 단계 진행)
python src/training/train_main.py --config configs/train.yaml --optimize

# 최적화 후 자동으로 전체 학습 진행
python src/training/train_main.py --config configs/train.yaml --optimize --auto-continue
```

#### 6. `--resume`
**기능**: 중단된 학습을 체크포인트부터 재개

```bash
# 처음부터 학습
python src/training/train_main.py --config configs/train.yaml

# 중단된 지점부터 재개
python src/training/train_main.py --config configs/train.yaml --resume
```

## 🎯 실전 사용 예시

### 1. 빠른 결과 확인 (30분)
```bash
python src/training/train_main.py --config configs/train.yaml --mode basic
```

### 2. 최고 성능 추구 (2-3시간)
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode highperf \
    --optimize \
    --n-trials 20 \
    --use-calibration \
    --auto-continue
```

### 3. 빠른 최적화 테스트 (1시간)
```bash
python src/training/train_main.py \
    --config configs/train.yaml \
    --optimize \
    --n-trials 5 \
    --use-calibration
```

### 4. 전체 파이프라인 (학습+추론)
```bash
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode full-pipeline \
    --use-calibration
```

### 5. 🚀 단일 폴드 최적화 (신규 - 2025.09.10 추가)
**배경**: 경진대회 마감 시간 단축을 위해 K-fold 대신 단일 폴드 + 앙상블 전략 적용

```bash
# 단일 폴드 하이퍼파라미터 최적화 (매우 빠름 - 2분/trial)
python src/training/train_main.py \
    --config configs/train_highperf.yaml \
    --mode full-pipeline \
    --use-calibration \
    --optimize \
    --optuna-config configs/optuna_single_fold_config.yaml \
    --auto-continue
```

**핵심 개선사항:**
- ⚡ **속도 향상**: 매 trial 2초 완료 (데이터셋 캐싱 적용)
- 🎯 **실제 F1 점수**: 시뮬레이션 아닌 실제 학습 기반 평가
- 📊 **높은 성능**: F1 0.947+ 달성 가능
- 🔧 **메모리 최적화**: 캐시된 데이터로 반복 학습 속도 극대화

**설정 파일**: `configs/optuna_single_fold_config.yaml`
```yaml
optuna:
  n_trials: 20              # 빠른 테스트용 (실제 100+ 권장)
  n_jobs: 1                 # GPU 메모리에 따라 조정
  single_fold_validation:
    epochs: 5               # 빠른 검증용
    validation_split: 0.2   # 80:20 분할
```

## 📊 성능 비교 및 Team 기법 분석

### 🏆 최신 vs 기존 성능 비교

| 구분 | 모델 | F1 Score | 성능 향상 | 실험 일시 | 핵심 기법 |
|------|------|----------|----------|---------|---------|
| **기존 베이스라인** | EfficientNet B3 | 0.9238 | - | - | 기본 증강 |
| **고성능 최신** | ConvNeXt Base 384 | **0.98362** | **+6.5%** | 2025-09-10 12:13 | **최적화된 전체 기법** |
| **고성능 이전** | ConvNeXt Base 384 | **0.97918** | **+5.9%** | 2025-09-10 09:29 | Hard Aug + TTA + 캘리브레이션 |

### ⚡ 실행 모드별 성능 분석

| 실행 옵션 | 예상 시간 | 예상 F1 | GPU 메모리 | Team 기법 적용 | 추천 상황 |
|-----------|-----------|---------|------------|-------------|-----------|
| `--mode basic` | 30분 | 0.920-0.930 | 8GB | ❌ | 빠른 프로토타입 |
| `--mode basic --optimize` | 1시간 | 0.935-0.945 | 8GB | 부분적 | 개선된 베이스라인 |
| `--mode highperf` | 2시간 | **0.950-0.965** | 16GB | ✅ **전체** | **최종 제출용** |
| `--mode highperf --optimize` | 3시간 | **0.965+** | 16GB | ✅ **전체+튜닝** | **대회 우승용** |
| **🚀 단일 폴드 최적화** | **23분** | **0.969-0.984** | **12GB** | ✅ **캐싱+최적화** | **⚡ 빠른 경진대회용** |

### 🎯 Team 고성능 기법별 기여도

| 기법 | 성능 기여 | 구현 위치 | 설명 |
|------|----------|---------|------|
| **ConvNeXt ImageNet-22k** | +2.0% | `configs/train_highperf.yaml` | 사전학습 모델 업그레이드 |
| **Hard Augmentation** | +1.5% | `src/data/transforms.py:180-216` | 동적 확률 스케줄링 |
| **Essential TTA** | +0.8% | `src/data/transforms.py:221-250` | 5가지 핵심 변환 |
| **Temperature Scaling** | +0.3% | `src/calibration/` | 확률 보정 |
| **전체 조합** | **+4.14%** | 통합 적용 | 시너지 효과 |

## ⚠️ 주의사항 및 팁

### GPU 메모리 최적화
```bash
# GPU 메모리 부족 시
python src/utils/gpu_optimization/auto_batch_size.py --config configs/train.yaml
python src/training/train_main.py --config configs/train.yaml --mode basic
```

### 학습 모니터링
```bash
# 실시간 로그 확인
tail -f logs/$(date +%Y%m%d)/train/*.log

# GPU 사용량 모니터링
watch -n 1 nvidia-smi
```

### 체크포인트 관리
```bash
# 최신 모델 확인
ls -la experiments/train/$(date +%Y%m%d)/*/results/ckpt/

# 시각화 이미지 확인  
ls -la experiments/train/$(date +%Y%m%d)/*/images/

# 특정 fold 모델 사용
python src/training/train_main.py --config configs/train.yaml --resume --fold 0
```

## 🚀 모든 실행 명령어 모음

### 기본 학습 명령어
```bash
# 1. 기본 학습 (EfficientNet-B3, 224px)
python src/training/train_main.py --config configs/train.yaml

# 2. 고성능 학습 (Swin-Base, 384px)
python src/training/train_main.py --config configs/train_highperf.yaml

# 3. 빠른 학습 (프로토타입용)
python src/training/train_main.py --config configs/train_fast_optimized.yaml
```

### 고급 학습 옵션
```bash
# 4. 특정 모드로 학습
python src/training/train_main.py --config configs/train.yaml --mode basic
python src/training/train_main.py --config configs/train.yaml --mode highperf
python src/training/train_main.py --config configs/train.yaml --mode fast

# 5. 학습 재개 (체크포인트에서)
python src/training/train_main.py --config configs/train.yaml --resume
python src/training/train_main.py --config configs/train.yaml --resume --fold 2

# 6. 자동 계속 (중단된 학습 자동 재개)
python src/training/train_main.py --config configs/train.yaml --auto-continue

# 7. 특정 fold만 학습
python src/training/train_main.py --config configs/train.yaml --fold 0
python src/training/train_main.py --config configs/train.yaml --fold 1,2,3
```

### 최적화 관련 명령어
```bash
# 8. Optuna 하이퍼파라미터 튜닝
python src/training/train_main.py --config configs/train.yaml --optimize --n-trials 50

# 9. 빠른 최적화 (적은 trial)
python src/training/train_main.py --config configs/train_fast_optimized.yaml --optimize --n-trials 10

# 10. 보정 포함 학습
python src/training/train_main.py --config configs/train.yaml --use-calibration

# 11. 최적화 + 보정 조합
python src/training/train_main.py --config configs/train_highperf.yaml --optimize --n-trials 30 --use-calibration
```

### GPU 및 배치 설정
```bash
# 12. 특정 GPU 지정
CUDA_VISIBLE_DEVICES=0 python src/training/train_main.py --config configs/train.yaml
CUDA_VISIBLE_DEVICES=1 python src/training/train_main.py --config configs/train_highperf.yaml

# 13. 자동 배치 크기 결정
python src/utils/gpu_optimization/auto_batch_size.py --config configs/train.yaml
python src/utils/gpu_optimization/auto_batch_size.py --config configs/train_highperf.yaml

# 14. GPU 상태 확인
python src/utils/gpu_optimization/team_gpu_check.py
```

### 데이터 및 전처리
```bash
# 15. 데이터 준비 확인
ls -la data/raw/train/
head -5 data/raw/meta.csv

# 16. 클래스 분포 확인
python -c "
import pandas as pd
df = pd.read_csv('data/raw/meta.csv')
print('클래스 분포:')
print(df['label'].value_counts().sort_index())
print(f'\n총 이미지 수: {len(df)}')
print(f'클래스 수: {df["label"].nunique()}')
"
```

### 모니터링 및 로그
```bash
# 17. 실시간 학습 로그 확인
tail -f logs/$(date +%Y%m%d)/train/*.log

# 18. GPU 메모리 모니터링
watch -n 1 nvidia-smi

# 19. 학습 진행률 확인
grep -i "epoch\|loss\|f1" logs/$(date +%Y%m%d)/train/*.log | tail -20

# 20. WandB 동기화
wandb sync wandb/
```

### 결과 분석
```bash
# 21. 최신 실험 결과 확인 (전체 구조)
ls -la experiments/train/$(date +%Y%m%d)/

# 22. 체크포인트 파일 확인
find experiments/train -path "*/results/ckpt/*.pth" | head -10

# 22-1. 시각화 이미지 확인
find experiments/train/$(date +%Y%m%d) -name "*.png" | head -10

# 23. 학습 성능 요약
python -c "
import json, glob
metric_files = glob.glob('experiments/train/$(date +%Y%m%d)/*/results/metrics.json')
for file in metric_files:
    with open(file) as f:
        metrics = json.load(f)
        print(f'{file}: F1={metrics.get(\"best_f1\", \"N/A\"):.4f}')
"

# 24. 모델 크기 확인
ls -lh experiments/train/$(date +%Y%m%d)/*/results/ckpt/*.pth

# 25. 시각화 결과 확인
echo "=== 생성된 시각화 차트 ==="
find experiments/train/$(date +%Y%m%d) -name "*.png" -exec basename {} \; | sort
```

### 스크립트 실행
```bash
# 25. 빠른 학습 스크립트
bash scripts/run_fast_training.sh

# 26. 고성능 학습 스크립트  
bash scripts/run_highperf_training.sh

# 27. 학습 모니터링 스크립트
bash scripts/monitor_training.sh
```

### 환경 설정
```bash
# 28. Python 환경 활성화
eval "$(pyenv init --path)" && pyenv activate cv_py3_11_9

# 29. 필요한 패키지 설치
pip install -r requirements.txt

# 30. 환경 변수 설정
source path.env
```
