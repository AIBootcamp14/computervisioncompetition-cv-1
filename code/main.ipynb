{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* 데이터 로드를 위한 구글 드라이브를 마운트합니다.\n",
    "* 필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8489,
     "status": "ok",
     "timestamp": 1700314558888,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "NC8V-D393wY4",
    "outputId": "e9927325-26c4-4b89-9c51-c1d6541388d6"
   },
   "outputs": [],
   "source": [
    "# 필요한 라이브러리를 설치합니다.\n",
    "# !pip install timm\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install optuna"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n",
    "* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision용\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# 한글 폰트 설정 (시각화용)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 데이터셋 클래스를 정의합니다. (Hard Augmentation 포함)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, epoch=0, total_epochs=10, is_train=True):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.epoch = epoch\n",
    "        self.total_epochs = total_epochs\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Hard augmentation 확률 계산\n",
    "        self.p_hard = 0.2 + 0.3 * (epoch / total_epochs) if is_train else 0\n",
    "        \n",
    "        # Normal augmentation\n",
    "        self.normal_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "            ], p=0.6),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "            A.GaussNoise(var_limit=(30.0, 100.0), p=0.7),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # Hard augmentation\n",
    "        self.hard_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "                A.Rotate(limit=[-15,15], p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=15, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=15, p=1.0),\n",
    "            ], p=0.95),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.9),\n",
    "            A.GaussNoise(var_limit=(50.0, 150.0), p=0.8),\n",
    "            A.JpegCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 배치별 증강 선택\n",
    "        if self.is_train and random.random() < self.p_hard:\n",
    "            img = self.hard_aug(image=img)['image']\n",
    "        else:\n",
    "            img = self.normal_aug(image=img)['image']\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    scaler = GradScaler()  # Mixed Precision용\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Cutmix/Mixup 적용 (30% 확률)\n",
    "        if random.random() < 0.3:\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "        else:\n",
    "            with autocast(): preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        scaler.scale(loss).backward()  # Mixed Precision용\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()  # Mixed Precision용\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation을 위한 함수 추가\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    한 에폭 검증을 수행하는 함수\n",
    "    - model.eval()로 모델을 평가 모드로 전환\n",
    "    - torch.no_grad()로 gradient 계산 비활성화하여 메모리 절약\n",
    "    - 검증 데이터에 대한 loss, accuracy, f1 score 계산\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 전환 (dropout, batchnorm 비활성화)\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():  # gradient 계산 비활성화로 메모리 절약\n",
    "        pbar = tqdm(loader, desc=\"Validating\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)  # 모델 예측\n",
    "            loss = loss_fn(preds, targets)  # 손실 계산\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측 클래스 저장\n",
    "            targets_list.extend(targets.detach().cpu().numpy())  # 실제 클래스 저장\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)  # 평균 손실 계산\n",
    "    val_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')  # Macro F1 계산 (대회 평가지표)\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "# model_name = 'tf_efficientnetv2_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "# model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "model_name = 'convnext_base_384_in22ft1k'\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 2e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 30\n",
    "num_workers = 8\n",
    "EMA = True  # Exponential Moving Average 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna를 사용한 하이퍼파라미터 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        \n",
    "        # 간단한 3-fold CV로 빠른 평가\n",
    "        skf_simple = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf_simple.split(train_df, train_df['target'])):\n",
    "            # 모델 생성\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "            optimizer = Adam(model.parameters(), lr=lr)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # 간단한 2 epoch 학습\n",
    "            for epoch in range(2):\n",
    "                train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "            \n",
    "            val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "            fold_scores.append(val_ret['val_f1'])\n",
    "        \n",
    "        return np.mean(fold_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    # 최적 파라미터 적용\n",
    "    LR = study.best_params['lr']\n",
    "    BATCH_SIZE = study.best_params['batch_size']\n",
    "    print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    # 위의 objective 함수와 study 코드\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross Validation...\n",
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3115: 100%|██████████| 42/42 [00:59<00:00,  1.41s/it]\n",
      "Val Loss: 1.3557: 100%|██████████| 11/11 [00:14<00:00,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.0756 | Train F1: 0.3751 | Val Loss: 1.1050 | Val F1: 0.7195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7534: 100%|██████████| 42/42 [00:23<00:00,  1.81it/s]\n",
      "Val Loss: 0.9701: 100%|██████████| 11/11 [00:08<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.2690 | Train F1: 0.5794 | Val Loss: 0.7293 | Val F1: 0.8211\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6963: 100%|██████████| 42/42 [00:22<00:00,  1.85it/s]\n",
      "Val Loss: 0.6948: 100%|██████████| 11/11 [00:08<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.9580 | Train F1: 0.7143 | Val Loss: 0.6423 | Val F1: 0.8643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.5518: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 0.8080: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.9589 | Train F1: 0.7051 | Val Loss: 0.6204 | Val F1: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4604: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 0.7042: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.8196 | Train F1: 0.7480 | Val Loss: 0.5693 | Val F1: 0.8773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.4424: 100%|██████████| 42/42 [00:22<00:00,  1.85it/s]\n",
      "Val Loss: 0.7360: 100%|██████████| 11/11 [00:08<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.8076 | Train F1: 0.7942 | Val Loss: 0.5603 | Val F1: 0.8893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6440: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n",
      "Val Loss: 0.7574: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.8544 | Train F1: 0.7232 | Val Loss: 0.5600 | Val F1: 0.8903\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7764: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 0.7165: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.6906 | Train F1: 0.7938 | Val Loss: 0.5554 | Val F1: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5791: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 0.6886: 100%|██████████| 11/11 [00:07<00:00,  1.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6639 | Train F1: 0.7331 | Val Loss: 0.5388 | Val F1: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.1104: 100%|██████████| 42/42 [00:23<00:00,  1.81it/s]\n",
      "Val Loss: 0.7972: 100%|██████████| 11/11 [00:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.7396 | Train F1: 0.8264 | Val Loss: 0.5415 | Val F1: 0.9098\n",
      "Fold 1 Best Validation F1: 0.9260\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.2598: 100%|██████████| 42/42 [00:18<00:00,  2.28it/s]\n",
      "Val Loss: 1.4105: 100%|██████████| 11/11 [00:08<00:00,  1.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.0905 | Train F1: 0.3682 | Val Loss: 1.1281 | Val F1: 0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6377: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n",
      "Val Loss: 0.8281: 100%|██████████| 11/11 [00:08<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.2393 | Train F1: 0.6265 | Val Loss: 0.7332 | Val F1: 0.8716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7466: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n",
      "Val Loss: 0.6774: 100%|██████████| 11/11 [00:08<00:00,  1.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 1.0978 | Train F1: 0.6130 | Val Loss: 0.6357 | Val F1: 0.8750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.5366: 100%|██████████| 42/42 [00:22<00:00,  1.84it/s]\n",
      "Val Loss: 0.5902: 100%|██████████| 11/11 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8490 | Train F1: 0.7652 | Val Loss: 0.6484 | Val F1: 0.8299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6270: 100%|██████████| 42/42 [00:22<00:00,  1.85it/s]\n",
      "Val Loss: 0.6653: 100%|██████████| 11/11 [00:08<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.9368 | Train F1: 0.6702 | Val Loss: 0.5882 | Val F1: 0.9039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.5894: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 0.6158: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.8049 | Train F1: 0.7302 | Val Loss: 0.5677 | Val F1: 0.8854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7334: 100%|██████████| 42/42 [00:24<00:00,  1.75it/s]\n",
      "Val Loss: 0.6587: 100%|██████████| 11/11 [00:07<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7890 | Train F1: 0.7432 | Val Loss: 0.5391 | Val F1: 0.9100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.4448: 100%|██████████| 42/42 [00:23<00:00,  1.79it/s]\n",
      "Val Loss: 0.5100: 100%|██████████| 11/11 [00:07<00:00,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.7109 | Train F1: 0.7935 | Val Loss: 0.5300 | Val F1: 0.9064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5034: 100%|██████████| 42/42 [00:23<00:00,  1.79it/s]\n",
      "Val Loss: 0.5285: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.7113 | Train F1: 0.8241 | Val Loss: 0.5000 | Val F1: 0.9137\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6064: 100%|██████████| 42/42 [00:22<00:00,  1.85it/s]\n",
      "Val Loss: 0.5173: 100%|██████████| 11/11 [00:07<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6533 | Train F1: 0.8308 | Val Loss: 0.5089 | Val F1: 0.9150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 Best Validation F1: 0.9150\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8848: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 1.0867: 100%|██████████| 11/11 [00:08<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.0470 | Train F1: 0.3641 | Val Loss: 1.1748 | Val F1: 0.6659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7656: 100%|██████████| 42/42 [00:24<00:00,  1.72it/s]\n",
      "Val Loss: 0.6389: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.1272 | Train F1: 0.6595 | Val Loss: 0.7568 | Val F1: 0.8121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.7637: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n",
      "Val Loss: 0.4676: 100%|██████████| 11/11 [00:07<00:00,  1.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.9843 | Train F1: 0.7651 | Val Loss: 0.6213 | Val F1: 0.8551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.3555: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n",
      "Val Loss: 0.5123: 100%|██████████| 11/11 [00:08<00:00,  1.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8836 | Train F1: 0.7389 | Val Loss: 0.5934 | Val F1: 0.8611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.4551: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n",
      "Val Loss: 0.4565: 100%|██████████| 11/11 [00:08<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.8901 | Train F1: 0.7183 | Val Loss: 0.5376 | Val F1: 0.8793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.9688: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 0.4032: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.7910 | Train F1: 0.7655 | Val Loss: 0.5281 | Val F1: 0.8922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.2510: 100%|██████████| 42/42 [00:22<00:00,  1.88it/s]\n",
      "Val Loss: 0.3985: 100%|██████████| 11/11 [00:08<00:00,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7279 | Train F1: 0.7720 | Val Loss: 0.5245 | Val F1: 0.8880\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4827: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 0.4001: 100%|██████████| 11/11 [00:08<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.8831 | Train F1: 0.7560 | Val Loss: 0.5286 | Val F1: 0.9040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7061: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n",
      "Val Loss: 0.4081: 100%|██████████| 11/11 [00:08<00:00,  1.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6944 | Train F1: 0.7767 | Val Loss: 0.4950 | Val F1: 0.9215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.5342: 100%|██████████| 42/42 [00:23<00:00,  1.82it/s]\n",
      "Val Loss: 0.3808: 100%|██████████| 11/11 [00:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6772 | Train F1: 0.8295 | Val Loss: 0.5054 | Val F1: 0.9039\n",
      "Fold 3 Best Validation F1: 0.9215\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3633: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 1.1440: 100%|██████████| 11/11 [00:08<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.9597 | Train F1: 0.4262 | Val Loss: 1.1056 | Val F1: 0.7168\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.9487: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 1.0209: 100%|██████████| 11/11 [00:08<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.1805 | Train F1: 0.6562 | Val Loss: 0.7753 | Val F1: 0.8185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.4512: 100%|██████████| 42/42 [00:23<00:00,  1.78it/s]\n",
      "Val Loss: 0.7180: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.9779 | Train F1: 0.6842 | Val Loss: 0.6693 | Val F1: 0.8476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7935: 100%|██████████| 42/42 [00:23<00:00,  1.78it/s]\n",
      "Val Loss: 0.7948: 100%|██████████| 11/11 [00:08<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8791 | Train F1: 0.6862 | Val Loss: 0.6307 | Val F1: 0.8372\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4570: 100%|██████████| 42/42 [00:23<00:00,  1.76it/s]\n",
      "Val Loss: 0.5046: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.8328 | Train F1: 0.7899 | Val Loss: 0.6215 | Val F1: 0.8535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.4705: 100%|██████████| 42/42 [00:23<00:00,  1.76it/s]\n",
      "Val Loss: 0.5983: 100%|██████████| 11/11 [00:08<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.6884 | Train F1: 0.8170 | Val Loss: 0.5788 | Val F1: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 2.0723: 100%|██████████| 42/42 [00:22<00:00,  1.83it/s]\n",
      "Val Loss: 0.6947: 100%|██████████| 11/11 [00:09<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7036 | Train F1: 0.8020 | Val Loss: 0.5859 | Val F1: 0.8620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.4863: 100%|██████████| 42/42 [00:23<00:00,  1.79it/s]\n",
      "Val Loss: 0.4889: 100%|██████████| 11/11 [00:08<00:00,  1.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.7324 | Train F1: 0.7972 | Val Loss: 0.5463 | Val F1: 0.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.5835: 100%|██████████| 42/42 [00:24<00:00,  1.74it/s]\n",
      "Val Loss: 0.4947: 100%|██████████| 11/11 [00:09<00:00,  1.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6515 | Train F1: 0.8620 | Val Loss: 0.5382 | Val F1: 0.8703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7539: 100%|██████████| 42/42 [00:23<00:00,  1.79it/s]\n",
      "Val Loss: 0.5005: 100%|██████████| 11/11 [00:08<00:00,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6833 | Train F1: 0.8468 | Val Loss: 0.5333 | Val F1: 0.8827\n",
      "Fold 4 Best Validation F1: 0.9028\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.9775: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 1.2829: 100%|██████████| 11/11 [00:08<00:00,  1.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.9699 | Train F1: 0.4125 | Val Loss: 1.1509 | Val F1: 0.6657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.8652: 100%|██████████| 42/42 [00:22<00:00,  1.88it/s]\n",
      "Val Loss: 0.8114: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.1908 | Train F1: 0.6368 | Val Loss: 0.7506 | Val F1: 0.8213\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.4365: 100%|██████████| 42/42 [00:23<00:00,  1.81it/s]\n",
      "Val Loss: 0.8198: 100%|██████████| 11/11 [00:08<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 1.0288 | Train F1: 0.6517 | Val Loss: 0.6642 | Val F1: 0.8510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6836: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 0.7804: 100%|██████████| 11/11 [00:08<00:00,  1.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8263 | Train F1: 0.7790 | Val Loss: 0.6532 | Val F1: 0.8558\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.3408: 100%|██████████| 42/42 [00:23<00:00,  1.75it/s]\n",
      "Val Loss: 0.8375: 100%|██████████| 11/11 [00:08<00:00,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.9056 | Train F1: 0.6955 | Val Loss: 0.7022 | Val F1: 0.8371\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6025: 100%|██████████| 42/42 [00:23<00:00,  1.80it/s]\n",
      "Val Loss: 0.5978: 100%|██████████| 11/11 [00:08<00:00,  1.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.8071 | Train F1: 0.7841 | Val Loss: 0.5711 | Val F1: 0.8806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.7334: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n",
      "Val Loss: 0.5250: 100%|██████████| 11/11 [00:08<00:00,  1.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7729 | Train F1: 0.7995 | Val Loss: 0.5625 | Val F1: 0.8924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6484: 100%|██████████| 42/42 [00:23<00:00,  1.79it/s]\n",
      "Val Loss: 0.6134: 100%|██████████| 11/11 [00:08<00:00,  1.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.6925 | Train F1: 0.8341 | Val Loss: 0.5545 | Val F1: 0.8964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.4609: 100%|██████████| 42/42 [00:23<00:00,  1.83it/s]\n",
      "Val Loss: 0.5070: 100%|██████████| 11/11 [00:08<00:00,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.7101 | Train F1: 0.8502 | Val Loss: 0.5479 | Val F1: 0.8948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5244: 100%|██████████| 42/42 [00:23<00:00,  1.77it/s]\n",
      "Val Loss: 0.5421: 100%|██████████| 11/11 [00:07<00:00,  1.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6879 | Train F1: 0.8840 | Val Loss: 0.5508 | Val F1: 0.8926\n",
      "Fold 5 Best Validation F1: 0.8964\n",
      "\n",
      "============================================================\n",
      "K-FOLD CROSS VALIDATION RESULTS\n",
      "============================================================\n",
      "Fold 1: 0.9260\n",
      "Fold 2: 0.9150\n",
      "Fold 3: 0.9215\n",
      "Fold 4: 0.9028\n",
      "Fold 5: 0.8964\n",
      "\n",
      "Mean CV F1: 0.9123 ± 0.0112\n",
      "Best single fold: 0.9260\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# K-Fold 설정\n",
    "N_FOLDS = 5  # 5-fold로 설정 (데이터가 적으므로)\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# 클래스별 최소 샘플 보장 확인\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "#     assert len(np.unique(train_df.iloc[val_idx]['target'])) == 17\n",
    "\n",
    "# 전체 학습 데이터 로드\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# K-Fold 결과를 저장할 리스트\n",
    "fold_results = []\n",
    "fold_models = []  # 각 fold의 최고 성능 모델을 저장\n",
    "fold_class_accuracies = [] # 각 fold의 클래스별 정확도 저장\n",
    "\n",
    "print(f\"Starting {N_FOLDS}-Fold Cross Validation...\")\n",
    "\n",
    "# LR = best_params['lr']\n",
    "# BATCH_SIZE = best_params['batch_size']\n",
    "\n",
    "# K-Fold Cross Validation 시작\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    current_model = model_name\n",
    "    \n",
    "    # 현재 fold의 train/validation 데이터 분할\n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # 현재 fold의 Dataset 생성\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        # transform=trn_transform\n",
    "        epoch=0,  # 현재 epoch 전달\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        # transform=tst_transform  # 검증에는 증강 적용 안함\n",
    "        epoch=0,  # validation은 epoch 관계없음\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=False  # validation이므로 hard augmentation 비활성화\n",
    "    )\n",
    "    \n",
    "    # 현재 fold의 DataLoader 생성\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # 모델 초기화 (각 fold마다 새로운 모델)\n",
    "    model = timm.create_model(\n",
    "        current_model,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)  # Label Smoothing 적용\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler 추가\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # 현재 fold의 최고 성능 추적\n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # 현재 fold 학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        # Scheduler step 추가\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f}\")\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # Best 모델 분석\n",
    "            model.eval()\n",
    "            val_preds, val_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for image, targets in val_loader:\n",
    "                    preds = model(image.to(device)).argmax(dim=1)\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(targets.numpy())\n",
    "            \n",
    "            # 클래스별 정확도\n",
    "            fold_class_acc = {}\n",
    "            for c in range(17):\n",
    "                mask = np.array(val_targets) == c\n",
    "                if mask.sum() > 0:\n",
    "                    fold_class_acc[c] = (np.array(val_preds)[mask] == c).mean()\n",
    "    \n",
    "    # 현재 fold 결과 저장\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset)\n",
    "    })\n",
    "    \n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Best Validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    fold_class_accuracies.append(fold_class_acc) # 각 fold의 클래스별 정확도 저장\n",
    "\n",
    "# K-Fold 결과 요약\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"K-FOLD CROSS VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Best single fold: {max(val_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 3 classes:\n",
      "Class 14: 42.0%\n",
      "Class 7: 55.0%\n",
      "Class 3: 63.0%\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 성능 시각화\n",
    "meta_df = pd.read_csv(\"../data/meta.csv\")\n",
    "avg_acc = {c: np.mean([f.get(c,0) for f in fold_class_accuracies]) for c in range(17)}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "classes = list(avg_acc.keys())\n",
    "accs = [avg_acc[c] * 100 for c in classes]\n",
    "names = [f\"C{c}\" for c in classes]\n",
    "\n",
    "plt.bar(range(17), accs)\n",
    "plt.xticks(range(17), names)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Class-wise Prediction Accuracy')\n",
    "for i, acc in enumerate(accs):\n",
    "    plt.text(i, acc + 1, f'{acc:.1f}%', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Worst 3 classes:\")\n",
    "worst = sorted(avg_acc.items(), key=lambda x: x[1])[:3]\n",
    "for c, acc in worst:\n",
    "    print(f\"Class {c}: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m ensemble_models \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, state_dict \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(fold_models):\n\u001b[0;32m----> 4\u001b[0m     fold_model \u001b[38;5;241m=\u001b[39m \u001b[43mtimm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m17\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m     fold_model\u001b[38;5;241m.\u001b[39mload_state_dict(state_dict)\n\u001b[1;32m      6\u001b[0m     fold_model\u001b[38;5;241m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_factory.py:117\u001b[0m, in \u001b[0;36mcreate_model\u001b[0;34m(model_name, pretrained, pretrained_cfg, pretrained_cfg_overlay, checkpoint_path, scriptable, exportable, no_jit, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m create_fn \u001b[38;5;241m=\u001b[39m model_entrypoint(model_name)\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m set_layer_config(scriptable\u001b[38;5;241m=\u001b[39mscriptable, exportable\u001b[38;5;241m=\u001b[39mexportable, no_jit\u001b[38;5;241m=\u001b[39mno_jit):\n\u001b[0;32m--> 117\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_fn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained_cfg_overlay\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint_path:\n\u001b[1;32m    125\u001b[0m     load_checkpoint(model, checkpoint_path)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:1064\u001b[0m, in \u001b[0;36mconvnextv2_base\u001b[0;34m(pretrained, **kwargs)\u001b[0m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;129m@register_model\u001b[39m\n\u001b[1;32m   1062\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvnextv2_base\u001b[39m(pretrained\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ConvNeXt:\n\u001b[1;32m   1063\u001b[0m     model_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(depths\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m27\u001b[39m, \u001b[38;5;241m3\u001b[39m], dims\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m512\u001b[39m, \u001b[38;5;241m1024\u001b[39m], use_grn\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, ls_init_value\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1064\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_create_convnext\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mconvnextv2_base\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodel_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1065\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:486\u001b[0m, in \u001b[0;36m_create_convnext\u001b[0;34m(variant, pretrained, **kwargs)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_cfg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfcmae\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    482\u001b[0m     \u001b[38;5;66;03m# NOTE fcmae pretrained weights have no classifier or final norm-layer (`head.norm`)\u001b[39;00m\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;66;03m# This is workaround loading with num_classes=0 w/o removing norm-layer.\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39msetdefault(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpretrained_strict\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 486\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_model_with_cfg\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    487\u001b[0m \u001b[43m    \u001b[49m\u001b[43mConvNeXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvariant\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpretrained\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpretrained_filter_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheckpoint_filter_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_cfg\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflatten_sequential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/_builder.py:385\u001b[0m, in \u001b[0;36mbuild_model_with_cfg\u001b[0;34m(model_cls, variant, pretrained, pretrained_cfg, pretrained_cfg_overlay, model_cfg, feature_cfg, pretrained_strict, pretrained_filter_fn, kwargs_filter, **kwargs)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m    384\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_cfg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 385\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    386\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m     model \u001b[38;5;241m=\u001b[39m model_cls(cfg\u001b[38;5;241m=\u001b[39mmodel_cfg, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:341\u001b[0m, in \u001b[0;36mConvNeXt.__init__\u001b[0;34m(self, in_chans, num_classes, global_pool, output_stride, depths, dims, kernel_sizes, ls_init_value, stem_type, patch_size, head_init_scale, head_norm_first, head_hidden_size, conv_mlp, conv_bias, use_grn, act_layer, norm_layer, norm_eps, drop_rate, drop_path_rate)\u001b[0m\n\u001b[1;32m    339\u001b[0m first_dilation \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m dilation \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m    340\u001b[0m out_chs \u001b[38;5;241m=\u001b[39m dims[i]\n\u001b[0;32m--> 341\u001b[0m stages\u001b[38;5;241m.\u001b[39mappend(\u001b[43mConvNeXtStage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    342\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprev_chs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    343\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_chs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    344\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_sizes\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    345\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    346\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfirst_dilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    347\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdepth\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdepths\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdrop_path_rates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdp_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mls_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mls_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_grn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_grn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnorm_layer_cl\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer_cl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    357\u001b[0m prev_chs \u001b[38;5;241m=\u001b[39m out_chs\n\u001b[1;32m    358\u001b[0m \u001b[38;5;66;03m# NOTE feature_info use currently assumes stage 0 == stride 1, rest are stride 2\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:212\u001b[0m, in \u001b[0;36mConvNeXtStage.__init__\u001b[0;34m(self, in_chs, out_chs, kernel_size, stride, depth, dilation, drop_path_rates, ls_init_value, conv_mlp, conv_bias, use_grn, act_layer, norm_layer, norm_layer_cl)\u001b[0m\n\u001b[1;32m    210\u001b[0m stage_blocks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(depth):\n\u001b[0;32m--> 212\u001b[0m     stage_blocks\u001b[38;5;241m.\u001b[39mappend(\u001b[43mConvNeXtBlock\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    213\u001b[0m \u001b[43m        \u001b[49m\u001b[43min_chs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43min_chs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m        \u001b[49m\u001b[43mout_chs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout_chs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdilation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdrop_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdrop_path_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    218\u001b[0m \u001b[43m        \u001b[49m\u001b[43mls_init_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mls_init_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_mlp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_mlp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconv_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    221\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_grn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_grn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    222\u001b[0m \u001b[43m        \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnorm_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnorm_layer\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconv_mlp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnorm_layer_cl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    224\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    225\u001b[0m     in_chs \u001b[38;5;241m=\u001b[39m out_chs\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mSequential(\u001b[38;5;241m*\u001b[39mstage_blocks)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:142\u001b[0m, in \u001b[0;36mConvNeXtBlock.__init__\u001b[0;34m(self, in_chs, out_chs, kernel_size, stride, dilation, mlp_ratio, conv_mlp, conv_bias, use_grn, ls_init_value, act_layer, norm_layer, drop_path)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv_dw \u001b[38;5;241m=\u001b[39m create_conv2d(\n\u001b[1;32m    133\u001b[0m     in_chs,\n\u001b[1;32m    134\u001b[0m     out_chs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    139\u001b[0m     bias\u001b[38;5;241m=\u001b[39mconv_bias,\n\u001b[1;32m    140\u001b[0m )\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm \u001b[38;5;241m=\u001b[39m norm_layer(out_chs)\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmlp \u001b[38;5;241m=\u001b[39m \u001b[43mmlp_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_chs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmlp_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout_chs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_layer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mact_layer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mParameter(ls_init_value \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mones(out_chs)) \u001b[38;5;28;01mif\u001b[39;00m ls_init_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m in_chs \u001b[38;5;241m!=\u001b[39m out_chs \u001b[38;5;129;01mor\u001b[39;00m stride \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dilation[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m!=\u001b[39m dilation[\u001b[38;5;241m1\u001b[39m]:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/mlp.py:250\u001b[0m, in \u001b[0;36mGlobalResponseNormMlp.__init__\u001b[0;34m(self, in_features, hidden_features, out_features, act_layer, bias, drop, use_conv)\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(drop_probs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrn \u001b[38;5;241m=\u001b[39m GlobalResponseNorm(hidden_features, channels_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;129;01mnot\u001b[39;00m use_conv)\n\u001b[0;32m--> 250\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2 \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_layer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDropout(drop_probs[\u001b[38;5;241m1\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:103\u001b[0m, in \u001b[0;36mLinear.__init__\u001b[0;34m(self, in_features, out_features, bias, device, dtype)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister_parameter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m--> 103\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/linear.py:109\u001b[0m, in \u001b[0;36mLinear.reset_parameters\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mreset_parameters\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;66;03m# Setting a=sqrt(5) in kaiming_uniform is the same as initializing with\u001b[39;00m\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;66;03m# uniform(-1/sqrt(in_features), 1/sqrt(in_features)). For details, see\u001b[39;00m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;66;03m# https://github.com/pytorch/pytorch/issues/57109\u001b[39;00m\n\u001b[0;32m--> 109\u001b[0m     \u001b[43minit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkaiming_uniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    111\u001b[0m         fan_in, _ \u001b[38;5;241m=\u001b[39m init\u001b[38;5;241m.\u001b[39m_calculate_fan_in_and_fan_out(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/init.py:459\u001b[0m, in \u001b[0;36mkaiming_uniform_\u001b[0;34m(tensor, a, mode, nonlinearity, generator)\u001b[0m\n\u001b[1;32m    457\u001b[0m bound \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39msqrt(\u001b[38;5;241m3.0\u001b[39m) \u001b[38;5;241m*\u001b[39m std  \u001b[38;5;66;03m# Calculate uniform bounds from standard deviation\u001b[39;00m\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muniform_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbound\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 5-Fold 앙상블 모델 준비\n",
    "ensemble_models = []\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "print(f\"Using ensemble of all {len(ensemble_models)} fold models for inference\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling 클래스 정의\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_tta_transforms = [\n",
    "    # 원본\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90도 회전들\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 밝기 개선\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 추론을 위한 Dataset 클래스\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # 여러 transform을 리스트로 받음\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 모든 transform을 적용한 결과를 리스트로 반환\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# TTA Dataset 생성\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (배치 크기를 줄여서 메모리 절약)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTA는 메모리를 많이 사용하므로 배치 크기 줄임\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"TTA Dataset size: {len(tta_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_tta_inference(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold 모델 앙상블 + TTA 추론\"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # 각 fold 모델별 예측\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                # 각 TTA 변형별 예측\n",
    "                for images in images_list:\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ensemble TTA inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA: 100%|██████████| 50/50 [17:13<00:00, 20.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 TTA 실행\n",
    "print(\"Starting Ensemble TTA inference...\")\n",
    "tta_predictions = ensemble_tta_inference(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 결과로 submission 파일 생성\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 submission과 동일한 순서인지 확인\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA predictions saved\n",
      "TTA Prediction sample:\n"
     ]
    }
   ],
   "source": [
    "# TTA 결과 저장\n",
    "tta_pred_df.to_csv(\"../submission/choice.csv\", index=False)\n",
    "print(\"TTA predictions saved\")\n",
    "\n",
    "print(\"TTA Prediction sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1700315247734,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg      12\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
