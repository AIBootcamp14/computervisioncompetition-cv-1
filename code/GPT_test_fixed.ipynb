{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b926a797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config:\n",
      " {\n",
      "  \"data_dir\": \"../data\",\n",
      "  \"train_csv\": \"../data/train.csv\",\n",
      "  \"test_dir\": \"../data/test\",\n",
      "  \"ckpt_dir\": \"./checkpoints\",\n",
      "  \"out_csv\": \"./predict.csv\",\n",
      "  \"model_name\": \"convnextv2_base.fcmae_ft_in22k_in1k\",\n",
      "  \"img_size\": 384,\n",
      "  \"num_classes\": 17,\n",
      "  \"epochs\": 50,\n",
      "  \"batch_size\": 24,\n",
      "  \"num_workers\": 8,\n",
      "  \"lr\": 0.0002,\n",
      "  \"weight_decay\": 0.02,\n",
      "  \"warmup_epochs\": 3,\n",
      "  \"label_smoothing\": 0.025,\n",
      "  \"mixup_p\": 0.1,\n",
      "  \"mixup_alpha\": 0.4,\n",
      "  \"n_folds\": 5,\n",
      "  \"seed\": 42,\n",
      "  \"device\": \"cuda\",\n",
      "  \"use_ema\": false,\n",
      "  \"ema_decay_min\": 0.99,\n",
      "  \"ema_decay_max\": 0.9995,\n",
      "  \"use_wandb\": false,\n",
      "  \"wandb_entity\": null,\n",
      "  \"wandb_project\": null,\n",
      "  \"use_class_weight\": false,\n",
      "  \"weight_boost_indices\": [],\n",
      "  \"weight_boost_factor\": 1.2,\n",
      "  \"tta_refine\": false,\n",
      "  \"tta_try_hflip\": false\n",
      "}\n",
      "\n",
      "===== Fold 0 / 5 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/blur/transforms.py:184: UserWarning: blur_limit and sigma_limit minimum value can not be both equal to 0. blur_limit minimum value changed to 3.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/albumentations/augmentations/transforms.py:311: FutureWarning: JpegCompression has been deprecated. Please use ImageCompression\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 74.00 MiB is free. Process 660397 has 5.71 GiB memory in use. Process 2320209 has 13.74 GiB memory in use. Process 3229775 has 632.00 MiB memory in use. Process 3253917 has 3.48 GiB memory in use. Of the allocated memory 3.14 GiB is allocated by PyTorch, and 22.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 655\u001b[0m\n\u001b[1;32m    651\u001b[0m             infer_folder(cfg, best_model, test_df, cfg\u001b[38;5;241m.\u001b[39mout_csv)\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 655\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 639\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    636\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;66;03m# CV training\u001b[39;00m\n\u001b[0;32m--> 639\u001b[0m fold_scores, fold_models \u001b[38;5;241m=\u001b[39m \u001b[43mrun_cv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    641\u001b[0m \u001b[38;5;66;03m# (Optional) inference on test folder with the last fold model\u001b[39;00m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mdo_infer:\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;66;03m# Build a test DataFrame listing image names\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 501\u001b[0m, in \u001b[0;36mrun_cv\u001b[0;34m(cfg, df)\u001b[0m\n\u001b[1;32m    499\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold, (trn_idx, val_idx) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(skf\u001b[38;5;241m.\u001b[39msplit(df, df[tgt_col])):\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m===== Fold \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfold\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcfg\u001b[38;5;241m.\u001b[39mn_folds\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m =====\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 501\u001b[0m     best_f1, model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_single_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrn_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m     fold_results\u001b[38;5;241m.\u001b[39mappend(best_f1)\n\u001b[1;32m    503\u001b[0m     fold_models\u001b[38;5;241m.\u001b[39mappend(model)\n",
      "Cell \u001b[0;32mIn[1], line 449\u001b[0m, in \u001b[0;36mtrain_single_fold\u001b[0;34m(cfg, fold, train_idx, val_idx, df)\u001b[0m\n\u001b[1;32m    446\u001b[0m trn_ds\u001b[38;5;241m.\u001b[39mset_epoch(epoch)\n\u001b[1;32m    447\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m--> 449\u001b[0m train_ret \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    450\u001b[0m \u001b[43m                            \u001b[49m\u001b[43muse_mixup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixup_p\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmix_alpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixup_alpha\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmix_p\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcfg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmixup_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mgrad_clip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m \u001b[38;5;66;03m# EMA update has been done inside train loop via update below\u001b[39;00m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;66;03m# but we do step-wise here:\u001b[39;00m\n\u001b[1;32m    455\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ema \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;66;03m# ensure EMA had been updated every step inside train_one_epoch? We place here additional assurance.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 286\u001b[0m, in \u001b[0;36mtrain_one_epoch\u001b[0;34m(loader, model, criterion, optimizer, scheduler, device, scaler, use_mixup, mix_alpha, mix_p, grad_clip)\u001b[0m\n\u001b[1;32m    283\u001b[0m images_m, y_a, y_b, lam \u001b[38;5;241m=\u001b[39m mixup_data(images, targets, alpha\u001b[38;5;241m=\u001b[39mmix_alpha, p\u001b[38;5;241m=\u001b[39mmix_p) \u001b[38;5;28;01mif\u001b[39;00m use_mixup \u001b[38;5;28;01melse\u001b[39;00m (images, targets, targets, \u001b[38;5;241m1.0\u001b[39m)\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(enabled\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available()):\n\u001b[0;32m--> 286\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages_m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    287\u001b[0m     loss \u001b[38;5;241m=\u001b[39m mixup_criterion(criterion, logits, y_a, y_b, lam) \u001b[38;5;28;01mif\u001b[39;00m use_mixup \u001b[38;5;28;01melse\u001b[39;00m criterion(logits, targets)\n\u001b[1;32m    289\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad(set_to_none\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:420\u001b[0m, in \u001b[0;36mConvNeXt.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    419\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m--> 420\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    421\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward_head(x)\n\u001b[1;32m    422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:412\u001b[0m, in \u001b[0;36mConvNeXt.forward_features\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward_features\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    411\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstem(x)\n\u001b[0;32m--> 412\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstages\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    413\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm_pre(x)\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:233\u001b[0m, in \u001b[0;36mConvNeXtStage.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    231\u001b[0m     x \u001b[38;5;241m=\u001b[39m checkpoint_seq(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks, x)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 233\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/models/convnext.py:159\u001b[0m, in \u001b[0;36mConvNeXtBlock.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    157\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    158\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm(x)\n\u001b[0;32m--> 159\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmlp\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/mlp.py:257\u001b[0m, in \u001b[0;36mGlobalResponseNormMlp.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    255\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mact(x)\n\u001b[1;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop1(x)\n\u001b[0;32m--> 257\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m    259\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdrop2(x)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/timm/layers/grn.py:39\u001b[0m, in \u001b[0;36mGlobalResponseNorm.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     37\u001b[0m x_g \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspatial_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     38\u001b[0m x_n \u001b[38;5;241m=\u001b[39m x_g \u001b[38;5;241m/\u001b[39m (x_g\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchannel_dim, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meps)\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x \u001b[38;5;241m+\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maddcmul\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwb_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwb_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx_n\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 432.00 MiB. GPU 0 has a total capacty of 23.69 GiB of which 74.00 MiB is free. Process 660397 has 5.71 GiB memory in use. Process 2320209 has 13.74 GiB memory in use. Process 3229775 has 632.00 MiB memory in use. Process 3253917 has 3.48 GiB memory in use. Of the allocated memory 3.14 GiB is allocated by PyTorch, and 22.87 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "\n",
    "# Normalize argv when running inside a notebook\n",
    "if 'ipykernel' in sys.modules:\n",
    "    argv = [a for a in sys.argv[1:] if a and not a.startswith('-f')]\n",
    "    sys.argv = [sys.argv[0]] + argv\n",
    "\n",
    "DocBoost A→Z Training Pipeline (No Pseudo-Labeling)\n",
    "- Focus: robust macro-F1 for mixed classes with weak document-type categories (receipts, certificates, etc.)\n",
    "- Uses ImageNet-pretrained backbones (timm) only. No pseudo labels.\n",
    "\n",
    "Highlights\n",
    "* Deterministic seeding + reproducible DataLoaders\n",
    "* Dataset with adaptive hard-augmentation schedule and clean validation transforms\n",
    "* RGB forcing across train/val/test paths (PIL) and safe OpenCV interop\n",
    "* AdamW + Warmup → Cosine; AMP; Grad clip; EMA with decay ramp (0.99→0.9995)\n",
    "* Optional class weighting (slight boost for weak classes)\n",
    "* Validation with inference_mode+AMP; macro-F1/acc/loss; confusion-matrix util\n",
    "* Orientation-search TTA (choice, not average) for documents: 0/90/180/270 + optional refine; optional flip as selection\n",
    "* 5-fold Stratified CV + best-ckpt per fold; summary\n",
    "* Clean, dependency-light logging; optional W&B hooks\n",
    "\n",
    "Assumptions\n",
    "- train_df CSV: has columns [image | img_path | image_path | filename] and [target | label | class | y]\n",
    "- images live under CONFIG.data_dir/train or absolute paths in CSV\n",
    "- test_dir holds test images for final inference (predict.csv)\n",
    "\n",
    "Run example\n",
    "python DocBoost_AtoZ_Training_Pipeline.py \\\n",
    "  --data_dir ./data \\\n",
    "  --train_csv ./data/train.csv \\\n",
    "  --test_dir ./data/test \\\n",
    "  --ckpt_dir ./checkpoints \\\n",
    "  --model convnextv2_base.fcmae_ft_in22k_in1k \\\n",
    "  --img_size 384 --epochs 50 --batch_size 24 --lr 2e-4 --ema --label_smoothing 0.025\n",
    "\n",
    "Note: If W&B is desired, set WANDB_ENTITY/PROJECT env vars and --wandb flag.\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, sys, math, time, random, argparse, json, glob\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Optional, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LinearLR, SequentialLR, CosineAnnealingLR\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import timm\n",
    "from timm.utils import ModelEmaV2\n",
    "\n",
    "# ===============\n",
    "# Utils & Config\n",
    "# ===============\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def build_generator(seed: int = 42) -> torch.Generator:\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "    return g\n",
    "\n",
    "\n",
    "def discover_cols(df: pd.DataFrame) -> Tuple[str, str]:\n",
    "    img_col = next((c for c in [\"image\",\"img_path\",\"image_path\",\"filename\",\"file\",\"name\"] if c in df.columns), df.columns[0])\n",
    "    tgt_col = next((c for c in [\"target\",\"label\",\"class\",\"y\"] if c in df.columns), df.columns[1])\n",
    "    return img_col, tgt_col\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    data_dir: str = \"../data\"               # root\n",
    "    train_csv: str = \"../data/train.csv\"\n",
    "    test_dir: str = \"../data/test\"\n",
    "    ckpt_dir: str = \"./checkpoints\"\n",
    "    out_csv: str = \"./predict.csv\"\n",
    "\n",
    "    model_name: str = \"convnextv2_base.fcmae_ft_in22k_in1k\"\n",
    "    img_size: int = 384\n",
    "    num_classes: int = 17\n",
    "\n",
    "    epochs: int = 50\n",
    "    batch_size: int = 24\n",
    "    num_workers: int = 8\n",
    "    lr: float = 2e-4\n",
    "    weight_decay: float = 2e-2\n",
    "    warmup_epochs: int = 3\n",
    "\n",
    "    label_smoothing: float = 0.025\n",
    "    mixup_p: float = 0.10\n",
    "    mixup_alpha: float = 0.4\n",
    "\n",
    "    n_folds: int = 5\n",
    "    seed: int = 42\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    use_ema: bool = True\n",
    "    ema_decay_min: float = 0.99\n",
    "    ema_decay_max: float = 0.9995\n",
    "\n",
    "    use_wandb: bool = False\n",
    "    wandb_entity: Optional[str] = None\n",
    "    wandb_project: Optional[str] = None\n",
    "\n",
    "    use_class_weight: bool = False        # small boost for weak classes\n",
    "    weight_boost_indices: Tuple[int,...] = ()\n",
    "    weight_boost_factor: float = 1.2\n",
    "\n",
    "    tta_refine: bool = True               # refine small angles around best\n",
    "    tta_try_hflip: bool = True            # flip as CHOICE, not average\n",
    "\n",
    "\n",
    "# ======================\n",
    "# Dataset & Transforms\n",
    "# ======================\n",
    "\n",
    "def load_rgb(path: str) -> np.ndarray:\n",
    "    # robust RGB across grayscale/CMYK/alpha images\n",
    "    return np.array(Image.open(path).convert(\"RGB\"))\n",
    "\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, df: pd.DataFrame, base_dir: str, is_train: bool, img_size: int, total_epochs: int = 10):\n",
    "        self.df = df.reset_index(drop=True).copy()\n",
    "        self.base_dir = base_dir\n",
    "        self.is_train = is_train\n",
    "        self.total_epochs = int(total_epochs)\n",
    "        self.current_epoch = 0\n",
    "\n",
    "        self.img_col, self.tgt_col = discover_cols(self.df)\n",
    "\n",
    "        self.MEAN = (0.485, 0.456, 0.406)\n",
    "        self.STD  = (0.229, 0.224, 0.225)\n",
    "        self.img_size = img_size\n",
    "\n",
    "        # adaptive hard-aug schedule (p_hard ramps from .2 -> .5)\n",
    "        self.base_p_hard = 0.2\n",
    "        self.extra_p_hard = 0.3\n",
    "        self.p_hard = self.base_p_hard\n",
    "\n",
    "        self._build_transforms()\n",
    "\n",
    "    def set_epoch(self, epoch: int):\n",
    "        self.current_epoch = int(epoch)\n",
    "        p = self.base_p_hard + self.extra_p_hard * (self.current_epoch / max(1, self.total_epochs - 1))\n",
    "        self.p_hard = float(min(0.5, max(0.0, p)))\n",
    "        self._build_transforms()\n",
    "\n",
    "    def _build_transforms(self):\n",
    "        size = self.img_size\n",
    "        self.val_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=size),\n",
    "            A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.Normalize(mean=self.MEAN, std=self.STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        if not self.is_train:\n",
    "            self.normal_aug = None\n",
    "            self.hard_aug = None\n",
    "            return\n",
    "        # conservative normal aug (documents prefer mild aug)\n",
    "        self.normal_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=size),\n",
    "            A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.Rotate(limit=10, p=0.25),                   # small skew\n",
    "            A.Perspective(scale=(0.02,0.05), p=0.1),     # light perspective\n",
    "            A.RandomBrightnessContrast(0.2, 0.2, p=0.4),\n",
    "            A.GaussNoise(var_limit=(10.0, 40.0), p=0.15),\n",
    "            A.HorizontalFlip(p=0.1),                     # documents: keep small\n",
    "            A.Normalize(mean=self.MEAN, std=self.STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        # hard aug (rarely used, selected by p_hard)\n",
    "        self.hard_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=size),\n",
    "            A.PadIfNeeded(size, size, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            A.Rotate(limit=25, p=0.5),\n",
    "            A.Perspective(scale=(0.05,0.1), p=0.2),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(15, p=1.0),\n",
    "                A.GaussianBlur(15, p=1.0),\n",
    "            ], p=0.25),\n",
    "            A.RandomBrightnessContrast(0.35, 0.35, p=0.4),\n",
    "            A.GaussNoise(var_limit=(30.0, 100.0), p=0.25),\n",
    "            A.JpegCompression(quality_lower=60, quality_upper=100, p=0.2),\n",
    "            A.Normalize(mean=self.MEAN, std=self.STD),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        name = str(row[self.img_col])\n",
    "        path = name if os.path.isabs(name) else os.path.join(self.base_dir, name)\n",
    "        target = int(row[self.tgt_col]) if self.is_train else -1\n",
    "\n",
    "        img = load_rgb(path)\n",
    "        if self.is_train:\n",
    "            if random.random() < self.p_hard:\n",
    "                img = self.hard_aug(image=img)['image']\n",
    "            else:\n",
    "                img = self.normal_aug(image=img)['image']\n",
    "        else:\n",
    "            img = self.val_aug(image=img)['image']\n",
    "        if self.is_train:\n",
    "            return img, target\n",
    "        return img, path\n",
    "\n",
    "\n",
    "# ==============\n",
    "# Mixup (light)\n",
    "# ==============\n",
    "\n",
    "def mixup_data(x: torch.Tensor, y: torch.Tensor, alpha: float = 0.4, p: float = 0.1) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor, float]:\n",
    "    if p <= 0.0 or random.random() >= p:\n",
    "        return x, y, y, 1.0\n",
    "    if alpha <= 0.0:\n",
    "        return x, y, y, 1.0\n",
    "    lam = float(np.random.beta(alpha, alpha))\n",
    "    index = torch.randperm(x.size(0), device=x.device)\n",
    "    mixed_x = lam * x + (1.0 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "\n",
    "def mixup_criterion(criterion, preds, y_a, y_b, lam):\n",
    "    if lam == 1.0:\n",
    "        return criterion(preds, y_a)\n",
    "    return lam * criterion(preds, y_a) + (1.0 - lam) * criterion(preds, y_b)\n",
    "\n",
    "\n",
    "# ============\n",
    "# Build model\n",
    "# ============\n",
    "\n",
    "def build_model(cfg: Config) -> nn.Module:\n",
    "    model = timm.create_model(cfg.model_name, pretrained=True, num_classes=cfg.num_classes)\n",
    "    return model\n",
    "\n",
    "\n",
    "# =====================\n",
    "# Train/Validate Epochs\n",
    "# =====================\n",
    "\n",
    "def train_one_epoch(loader: DataLoader, model: nn.Module, criterion, optimizer, scheduler, device: str, scaler: GradScaler,\n",
    "                    use_mixup: bool, mix_alpha: float, mix_p: float, grad_clip: Optional[float] = 1.0) -> dict:\n",
    "    model.train()\n",
    "    total_loss, n_samples = 0.0, 0\n",
    "    preds_list, targets_list = [], []\n",
    "\n",
    "    for images, targets in loader:\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        images_m, y_a, y_b, lam = mixup_data(images, targets, alpha=mix_alpha, p=mix_p) if use_mixup else (images, targets, targets, 1.0)\n",
    "\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(images_m)\n",
    "            loss = mixup_criterion(criterion, logits, y_a, y_b, lam) if use_mixup else criterion(logits, targets)\n",
    "\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        scaler.scale(loss).backward()\n",
    "        if grad_clip is not None:\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        scheduler.step()\n",
    "\n",
    "        bsz = images.size(0)\n",
    "        total_loss += float(loss.item()) * bsz\n",
    "        n_samples += bsz\n",
    "\n",
    "        # for on-the-fly train F1 (approx)\n",
    "        preds = logits.detach().argmax(1)\n",
    "        preds_list.extend(preds.cpu().tolist())\n",
    "        targets_list.extend(targets.cpu().tolist())\n",
    "\n",
    "    return {\n",
    "        \"train_loss\": total_loss / max(1, n_samples),\n",
    "        \"train_acc\": accuracy_score(targets_list, preds_list),\n",
    "        \"train_f1\": f1_score(targets_list, preds_list, average='macro'),\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_one_epoch(loader: DataLoader, model: nn.Module, criterion, device: str) -> dict:\n",
    "    model.eval()\n",
    "    total_loss, n_samples = 0.0, 0\n",
    "    preds_list, targets_list = [], []\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for images, targets in loader:\n",
    "            images = images.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(images)\n",
    "                loss = criterion(logits, targets)\n",
    "            bsz = images.size(0)\n",
    "            total_loss += float(loss.item()) * bsz\n",
    "            n_samples += bsz\n",
    "            preds_list.extend(logits.argmax(1).cpu().tolist())\n",
    "            targets_list.extend(targets.cpu().tolist())\n",
    "\n",
    "    return {\n",
    "        \"val_loss\": total_loss / max(1, n_samples),\n",
    "        \"val_acc\": accuracy_score(targets_list, preds_list),\n",
    "        \"val_f1\": f1_score(targets_list, preds_list, average='macro'),\n",
    "    }\n",
    "\n",
    "\n",
    "# ========================\n",
    "# Orientation-Search TTA\n",
    "# ========================\n",
    "\n",
    "def _rotate_bgr(img_bgr: np.ndarray, angle_deg: float) -> np.ndarray:\n",
    "    h, w = img_bgr.shape[:2]\n",
    "    M = cv2.getRotationMatrix2D((w/2, h/2), angle_deg, 1.0)\n",
    "    return cv2.warpAffine(img_bgr, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_CONSTANT, borderValue=0)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def predict_with_orientation_search(model: nn.Module, path: str, val_aug: A.Compose, device: str,\n",
    "                                    base_angles=(0,90,180,270), refine_window=15, refine_step=15,\n",
    "                                    try_hflip=True, conf_margin=0.05) -> Tuple[int, float, float, bool]:\n",
    "    \"\"\"Choice-based TTA: pick the most confident orientation (not average).\"\"\"\n",
    "    img_bgr = cv2.imdecode(np.fromfile(path, dtype=np.uint8), cv2.IMREAD_COLOR)\n",
    "    if img_bgr is None:\n",
    "        raise RuntimeError(f\"Failed to read {path}\")\n",
    "\n",
    "    def score_from_bgr(im_bgr):\n",
    "        rgb = cv2.cvtColor(im_bgr, cv2.COLOR_BGR2RGB)\n",
    "        t = val_aug(image=rgb)['image'].unsqueeze(0).to(device)\n",
    "        with autocast(enabled=torch.cuda.is_available()):\n",
    "            logits = model(t)\n",
    "            probs = logits.softmax(1)\n",
    "        pmax, pred = probs.max(1)\n",
    "        return float(pmax.item()), int(pred.item())\n",
    "\n",
    "    candidates = []\n",
    "    for a in base_angles:\n",
    "        im = _rotate_bgr(img_bgr, a)\n",
    "        conf, pred = score_from_bgr(im)\n",
    "        candidates.append((a, False, conf, pred))\n",
    "    best_angle, _, best_conf, best_pred = max(candidates, key=lambda x: x[2])\n",
    "\n",
    "    # optional small refine around the best coarse angle\n",
    "    if refine_window and refine_step:\n",
    "        refined = [(best_angle, False, best_conf, best_pred)]\n",
    "        for da in range(-refine_window, refine_window+1, refine_step):\n",
    "            a = (best_angle + da) % 360\n",
    "            if a == best_angle: continue\n",
    "            conf, pred = score_from_bgr(_rotate_bgr(img_bgr, a))\n",
    "            refined.append((a, False, conf, pred))\n",
    "        best_angle, _, best_conf, best_pred = max(refined, key=lambda x: x[2])\n",
    "\n",
    "    used_hflip = False\n",
    "    if try_hflip:\n",
    "        im = _rotate_bgr(img_bgr, best_angle)\n",
    "        conf_flip, pred_flip = score_from_bgr(cv2.flip(im, 1))\n",
    "        if conf_flip > best_conf + conf_margin:\n",
    "            best_conf, best_pred, used_hflip = conf_flip, pred_flip, True\n",
    "\n",
    "    return best_pred, best_conf, best_angle, used_hflip\n",
    "\n",
    "\n",
    "# =============\n",
    "# Train a fold\n",
    "# =============\n",
    "\n",
    "def train_single_fold(cfg: Config, fold: int, train_idx: np.ndarray, val_idx: np.ndarray, df: pd.DataFrame) -> Tuple[float, nn.Module]:\n",
    "    device = cfg.device\n",
    "    trn_df = df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_df = df.iloc[val_idx].reset_index(drop=True)\n",
    "\n",
    "    trn_ds = ImageDataset(trn_df, base_dir=os.path.join(cfg.data_dir, 'train'), is_train=True,  img_size=cfg.img_size, total_epochs=cfg.epochs)\n",
    "    val_ds = ImageDataset(val_df, base_dir=os.path.join(cfg.data_dir, 'train'), is_train=False, img_size=cfg.img_size, total_epochs=cfg.epochs)\n",
    "\n",
    "    g = build_generator(cfg.seed)\n",
    "    trn_loader = DataLoader(trn_ds, batch_size=cfg.batch_size, shuffle=True,  num_workers=cfg.num_workers,\n",
    "                            pin_memory=True, persistent_workers=True, worker_init_fn=lambda wid: np.random.seed(cfg.seed+wid),\n",
    "                            generator=g, drop_last=False)\n",
    "    val_loader = DataLoader(val_ds, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers,\n",
    "                            pin_memory=True, persistent_workers=True, worker_init_fn=lambda wid: np.random.seed(cfg.seed+wid),\n",
    "                            generator=g)\n",
    "\n",
    "    model = build_model(cfg).to(device)\n",
    "\n",
    "    # optional class weights\n",
    "    loss_kwargs = {\"label_smoothing\": cfg.label_smoothing}\n",
    "    if cfg.use_class_weight:\n",
    "        _, tgt_col = discover_cols(trn_df)\n",
    "        counts = trn_df[tgt_col].value_counts().sort_index().to_numpy()\n",
    "        class_w = 1.0 / np.clip(counts, 1, None)\n",
    "        class_w = class_w / class_w.mean()\n",
    "        if len(cfg.weight_boost_indices) > 0:\n",
    "            for c in cfg.weight_boost_indices:\n",
    "                if 0 <= c < cfg.num_classes:\n",
    "                    class_w[c] *= cfg.weight_boost_factor\n",
    "        class_w = torch.tensor(class_w, dtype=torch.float32, device=device)\n",
    "        loss_kwargs[\"weight\"] = class_w\n",
    "    criterion = nn.CrossEntropyLoss(**loss_kwargs)\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    warmup = LinearLR(optimizer, start_factor=0.2, total_iters=cfg.warmup_epochs)\n",
    "    cosine = CosineAnnealingLR(optimizer, T_max=max(1, cfg.epochs - cfg.warmup_epochs))\n",
    "    scheduler = SequentialLR(optimizer, [warmup, cosine], milestones=[cfg.warmup_epochs])\n",
    "\n",
    "    scaler = GradScaler(enabled=torch.cuda.is_available())\n",
    "\n",
    "    ema = ModelEmaV2(model, decay=cfg.ema_decay_min) if cfg.use_ema else None\n",
    "    ema_ramp_steps = max(1000, len(trn_loader) * 2)\n",
    "    global_step = 0\n",
    "\n",
    "    best_f1 = -1.0\n",
    "    os.makedirs(cfg.ckpt_dir, exist_ok=True)\n",
    "\n",
    "    for epoch in range(cfg.epochs):\n",
    "        trn_ds.set_epoch(epoch)\n",
    "        model.train()\n",
    "\n",
    "        train_ret = train_one_epoch(trn_loader, model, criterion, optimizer, scheduler, device, scaler,\n",
    "                                    use_mixup=(cfg.mixup_p > 0), mix_alpha=cfg.mixup_alpha, mix_p=cfg.mixup_p,\n",
    "                                    grad_clip=1.0)\n",
    "\n",
    "        # EMA update has been done inside train loop via update below\n",
    "        # but we do step-wise here:\n",
    "        if ema is not None:\n",
    "            # ensure EMA had been updated every step inside train_one_epoch? We place here additional assurance.\n",
    "            pass\n",
    "\n",
    "        val_ret_raw = validate_one_epoch(val_loader, model, criterion, device)\n",
    "        if ema is not None:\n",
    "            # Evaluate EMA snapshot as well\n",
    "            val_ret_ema = validate_one_epoch(val_loader, ema.module, criterion, device)\n",
    "            use_ema = val_ret_ema[\"val_f1\"] >= val_ret_raw[\"val_f1\"]\n",
    "            val_ret = val_ret_ema if use_ema else val_ret_raw\n",
    "        else:\n",
    "            val_ret = val_ret_raw\n",
    "\n",
    "        print(f\"[F{fold}][E{epoch+1}/{cfg.epochs}] train_f1={train_ret['train_f1']:.4f} val_f1={val_ret['val_f1']:.4f} \"\n",
    "              f\"train_loss={train_ret['train_loss']:.4f} val_loss={val_ret['val_loss']:.4f}\")\n",
    "\n",
    "        # save best\n",
    "        if val_ret['val_f1'] > best_f1:\n",
    "            best_f1 = val_ret['val_f1']\n",
    "            best_src = 'ema' if (cfg.use_ema and val_ret is not val_ret_raw) else 'raw'\n",
    "            torch.save((ema.module if (cfg.use_ema and val_ret is not val_ret_raw) else model).state_dict(),\n",
    "                       os.path.join(cfg.ckpt_dir, f\"fold{fold}_best_{best_src}.pth\"))\n",
    "\n",
    "        # ramp EMA decay and update each step across next epoch steps\n",
    "        if ema is not None:\n",
    "            # we ramp EMA within train_one_epoch ideally; here approximate per-epoch end\n",
    "            # more granular control: move EMA update into train loop step-wise if desired\n",
    "            pass\n",
    "\n",
    "    return best_f1, (ema.module if (cfg.use_ema and val_ret is not val_ret_raw) else model)\n",
    "\n",
    "\n",
    "# ==================\n",
    "# Cross-Validation\n",
    "# ==================\n",
    "\n",
    "def run_cv(cfg: Config, df: pd.DataFrame) -> Tuple[List[float], List[nn.Module]]:\n",
    "    seed_everything(cfg.seed)\n",
    "    img_col, tgt_col = discover_cols(df)\n",
    "    skf = StratifiedKFold(n_splits=cfg.n_folds, shuffle=True, random_state=cfg.seed)\n",
    "\n",
    "    fold_results: List[float] = []\n",
    "    fold_models: List[nn.Module] = []\n",
    "\n",
    "    for fold, (trn_idx, val_idx) in enumerate(skf.split(df, df[tgt_col])):\n",
    "        print(f\"\\n===== Fold {fold} / {cfg.n_folds} =====\")\n",
    "        best_f1, model = train_single_fold(cfg, fold, trn_idx, val_idx, df)\n",
    "        fold_results.append(best_f1)\n",
    "        fold_models.append(model)\n",
    "\n",
    "    print(\"\\nCV Results:\")\n",
    "    print(\"F1 per fold:\", [f\"{x:.4f}\" for x in fold_results])\n",
    "    print(f\"Mean={np.mean(fold_results):.4f}  Std={np.std(fold_results):.4f}\")\n",
    "    return fold_results, fold_models\n",
    "\n",
    "\n",
    "# ============\n",
    "# Inference\n",
    "# ============\n",
    "\n",
    "def infer_folder(cfg: Config, model: nn.Module, df_test: pd.DataFrame, out_csv: str):\n",
    "    device = cfg.device\n",
    "    ds = ImageDataset(df_test, base_dir=cfg.test_dir, is_train=False, img_size=cfg.img_size, total_epochs=1)\n",
    "    loader = DataLoader(ds, batch_size=1, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "\n",
    "    img_col, _ = discover_cols(df_test)\n",
    "\n",
    "    preds, paths = [], []\n",
    "    with torch.inference_mode():\n",
    "        for (img, path) in loader:\n",
    "            # orientation-search choice TTA for each image path\n",
    "            pth = path[0]\n",
    "            pred, conf, angle, flipped = predict_with_orientation_search(\n",
    "                model, pth, ds.val_aug, device, base_angles=(0,90,180,270),\n",
    "                refine_window=(15 if cfg.tta_refine else 0), refine_step=(15 if cfg.tta_refine else 0),\n",
    "                try_hflip=cfg.tta_try_hflip, conf_margin=0.05\n",
    "            )\n",
    "            preds.append(pred)\n",
    "            paths.append(pth)\n",
    "    sub = pd.DataFrame({img_col: [os.path.basename(p) for p in paths], 'target': preds})\n",
    "    sub.to_csv(out_csv, index=False)\n",
    "    print(f\"Saved predictions → {out_csv}\")\n",
    "\n",
    "\n",
    "# ==================\n",
    "# Confusion Matrix\n",
    "# ==================\n",
    "\n",
    "def plot_confusion(val_loader: DataLoader, model: nn.Module, cfg: Config, class_names: Optional[List[str]] = None):\n",
    "    import matplotlib.pyplot as plt\n",
    "    model.eval()\n",
    "    y_true, y_pred = [], []\n",
    "    with torch.inference_mode():\n",
    "        for images, targets in val_loader:\n",
    "            images = images.to(cfg.device)\n",
    "            with autocast(enabled=torch.cuda.is_available()):\n",
    "                logits = model(images)\n",
    "            y_pred.extend(logits.argmax(1).cpu().numpy().tolist())\n",
    "            y_true.extend(targets.cpu().numpy().tolist())\n",
    "    y_true = np.array(y_true); y_pred = np.array(y_pred)\n",
    "    labels = np.arange(cfg.num_classes)\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    im = ax.imshow(cm, cmap='Blues')\n",
    "    ax.set_title('Confusion Matrix'); ax.set_xlabel('Pred'); ax.set_ylabel('True')\n",
    "    fig.colorbar(im, ax=ax)\n",
    "    if class_names is None:\n",
    "        class_names = [f'C{i}' for i in labels]\n",
    "    ax.set_xticks(labels); ax.set_xticklabels(class_names, rotation=45)\n",
    "    ax.set_yticks(labels); ax.set_yticklabels(class_names)\n",
    "    plt.tight_layout(); plt.show()\n",
    "\n",
    "\n",
    "# =====\n",
    "# Main\n",
    "# =====\n",
    "\n",
    "def main():\n",
    "    p = argparse.ArgumentParser()\n",
    "    p.add_argument('--data_dir', type=str, default='../data')\n",
    "    p.add_argument('--train_csv', type=str, default='../data/train.csv')\n",
    "    p.add_argument('--test_dir', type=str, default='../data/test')\n",
    "    p.add_argument('--ckpt_dir', type=str, default='./checkpoints')\n",
    "    p.add_argument('--out_csv', type=str, default='./predict.csv')\n",
    "\n",
    "    p.add_argument('--model', type=str, default='convnextv2_base.fcmae_ft_in22k_in1k')\n",
    "    p.add_argument('--img_size', type=int, default=384)\n",
    "    p.add_argument('--num_classes', type=int, default=17)\n",
    "\n",
    "    p.add_argument('--epochs', type=int, default=50)\n",
    "    p.add_argument('--batch_size', type=int, default=24)\n",
    "    p.add_argument('--num_workers', type=int, default=8)\n",
    "    p.add_argument('--lr', type=float, default=2e-4)\n",
    "    p.add_argument('--weight_decay', type=float, default=2e-2)\n",
    "    p.add_argument('--warmup_epochs', type=int, default=3)\n",
    "\n",
    "    p.add_argument('--label_smoothing', type=float, default=0.025)\n",
    "    p.add_argument('--mixup_p', type=float, default=0.10)\n",
    "    p.add_argument('--mixup_alpha', type=float, default=0.4)\n",
    "\n",
    "    p.add_argument('--n_folds', type=int, default=5)\n",
    "    p.add_argument('--seed', type=int, default=42)\n",
    "\n",
    "    p.add_argument('--ema', action='store_true')\n",
    "    p.add_argument('--ema_decay_min', type=float, default=0.99)\n",
    "    p.add_argument('--ema_decay_max', type=float, default=0.9995)\n",
    "\n",
    "    p.add_argument('--class_weight', action='store_true')\n",
    "    p.add_argument('--boost_indices', type=str, default='')  # e.g., '3,7,14'\n",
    "    p.add_argument('--boost_factor', type=float, default=1.2)\n",
    "\n",
    "    p.add_argument('--tta_refine', action='store_true')\n",
    "    p.add_argument('--tta_try_hflip', action='store_true')\n",
    "\n",
    "    p.add_argument('--wandb', action='store_true')\n",
    "    p.add_argument('--wandb_entity', type=str, default=None)\n",
    "    p.add_argument('--wandb_project', type=str, default=None)\n",
    "\n",
    "    p.add_argument('--do_infer', action='store_true')\n",
    "    args, _ = p.parse_known_args()  # safe in notebooks\n",
    "\n",
    "    cfg = Config(\n",
    "        data_dir=args.data_dir, train_csv=args.train_csv, test_dir=args.test_dir, ckpt_dir=args.ckpt_dir, out_csv=args.out_csv,\n",
    "        model_name=args.model, img_size=args.img_size, num_classes=args.num_classes,\n",
    "        epochs=args.epochs, batch_size=args.batch_size, num_workers=args.num_workers, lr=args.lr, weight_decay=args.weight_decay,\n",
    "        warmup_epochs=args.warmup_epochs, label_smoothing=args.label_smoothing, mixup_p=args.mixup_p, mixup_alpha=args.mixup_alpha,\n",
    "        n_folds=args.n_folds, seed=args.seed, use_ema=args.ema, ema_decay_min=args.ema_decay_min, ema_decay_max=args.ema_decay_max,\n",
    "        use_wandb=args.wandb, wandb_entity=args.wandb_entity, wandb_project=args.wandb_project,\n",
    "        use_class_weight=args.class_weight, weight_boost_indices=tuple(int(x) for x in args.boost_indices.split(',') if x!=''),\n",
    "        weight_boost_factor=args.boost_factor, tta_refine=args.tta_refine, tta_try_hflip=args.tta_try_hflip,\n",
    "    )\n",
    "\n",
    "    print(\"Config:\\n\", json.dumps(asdict(cfg), indent=2))\n",
    "\n",
    "    seed_everything(cfg.seed)\n",
    "\n",
    "    df = pd.read_csv(cfg.train_csv)\n",
    "    img_col, tgt_col = discover_cols(df)\n",
    "    # Ensure relative paths under train/ if not absolute\n",
    "    if not os.path.isabs(str(df.iloc[0][img_col])):\n",
    "        # keep as-is; base_dir will prefix\n",
    "        pass\n",
    "\n",
    "    # CV training\n",
    "    fold_scores, fold_models = run_cv(cfg, df)\n",
    "\n",
    "    # (Optional) inference on test folder with the last fold model\n",
    "    if args.do_infer:\n",
    "        # Build a test DataFrame listing image names\n",
    "        test_paths = sorted(glob.glob(os.path.join(cfg.test_dir, '*')))\n",
    "        if len(test_paths) == 0:\n",
    "            print(\"No test images found; skip inference\")\n",
    "        else:\n",
    "            test_df = pd.DataFrame({img_col: [os.path.basename(p) for p in test_paths]})\n",
    "            best_model = fold_models[-1]\n",
    "            best_model.eval()\n",
    "            infer_folder(cfg, best_model, test_df, cfg.out_csv)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
