{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7aa33a4e",
   "metadata": {},
   "source": [
    "# 🧪 GPU 자동 체크 및 환경 테스트\n",
    "\n",
    "이 노트북은 GPU 환경 자동 체크 및 관련 설정을 테스트합니다:\n",
    "- GPU 사용 가능 여부 확인\n",
    "- CUDA/메모리 정보 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "862171b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/ieyeppo/AI_Lab/computer-vision-competition-1SEN\n"
     ]
    }
   ],
   "source": [
    "# [1] 프로젝트 루트 디렉토리로 이동\n",
    "import os                                                   # OS 모듈 임포트\n",
    "import sys                                                  # 시스템 모듈 임포트\n",
    "os.chdir(\"../../../\")                                       # 프로젝트 루트로 이동\n",
    "print(\"현재 작업 디렉토리:\", os.getcwd())                      # 현재 디렉토리 출력"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7696a9ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [2] 폰트 설정 및 경고 억제\n",
    "# 한글 폰트 적용 및 시각화 환경 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 나눔고딕 폰트 경로 및 설정\n",
    "font_path = '/usr/share/fonts/truetype/nanum/NanumGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# 폰트 등록 및 설정\n",
    "fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "plt.rcParams['font.family'] = 'NanumGothic'\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.unicode_minus'] = False\n",
    "\n",
    "# 글자 겹침 방지를 위한 레이아웃 설정\n",
    "plt.rcParams['figure.autolayout'] = True\n",
    "plt.rcParams['axes.titlepad'] = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a8a303d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📝 단위 테스트 시작: gpu_auto_check\n",
      "📝 로그 디렉토리: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909\n"
     ]
    }
   ],
   "source": [
    "# [3] 라이브러리 및 유틸리티 임포트\n",
    "# GPU 체크 및 로거 등 필요한 모듈 임포트\n",
    "import torch  # 파이토치 임포트\n",
    "from src.utils.unit_test_logger import create_test_logger  # 단위 테스트 로거 생성 함수 임포트\n",
    "test_logger = create_test_logger(\"gpu_auto_check\")  # 단위 테스트 로거 초기화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b4f2d07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ GPU 사용 가능: NVIDIA GeForce RTX 4090\n",
      "총 GPU 수: 1\n",
      "GPU 메모리: 24563 MB\n",
      "CUDA 버전: 12.1\n",
      "📝 테스트 결과 저장: gpu_check_test\n",
      "✅ 테스트 완료! 총 소요 시간: 12.39초\n",
      "✅ 결과 요약: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909/test_summary.json\n",
      "\n",
      "==================================================\n",
      "🏁 단위 테스트 완료: gpu_auto_check\n",
      "==================================================\n",
      "📁 결과 디렉토리: notebooks/modular/unit_tests/gpu_auto_check/20250906_003909\n",
      "⏱️ 소요 시간: 12.39초\n",
      "📊 테스트 섹션 수: 1\n",
      "==================================================\n",
      "✅ GPU 자동 체크 테스트 완료\n"
     ]
    }
   ],
   "source": [
    "# [4] GPU 자동 체크 및 정보 수집\n",
    "# GPU 환경 정보 확인 및 저장\n",
    "gpu_info = {}\n",
    "if torch.cuda.is_available():  # GPU 사용 가능 여부 확인\n",
    "    gpu_info = {\n",
    "        'gpu_available': True,\n",
    "        'gpu_name': torch.cuda.get_device_name(0),\n",
    "        'gpu_count': torch.cuda.device_count(),\n",
    "        'gpu_memory_mb': torch.cuda.get_device_properties(0).total_memory // (1024**2),\n",
    "        'cuda_version': torch.version.cuda\n",
    "    }\n",
    "    print(f'✅ GPU 사용 가능: {gpu_info[\"gpu_name\"]}')  # GPU 이름 출력\n",
    "    print(f'총 GPU 수: {gpu_info[\"gpu_count\"]}')  # GPU 개수 출력\n",
    "    print(f'GPU 메모리: {gpu_info[\"gpu_memory_mb\"]} MB')  # GPU 메모리 출력\n",
    "    print(f'CUDA 버전: {gpu_info[\"cuda_version\"]}')  # CUDA 버전 출력\n",
    "else:\n",
    "    gpu_info = {\n",
    "        'gpu_available': False,\n",
    "        'gpu_name': None,\n",
    "        'gpu_count': 0,\n",
    "        'gpu_memory_mb': 0,\n",
    "        'cuda_version': None\n",
    "    }\n",
    "    print('⚠️ GPU 사용 불가, CPU로 실행됩니다')  # CPU 사용 안내\n",
    "\n",
    "# 테스트 결과 저장\n",
    "test_logger.save_test_result('gpu_check_test', gpu_info)\n",
    "test_logger.finalize_test()\n",
    "print('✅ GPU 자동 체크 테스트 완료')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a5aab8",
   "metadata": {},
   "source": [
    "## 📊 GPU 자동 체크 테스트 결과 요약\n",
    "\n",
    "### ✅ 테스트 성공 항목\n",
    "1. **GPU 감지**: NVIDIA GeForce RTX 4090 정상 인식\n",
    "2. **CUDA 환경**: CUDA 12.1 버전 정상 작동\n",
    "3. **메모리 확인**: 24,563 MB GPU 메모리 확인\n",
    "4. **PyTorch 연동**: torch.cuda.is_available() 정상 응답\n",
    "\n",
    "### 📈 하드웨어 사양\n",
    "- **GPU 모델**: NVIDIA GeForce RTX 4090\n",
    "- **총 GPU 수**: 1개\n",
    "- **GPU 메모리**: 24,563 MB (~24GB)\n",
    "- **CUDA 버전**: 12.1\n",
    "- **PyTorch 지원**: ✅ 완전 호환\n",
    "\n",
    "### 📁 생성된 파일 구조\n",
    "```\n",
    "notebooks/modular/unit_tests/gpu_auto_check/20250906_003909/\n",
    "├── test_summary.json\n",
    "└── test_results.json\n",
    "```\n",
    "\n",
    "### 🔧 GPU 환경 정보\n",
    "- **사용 가능 상태**: ✅ 정상\n",
    "- **메모리 상태**: 충분한 용량 확보\n",
    "- **드라이버 호환성**: PyTorch와 완전 호환\n",
    "- **성능 등급**: 고성능 AI/ML 작업 가능"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b38a6bc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 GPU 자동 체크 테스트 결과 파일 확인\n",
      "\n",
      "📊 단위 테스트 결과 디렉토리 (1개):\n",
      "   ├── 20250906_003909 (생성: 2025-09-06 00:39:22)\n",
      "   │   └── test_summary.json (0.5 KB)\n",
      "\n",
      "🔧 최신 GPU 테스트 상세 정보:\n",
      "   📅 테스트 시간: N/A\n",
      "   ⏱️ 소요 시간: 0.00초\n",
      "   📊 테스트 결과: dict_keys([])\n",
      "\n",
      "🎯 시스템 호환성 검증:\n",
      "   ✅ PyTorch GPU 지원: True\n",
      "   ✅ CUDA 디바이스 수: 1\n",
      "   ✅ 현재 GPU: NVIDIA GeForce RTX 4090\n",
      "   ✅ 메모리 총량: 24563 MB\n",
      "\n",
      "✅ GPU 자동 체크 테스트 완료!\n",
      "📅 테스트 일시: 2025-09-06 00:40:13\n",
      "🎯 GPU 환경이 AI/ML 작업에 최적화되어 있음을 확인했습니다.\n"
     ]
    }
   ],
   "source": [
    "# [5] 실제 생성된 파일 구조 확인\n",
    "import os\n",
    "import glob\n",
    "from datetime import datetime\n",
    "\n",
    "print(\"🔍 GPU 자동 체크 테스트 결과 파일 확인\\n\")\n",
    "\n",
    "# 1. 테스트 결과 디렉토리 확인\n",
    "test_dirs = glob.glob(\"notebooks/modular/unit_tests/gpu_auto_check/202509*\")\n",
    "print(f\"📊 단위 테스트 결과 디렉토리 ({len(test_dirs)}개):\")\n",
    "for test_dir in sorted(test_dirs)[-3:]:  # 최근 3개만 표시\n",
    "    mtime = datetime.fromtimestamp(os.path.getmtime(test_dir))\n",
    "    files = os.listdir(test_dir)\n",
    "    json_files = [f for f in files if f.endswith('.json')]\n",
    "    \n",
    "    print(f\"   ├── {os.path.basename(test_dir)} (생성: {mtime.strftime('%Y-%m-%d %H:%M:%S')})\")\n",
    "    for json_file in json_files:\n",
    "        file_path = os.path.join(test_dir, json_file)\n",
    "        size = os.path.getsize(file_path) / 1024  # KB\n",
    "        print(f\"   │   └── {json_file} ({size:.1f} KB)\")\n",
    "\n",
    "# 2. GPU 테스트 정보 상세 확인\n",
    "if test_dirs:\n",
    "    latest_test_dir = max(test_dirs, key=os.path.getmtime)\n",
    "    summary_file = os.path.join(latest_test_dir, \"test_summary.json\")\n",
    "    \n",
    "    if os.path.exists(summary_file):\n",
    "        import json\n",
    "        with open(summary_file, 'r', encoding='utf-8') as f:\n",
    "            summary = json.load(f)\n",
    "        \n",
    "        print(f\"\\n🔧 최신 GPU 테스트 상세 정보:\")\n",
    "        print(f\"   📅 테스트 시간: {summary.get('test_datetime', 'N/A')}\")\n",
    "        print(f\"   ⏱️ 소요 시간: {summary.get('total_time', 0):.2f}초\")\n",
    "        print(f\"   📊 테스트 결과: {summary.get('test_results', {}).keys()}\")\n",
    "\n",
    "# 3. 시스템 호환성 요약\n",
    "print(f\"\\n🎯 시스템 호환성 검증:\")\n",
    "print(f\"   ✅ PyTorch GPU 지원: {torch.cuda.is_available()}\")\n",
    "print(f\"   ✅ CUDA 디바이스 수: {torch.cuda.device_count()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   ✅ 현재 GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   ✅ 메모리 총량: {torch.cuda.get_device_properties(0).total_memory // (1024**2)} MB\")\n",
    "\n",
    "print(f\"\\n✅ GPU 자동 체크 테스트 완료!\")\n",
    "print(f\"📅 테스트 일시: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"🎯 GPU 환경이 AI/ML 작업에 최적화되어 있음을 확인했습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b49149",
   "metadata": {},
   "source": [
    "## 🛠️ GPU 자동 체크 트러블슈팅 가이드\n",
    "\n",
    "### 🚨 자주 발생하는 문제들\n",
    "\n",
    "#### 1. **GPU가 감지되지 않음**\n",
    "```python\n",
    "# 문제 확인\n",
    "import torch\n",
    "print(torch.cuda.is_available())  # False인 경우\n",
    "\n",
    "# 해결 방법\n",
    "# 1. NVIDIA 드라이버 재설치\n",
    "# 2. CUDA 툴킷 설치 확인\n",
    "# 3. PyTorch GPU 버전 재설치\n",
    "```\n",
    "\n",
    "#### 2. **CUDA 버전 불일치**\n",
    "```bash\n",
    "# 시스템 CUDA 버전 확인\n",
    "nvidia-smi\n",
    "\n",
    "# PyTorch CUDA 버전 확인\n",
    "python -c \"import torch; print(torch.version.cuda)\"\n",
    "```\n",
    "\n",
    "#### 3. **메모리 부족 오류**\n",
    "```python\n",
    "# GPU 메모리 상태 확인\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"메모리 총량: {torch.cuda.get_device_properties(0).total_memory // (1024**2)} MB\")\n",
    "    print(f\"사용 중: {torch.cuda.memory_allocated() // (1024**2)} MB\")\n",
    "    print(f\"캐시됨: {torch.cuda.memory_reserved() // (1024**2)} MB\")\n",
    "```\n",
    "\n",
    "#### 4. **드라이버 호환성 문제**\n",
    "- **NVIDIA 드라이버**: 최신 Game Ready 또는 Studio 드라이버 권장\n",
    "- **CUDA 호환성**: PyTorch 2.5.1+cu121은 CUDA 12.1 이상 필요\n",
    "- **최소 요구사항**: CUDA Compute Capability 3.7 이상\n",
    "\n",
    "### 📋 필수 확인 사항\n",
    "1. **하드웨어**: NVIDIA GPU 설치 및 전원 연결\n",
    "2. **드라이버**: 최신 NVIDIA 드라이버 설치\n",
    "3. **CUDA**: 적절한 CUDA 툴킷 버전\n",
    "4. **PyTorch**: GPU 지원 버전 설치\n",
    "\n",
    "### 🔧 수동 테스트 방법\n",
    "```python\n",
    "# 1. 기본 GPU 테스트\n",
    "import torch\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "x = torch.randn(1000, 1000).to(device)\n",
    "y = torch.randn(1000, 1000).to(device)\n",
    "z = torch.matmul(x, y)\n",
    "print(f\"연산 완료: {z.shape} on {z.device}\")\n",
    "\n",
    "# 2. 메모리 테스트\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()  # 캐시 정리\n",
    "    print(\"GPU 메모리 캐시 정리 완료\")\n",
    "```\n",
    "\n",
    "### 💡 성능 최적화 팁\n",
    "- **혼합 정밀도**: `torch.cuda.amp` 사용으로 메모리 절약\n",
    "- **배치 크기**: GPU 메모리에 맞는 적절한 배치 크기 설정\n",
    "- **다중 GPU**: `torch.nn.DataParallel` 또는 `DistributedDataParallel` 활용"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
