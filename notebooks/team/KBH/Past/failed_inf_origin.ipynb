{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45d6d780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "from scipy import ndimage\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import cv2\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision용\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# 한글 폰트 설정 (시각화용)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "956d41cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91d8a221",
   "metadata": {},
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "# model_name = 'tf_efficientnetv2_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "# model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "model_name = 'convnext_base_384_in22ft1k'\n",
    "# model_name = 'convnextv2_base.fcmae_ft_in22k_in1k_384'\n",
    "# model_name = 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k' # openclip\n",
    "# model_name = 'vit_base_patch16_384.augreg_in1k' # augreg\n",
    "# model_name = 'eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k' # eva-02 멀티모달\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "# model_name = 'vit_base_patch14_reg4_dinov2.lvd142m' # dinov2 reg4\n",
    "\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 2e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 24\n",
    "num_workers = 12\n",
    "EMA = True  # Exponential Moving Average 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8905006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Fold 1 model loaded from models/fold_1_best.pth\n",
      "✓ Fold 2 model loaded from models/fold_2_best.pth\n",
      "✓ Fold 3 model loaded from models/fold_3_best.pth\n",
      "✓ Fold 4 model loaded from models/fold_4_best.pth\n",
      "✓ Fold 5 model loaded from models/fold_5_best.pth\n",
      "Using ensemble of all 5 fold models for inference\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold 앙상블 모델 준비\n",
    "ensemble_models = []\n",
    "for i in range(5):  # fold 개수만큼\n",
    "    fold_model = timm.create_model(model_name, pretrained=False, num_classes=17).to(device)  # pretrained=False로 변경\n",
    "    \n",
    "    # fold별 저장된 파일 로드\n",
    "    checkpoint = torch.load(f'models/fold_{i+1}_best.pth')  # fold별 파일 경로\n",
    "    fold_model.load_state_dict(checkpoint)\n",
    "    fold_model.eval()\n",
    "    \n",
    "    ensemble_models.append(fold_model)\n",
    "    print(f\"✓ Fold {i+1} model loaded from models/fold_{i+1}_best.pth\")\n",
    "\n",
    "print(f\"Using ensemble of all {len(ensemble_models)} fold models for inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e0fc813",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling 클래스 정의\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "99f98e06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic TTA transforms: 5\n",
      "Enhanced TTA transforms: 8\n"
     ]
    }
   ],
   "source": [
    "# 기본 TTA 변형들 (모든 클래스 공통)\n",
    "essential_tta_transforms = [\n",
    "    # 원본\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90도 회전들\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 밝기 개선\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "# 취약 클래스(3,4,7,14) 전용 강화 TTA\n",
    "enhanced_tta_transforms = essential_tta_transforms + [\n",
    "    # 문서 특화 변형들 추가\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.CLAHE(clip_limit=4.0, tile_grid_size=(8, 8), p=1.0),  # 대비 향상\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Sharpen(alpha=(0.2, 0.5), lightness=(0.5, 1.0), p=1.0),  # 선명화\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.4, contrast_limit=0.4, p=1.0),\n",
    "        A.CLAHE(clip_limit=2.0, tile_grid_size=(8, 8), p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]\n",
    "\n",
    "print(f\"Basic TTA transforms: {len(essential_tta_transforms)}\")\n",
    "print(f\"Enhanced TTA transforms: {len(enhanced_tta_transforms)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94b9f6f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_image_quality(img):\n",
    "    \"\"\"\n",
    "    이미지 품질을 평가하여 전처리 필요성 판단\n",
    "    Args:\n",
    "        img: RGB numpy array (H, W, 3)\n",
    "    Returns:\n",
    "        quality_score: 0~1 (낮을수록 오염됨)\n",
    "        metrics: 개별 품질 지표들\n",
    "    \"\"\"\n",
    "    # RGB to GRAY 변환\n",
    "    if len(img.shape) == 3:\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    else:\n",
    "        gray = img\n",
    "    \n",
    "    # 1. 블러 정도 측정 (Laplacian variance)\n",
    "    blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "    blur_normalized = min(blur_score / 500.0, 1.0)  # 500 이상이면 선명\n",
    "    \n",
    "    # 2. 대비 측정 (표준편차)\n",
    "    contrast_score = gray.std()\n",
    "    contrast_normalized = min(contrast_score / 50.0, 1.0)  # 50 이상이면 대비 좋음\n",
    "    \n",
    "    # 3. 밝기 분포 (히스토그램 엔트로피)\n",
    "    hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "    hist_norm = hist / hist.sum()\n",
    "    hist_entropy = -np.sum(hist_norm * np.log(hist_norm + 1e-7))\n",
    "    brightness_normalized = min(hist_entropy / 6.0, 1.0)  # 엔트로피 정규화\n",
    "    \n",
    "    # 4. 전체 품질 점수 계산 (가중 평균)\n",
    "    quality_score = (blur_normalized * 0.4 + \n",
    "                    contrast_normalized * 0.4 + \n",
    "                    brightness_normalized * 0.2)\n",
    "    \n",
    "    metrics = {\n",
    "        'blur': blur_normalized,\n",
    "        'contrast': contrast_normalized, \n",
    "        'brightness': brightness_normalized,\n",
    "        'overall': quality_score\n",
    "    }\n",
    "    \n",
    "    return quality_score, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "89d93941",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_preprocessing(img):\n",
    "    \"\"\"\n",
    "    품질 점수에 따른 적응형 전처리\n",
    "    Args:\n",
    "        img: RGB numpy array (H, W, 3)\n",
    "    Returns:\n",
    "        processed_img: 전처리된 RGB numpy array\n",
    "    \"\"\"\n",
    "    quality_score, metrics = assess_image_quality(img)\n",
    "    \n",
    "    # 임계값 설정\n",
    "    CLEAN_THRESHOLD = 0.6    # 깨끗한 이미지\n",
    "    MODERATE_THRESHOLD = 0.3  # 중간 오염\n",
    "    \n",
    "    if quality_score > CLEAN_THRESHOLD:\n",
    "        # 정상 이미지 - 원본 그대로 반환\n",
    "        return img\n",
    "    \n",
    "    elif quality_score > MODERATE_THRESHOLD:\n",
    "        # 중간 오염 - 가벼운 복원\n",
    "        processed_img = img.copy()\n",
    "        \n",
    "        # 가벼운 블러 제거\n",
    "        if metrics['blur'] < 0.5:\n",
    "            processed_img = cv2.bilateralFilter(processed_img, 5, 50, 50)\n",
    "        \n",
    "        # 대비 개선\n",
    "        if metrics['contrast'] < 0.5:\n",
    "            processed_img = cv2.convertScaleAbs(processed_img, alpha=1.2, beta=10)\n",
    "        \n",
    "        return np.clip(processed_img, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    else:\n",
    "        # 심각한 오염 - 강력한 복원\n",
    "        processed_img = img.copy()\n",
    "        \n",
    "        # 노이즈 제거 (Non-local Means Denoising)\n",
    "        processed_img = cv2.fastNlMeansDenoisingColored(processed_img, None, 10, 10, 7, 21)\n",
    "        \n",
    "        # 선명화 (Unsharp Mask)\n",
    "        if metrics['blur'] < 0.3:\n",
    "            gaussian = cv2.GaussianBlur(processed_img, (0, 0), 2.0)\n",
    "            processed_img = cv2.addWeighted(processed_img, 1.5, gaussian, -0.5, 0)\n",
    "        \n",
    "        # 대비 향상 (CLAHE)\n",
    "        if metrics['contrast'] < 0.3:\n",
    "            lab = cv2.cvtColor(processed_img, cv2.COLOR_RGB2LAB)\n",
    "            lab[:,:,0] = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8,8)).apply(lab[:,:,0])\n",
    "            processed_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "        \n",
    "        return np.clip(processed_img, 0, 255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63b3434a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트 함수\n",
    "def test_preprocessing_functions():\n",
    "    \"\"\"구현한 함수들의 기본 동작 테스트\"\"\"\n",
    "    print(\"Testing adaptive preprocessing functions...\")\n",
    "    \n",
    "    # 더미 이미지로 테스트\n",
    "    test_img = np.random.randint(0, 255, (384, 384, 3), dtype=np.uint8)\n",
    "    \n",
    "    try:\n",
    "        quality_score, metrics = assess_image_quality(test_img)\n",
    "        print(f\"Quality assessment: {quality_score:.3f}\")\n",
    "        print(f\"Metrics: {metrics}\")\n",
    "        \n",
    "        processed_img = adaptive_preprocessing(test_img)\n",
    "        print(f\"Preprocessing completed. Input shape: {test_img.shape}, Output shape: {processed_img.shape}\")\n",
    "        print(\"✅ All functions working correctly\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in functions: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9124dcc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing adaptive preprocessing functions...\n",
      "Quality assessment: 0.971\n",
      "Metrics: {'blur': 1.0, 'contrast': 0.9860672067540966, 'brightness': 0.8811639944712321, 'overall': 0.9706596815958852}\n",
      "Preprocessing completed. Input shape: (384, 384, 3), Output shape: (384, 384, 3)\n",
      "✅ All functions working correctly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 함수 테스트 실행\n",
    "test_preprocessing_functions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "24cb2557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 추론을 위한 Dataset 클래스\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # 여러 transform을 리스트로 받음\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # ★ 적응형 전처리 적용\n",
    "        img = adaptive_preprocessing(img)\n",
    "        \n",
    "        # 모든 transform을 적용한 결과를 리스트로 반환\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78f49be4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# TTA Dataset 생성\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    enhanced_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (배치 크기를 줄여서 메모리 절약)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=96,  # TTA는 메모리를 많이 사용하므로 배치 크기 줄임\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"TTA Dataset size: {len(tta_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a37d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enhanced_ensemble_tta_inference(models, loader, weak_classes=[3,4,7,14], confidence_threshold=0.40):\n",
    "    \"\"\"취약 클래스 대응 강화 앙상블 TTA 추론\"\"\"\n",
    "    temp_scaling = TemperatureScaling().to(device)\n",
    "    all_predictions = []\n",
    "    all_confidences = []\n",
    "    weak_class_count = 0\n",
    "    enhanced_count = 0\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Enhanced Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        batch_predictions = []\n",
    "        batch_confidences = []\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            # 각 이미지별로 개별 처리\n",
    "            single_images = [img_tensor[i:i+1] for img_tensor in images_list]  # 각 TTA에서 i번째 이미지\n",
    "            \n",
    "            # 1차: 기본 TTA (5개 변형) 앙상블\n",
    "            basic_probs = torch.zeros(1, 17).to(device)\n",
    "            for model in models:\n",
    "                with torch.no_grad():\n",
    "                    for j, images in enumerate(single_images[:5]):  # 기본 TTA만 (처음 5개)\n",
    "                        images = images.to(device)\n",
    "                        preds = model(images)\n",
    "                        scaled_preds = temp_scaling(preds)\n",
    "                        probs = torch.softmax(scaled_preds, dim=1)\n",
    "                        basic_probs += probs / (len(models) * 5)\n",
    "            \n",
    "            basic_pred = torch.argmax(basic_probs, dim=1).item()\n",
    "            basic_conf = torch.max(basic_probs, dim=1)[0].item()\n",
    "            \n",
    "            # 2차: 취약 클래스 + 낮은 신뢰도인 경우 강화 TTA 적용\n",
    "            if basic_pred in weak_classes and basic_conf < confidence_threshold:\n",
    "                weak_class_count += 1\n",
    "                enhanced_count += 1\n",
    "                \n",
    "                # 강화 TTA (8개 변형) 앙상블\n",
    "                enhanced_probs = torch.zeros(1, 17).to(device)\n",
    "                for model in models:\n",
    "                    with torch.no_grad():\n",
    "                        for images in single_images:  # 모든 TTA 변형 (8개)\n",
    "                            images = images.to(device)\n",
    "                            preds = model(images)\n",
    "                            scaled_preds = temp_scaling(preds)\n",
    "                            probs = torch.softmax(scaled_preds, dim=1)\n",
    "                            enhanced_probs += probs / (len(models) * len(single_images))\n",
    "                \n",
    "                enhanced_pred = torch.argmax(enhanced_probs, dim=1).item()\n",
    "                enhanced_conf = torch.max(enhanced_probs, dim=1)[0].item()\n",
    "                \n",
    "                # 더 높은 신뢰도 선택\n",
    "                if enhanced_conf > basic_conf + 0.1:  # 10% 이상 향상 시 채택\n",
    "                    final_pred = enhanced_pred\n",
    "                    final_conf = enhanced_conf\n",
    "                else:\n",
    "                    final_pred = basic_pred\n",
    "                    final_conf = basic_conf\n",
    "            else:\n",
    "                # 정상 클래스 또는 높은 신뢰도 → 기본 TTA 결과 사용\n",
    "                final_pred = basic_pred\n",
    "                final_conf = basic_conf\n",
    "                if basic_pred in weak_classes:\n",
    "                    weak_class_count += 1\n",
    "            \n",
    "            batch_predictions.append(final_pred)\n",
    "            batch_confidences.append(final_conf)\n",
    "        \n",
    "        all_predictions.extend(batch_predictions)\n",
    "        all_confidences.extend(batch_confidences)\n",
    "    \n",
    "    print(f\"취약 클래스 예측: {weak_class_count}개\")\n",
    "    print(f\"강화 TTA 적용: {enhanced_count}개\")\n",
    "    print(f\"평균 신뢰도: {np.mean(all_confidences):.3f}\")\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f46b9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ensemble TTA inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced Ensemble TTA:   0%|          | 0/33 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Enhanced Ensemble TTA: 100%|██████████| 33/33 [15:24<00:00, 28.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "취약 클래스 예측: 692개\n",
      "강화 TTA 적용: 174개\n",
      "평균 신뢰도: 0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 TTA 실행\n",
    "print(\"Starting Ensemble TTA inference...\")\n",
    "tta_predictions = enhanced_ensemble_tta_inference(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    weak_classes=[3,4,7,14],\n",
    "    confidence_threshold=0.40\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "67b7a954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 결과로 submission 파일 생성\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cfa6c1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 submission과 동일한 순서인지 확인\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "540b8978",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA predictions saved\n",
      "TTA Prediction sample:\n"
     ]
    }
   ],
   "source": [
    "# TTA 결과 저장\n",
    "tta_pred_df.to_csv(\"../submission/choice.csv\", index=False)\n",
    "print(\"TTA predictions saved\")\n",
    "\n",
    "print(\"TTA Prediction sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bf86010a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg      12\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
