{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* 데이터 로드를 위한 구글 드라이브를 마운트합니다.\n",
    "* 필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8489,
     "status": "ok",
     "timestamp": 1700314558888,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "NC8V-D393wY4",
    "outputId": "e9927325-26c4-4b89-9c51-c1d6541388d6"
   },
   "outputs": [],
   "source": [
    "# # 필요한 라이브러리를 설치합니다.\n",
    "# !pip install timm\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install optuna\n",
    "# !apt install -y libgl1-mesa-glx\n",
    "# !pip install albumentations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n",
    "* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/opt/conda/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision용\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "\n",
    "# 한글 폰트 설정 (시각화용)\n",
    "plt.rcParams['font.family'] = ['DejaVu Sans']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 데이터셋 클래스를 정의합니다. (Hard Augmentation 포함)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, epoch=0, total_epochs=10, is_train=True):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.epoch = epoch\n",
    "        self.total_epochs = total_epochs\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Hard augmentation 확률 계산\n",
    "        self.p_hard = 0.2 + 0.3 * (epoch / total_epochs) if is_train else 0\n",
    "        \n",
    "        # Normal augmentation\n",
    "        self.normal_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "            ], p=0.6),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "            A.GaussNoise(var_limit=(30.0, 100.0), p=0.7),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # Hard augmentation\n",
    "        self.hard_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "                A.Rotate(limit=[-15,15], p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=15, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=15, p=1.0),\n",
    "            ], p=0.95),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.9),\n",
    "            A.GaussNoise(var_limit=(50.0, 150.0), p=0.8),\n",
    "            A.JpegCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 배치별 증강 선택\n",
    "        if self.is_train and random.random() < self.p_hard:\n",
    "            img = self.hard_aug(image=img)['image']\n",
    "        else:\n",
    "            img = self.normal_aug(image=img)['image']\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    scaler = GradScaler()  # Mixed Precision용\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Cutmix/Mixup 적용 (30% 확률)\n",
    "        if random.random() < 0.3:\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "        else:\n",
    "            with autocast(): preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        scaler.scale(loss).backward()  # Mixed Precision용\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()  # Mixed Precision용\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation을 위한 함수 추가\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    한 에폭 검증을 수행하는 함수\n",
    "    - model.eval()로 모델을 평가 모드로 전환\n",
    "    - torch.no_grad()로 gradient 계산 비활성화하여 메모리 절약\n",
    "    - 검증 데이터에 대한 loss, accuracy, f1 score 계산\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 전환 (dropout, batchnorm 비활성화)\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():  # gradient 계산 비활성화로 메모리 절약\n",
    "        pbar = tqdm(loader, desc=\"Validating\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)  # 모델 예측\n",
    "            loss = loss_fn(preds, targets)  # 손실 계산\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측 클래스 저장\n",
    "            targets_list.extend(targets.detach().cpu().numpy())  # 실제 클래스 저장\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)  # 평균 손실 계산\n",
    "    val_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')  # Macro F1 계산 (대회 평가지표)\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = '../data/'\n",
    "\n",
    "# model config\n",
    "# model_name = 'tf_efficientnetv2_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "# model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "model_name = 'convnext_base_384_in22ft1k'\n",
    "# model_name = 'convnextv2_base.fcmae_ft_in22k_in1k_384'\n",
    "# model_name = 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k' # openclip\n",
    "# model_name = 'vit_base_patch16_384.augreg_in1k' # augreg\n",
    "# model_name = 'eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k' # eva-02 멀티모달\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "# model_name = 'vit_base_patch14_reg4_dinov2.lvd142m' # dinov2 reg4\n",
    "\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "\n",
    "# training config\n",
    "img_size = 384\n",
    "LR = 2e-4\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 24\n",
    "num_workers = 8\n",
    "EMA = True  # Exponential Moving Average 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna를 사용한 하이퍼파라미터 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        \n",
    "        # 간단한 3-fold CV로 빠른 평가\n",
    "        skf_simple = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf_simple.split(train_df, train_df['target'])):\n",
    "            # 모델 생성\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "            optimizer = Adam(model.parameters(), lr=lr)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # 간단한 2 epoch 학습\n",
    "            for epoch in range(2):\n",
    "                train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "            \n",
    "            val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "            fold_scores.append(val_ret['val_f1'])\n",
    "        \n",
    "        return np.mean(fold_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    # 최적 파라미터 적용\n",
    "    LR = study.best_params['lr']\n",
    "    BATCH_SIZE = study.best_params['batch_size']\n",
    "    print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    # 위의 objective 함수와 study 코드\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross Validation...\n",
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7852: 100%|██████████| 53/53 [00:44<00:00,  1.18it/s]\n",
      "Val Loss: 1.7043: 100%|██████████| 14/14 [00:07<00:00,  1.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.1714 | Train F1: 0.3167 | Val Loss: 1.1284 | Val F1: 0.6849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6445: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 1.5191: 100%|██████████| 14/14 [00:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.4073 | Train F1: 0.5418 | Val Loss: 0.8987 | Val F1: 0.7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7285: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 1.1930: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 1.0131 | Train F1: 0.6831 | Val Loss: 0.7150 | Val F1: 0.8481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7065: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s]\n",
      "Val Loss: 1.3854: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.9164 | Train F1: 0.6982 | Val Loss: 0.7124 | Val F1: 0.8583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0381: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.7129: 100%|██████████| 14/14 [00:05<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.9047 | Train F1: 0.7405 | Val Loss: 0.7554 | Val F1: 0.7849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5625: 100%|██████████| 53/53 [00:28<00:00,  1.89it/s]\n",
      "Val Loss: 0.8230: 100%|██████████| 14/14 [00:04<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.8798 | Train F1: 0.6928 | Val Loss: 0.5860 | Val F1: 0.9020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7158: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 1.1953: 100%|██████████| 14/14 [00:04<00:00,  2.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7057 | Train F1: 0.7420 | Val Loss: 0.6002 | Val F1: 0.8758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5059: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.6968: 100%|██████████| 14/14 [00:05<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.7386 | Train F1: 0.8181 | Val Loss: 0.5428 | Val F1: 0.8753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3760: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.9423: 100%|██████████| 14/14 [00:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.7938 | Train F1: 0.7895 | Val Loss: 0.5602 | Val F1: 0.8839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0645: 100%|██████████| 53/53 [00:28<00:00,  1.88it/s]\n",
      "Val Loss: 0.9153: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.7683 | Train F1: 0.7512 | Val Loss: 0.5549 | Val F1: 0.9094\n",
      "Fold 1 Best Validation F1: 0.9094\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.8193: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 1.3635: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.0860 | Train F1: 0.3663 | Val Loss: 1.1425 | Val F1: 0.6932\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1816: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.8488: 100%|██████████| 14/14 [00:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.2818 | Train F1: 0.5534 | Val Loss: 0.8129 | Val F1: 0.7876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 2.0117: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.8132: 100%|██████████| 14/14 [00:04<00:00,  2.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.9959 | Train F1: 0.6858 | Val Loss: 0.6531 | Val F1: 0.8361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.9419: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 1.2741: 100%|██████████| 14/14 [00:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8455 | Train F1: 0.7298 | Val Loss: 0.7414 | Val F1: 0.8200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.5781: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 1.2360: 100%|██████████| 14/14 [00:05<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.7628 | Train F1: 0.8100 | Val Loss: 0.5875 | Val F1: 0.8829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3359: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 1.0089: 100%|██████████| 14/14 [00:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.6421 | Train F1: 0.8338 | Val Loss: 0.5843 | Val F1: 0.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4480: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.7515: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7405 | Train F1: 0.7969 | Val Loss: 0.5551 | Val F1: 0.9051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4160: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.5406: 100%|██████████| 14/14 [00:05<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.6453 | Train F1: 0.8471 | Val Loss: 0.5128 | Val F1: 0.9102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7031: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.9389: 100%|██████████| 14/14 [00:05<00:00,  2.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6632 | Train F1: 0.8464 | Val Loss: 0.5480 | Val F1: 0.8951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4517: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.6515: 100%|██████████| 14/14 [00:05<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.7357 | Train F1: 0.7731 | Val Loss: 0.5167 | Val F1: 0.9127\n",
      "Fold 2 Best Validation F1: 0.9127\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.3379: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3553: 100%|██████████| 14/14 [00:04<00:00,  2.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 2.0173 | Train F1: 0.3786 | Val Loss: 1.1520 | Val F1: 0.6343\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 2.0312: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3532: 100%|██████████| 14/14 [00:05<00:00,  2.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.2808 | Train F1: 0.5721 | Val Loss: 0.8004 | Val F1: 0.7832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 2.0762: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3326: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 1.0788 | Train F1: 0.6451 | Val Loss: 0.6849 | Val F1: 0.8119\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7080: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3250: 100%|██████████| 14/14 [00:05<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8573 | Train F1: 0.7590 | Val Loss: 0.5760 | Val F1: 0.8832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3335: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3379: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.8049 | Train F1: 0.7773 | Val Loss: 0.5517 | Val F1: 0.8621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.8721: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s]\n",
      "Val Loss: 0.3376: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.8139 | Train F1: 0.7408 | Val Loss: 0.5757 | Val F1: 0.8473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5322: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3285: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.8143 | Train F1: 0.6922 | Val Loss: 0.5188 | Val F1: 0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.3799: 100%|██████████| 53/53 [00:28<00:00,  1.88it/s]\n",
      "Val Loss: 0.3249: 100%|██████████| 14/14 [00:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.6693 | Train F1: 0.8568 | Val Loss: 0.5153 | Val F1: 0.8752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3882: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s]\n",
      "Val Loss: 0.3262: 100%|██████████| 14/14 [00:05<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6660 | Train F1: 0.7640 | Val Loss: 0.4808 | Val F1: 0.9163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4119: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3254: 100%|██████████| 14/14 [00:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6875 | Train F1: 0.8367 | Val Loss: 0.4934 | Val F1: 0.9010\n",
      "Fold 3 Best Validation F1: 0.9163\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0928: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s]\n",
      "Val Loss: 0.3942: 100%|██████████| 14/14 [00:05<00:00,  2.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.7843 | Train F1: 0.4704 | Val Loss: 0.8842 | Val F1: 0.7304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.2334: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3318: 100%|██████████| 14/14 [00:05<00:00,  2.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 0.9787 | Train F1: 0.6771 | Val Loss: 0.6367 | Val F1: 0.8374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.7256: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3291: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.8519 | Train F1: 0.7677 | Val Loss: 0.5536 | Val F1: 0.8597\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.3711: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3260: 100%|██████████| 14/14 [00:05<00:00,  2.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.8425 | Train F1: 0.7275 | Val Loss: 0.5480 | Val F1: 0.8540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4824: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3260: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.7611 | Train F1: 0.7875 | Val Loss: 0.5283 | Val F1: 0.8901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5161: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3273: 100%|██████████| 14/14 [00:05<00:00,  2.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.7685 | Train F1: 0.7370 | Val Loss: 0.5644 | Val F1: 0.8688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6348: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3275: 100%|██████████| 14/14 [00:05<00:00,  2.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7006 | Train F1: 0.8446 | Val Loss: 0.4921 | Val F1: 0.9266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3330: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3273: 100%|██████████| 14/14 [00:05<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.7152 | Train F1: 0.8196 | Val Loss: 0.4969 | Val F1: 0.9130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4683: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3245: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.6713 | Train F1: 0.8687 | Val Loss: 0.4791 | Val F1: 0.9206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3604: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3257: 100%|██████████| 14/14 [00:05<00:00,  2.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.5997 | Train F1: 0.8943 | Val Loss: 0.4868 | Val F1: 0.9230\n",
      "Fold 4 Best Validation F1: 0.9266\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.6104: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3756: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.9581 | Train F1: 0.3852 | Val Loss: 0.9913 | Val F1: 0.6786\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 1.3486: 100%|██████████| 53/53 [00:28<00:00,  1.83it/s]\n",
      "Val Loss: 0.3368: 100%|██████████| 14/14 [00:05<00:00,  2.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  2 | Train Loss: 1.0871 | Train F1: 0.6481 | Val Loss: 0.6889 | Val F1: 0.8262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6890: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3345: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  3 | Train Loss: 0.9297 | Train F1: 0.7301 | Val Loss: 0.6268 | Val F1: 0.8338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.9897: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3337: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  4 | Train Loss: 0.7388 | Train F1: 0.7914 | Val Loss: 0.6181 | Val F1: 0.8618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loss: 0.6230: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3260: 100%|██████████| 14/14 [00:05<00:00,  2.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  5 | Train Loss: 0.8233 | Train F1: 0.7676 | Val Loss: 0.5485 | Val F1: 0.8886\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.6006: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3255: 100%|██████████| 14/14 [00:05<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  6 | Train Loss: 0.7224 | Train F1: 0.8004 | Val Loss: 0.5073 | Val F1: 0.9050\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3745: 100%|██████████| 53/53 [00:28<00:00,  1.87it/s]\n",
      "Val Loss: 0.3321: 100%|██████████| 14/14 [00:05<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  7 | Train Loss: 0.7847 | Train F1: 0.7892 | Val Loss: 0.5285 | Val F1: 0.8985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.0889: 100%|██████████| 53/53 [00:28<00:00,  1.85it/s]\n",
      "Val Loss: 0.3288: 100%|██████████| 14/14 [00:05<00:00,  2.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  8 | Train Loss: 0.6355 | Train F1: 0.8651 | Val Loss: 0.5354 | Val F1: 0.8827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1777: 100%|██████████| 53/53 [00:28<00:00,  1.84it/s]\n",
      "Val Loss: 0.3245: 100%|██████████| 14/14 [00:05<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  9 | Train Loss: 0.7464 | Train F1: 0.7471 | Val Loss: 0.5033 | Val F1: 0.9012\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.3628: 100%|██████████| 53/53 [00:28<00:00,  1.86it/s]\n",
      "Val Loss: 0.3319: 100%|██████████| 14/14 [00:05<00:00,  2.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 | Train Loss: 0.6384 | Train F1: 0.8259 | Val Loss: 0.5046 | Val F1: 0.9105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 Best Validation F1: 0.9105\n",
      "\n",
      "============================================================\n",
      "K-FOLD CROSS VALIDATION RESULTS\n",
      "============================================================\n",
      "Fold 1: 0.9094\n",
      "Fold 2: 0.9127\n",
      "Fold 3: 0.9163\n",
      "Fold 4: 0.9266\n",
      "Fold 5: 0.9105\n",
      "\n",
      "Mean CV F1: 0.9151 ± 0.0062\n",
      "Best single fold: 0.9266\n"
     ]
    }
   ],
   "source": [
    "# K-Fold 설정\n",
    "N_FOLDS = 5  # 5-fold로 설정 (데이터가 적으므로)\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# 클래스별 최소 샘플 보장 확인\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "#     assert len(np.unique(train_df.iloc[val_idx]['target'])) == 17\n",
    "\n",
    "# 전체 학습 데이터 로드\n",
    "train_df = pd.read_csv(\"../data/train.csv\")\n",
    "\n",
    "# K-Fold 결과를 저장할 리스트\n",
    "fold_results = []\n",
    "fold_models = []  # 각 fold의 최고 성능 모델을 저장\n",
    "fold_class_accuracies = [] # 각 fold의 클래스별 정확도 저장\n",
    "\n",
    "print(f\"Starting {N_FOLDS}-Fold Cross Validation...\")\n",
    "\n",
    "# LR = best_params['lr']\n",
    "# BATCH_SIZE = best_params['batch_size']\n",
    "\n",
    "# K-Fold Cross Validation 시작\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    current_model = model_name\n",
    "    \n",
    "    # 현재 fold의 train/validation 데이터 분할\n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # 현재 fold의 Dataset 생성\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"../data/train/\",\n",
    "        # transform=trn_transform\n",
    "        epoch=0,  # 현재 epoch 전달\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"../data/train/\",\n",
    "        # transform=tst_transform  # 검증에는 증강 적용 안함\n",
    "        epoch=0,  # validation은 epoch 관계없음\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=False  # validation이므로 hard augmentation 비활성화\n",
    "    )\n",
    "    \n",
    "    # 현재 fold의 DataLoader 생성\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # 모델 초기화 (각 fold마다 새로운 모델)\n",
    "    model = timm.create_model(\n",
    "        current_model,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)  # Label Smoothing 적용\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler 추가\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # 현재 fold의 최고 성능 추적\n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # 현재 fold 학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        # Scheduler step 추가\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f}\")\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # Best 모델 분석\n",
    "            model.eval()\n",
    "            val_preds, val_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for image, targets in val_loader:\n",
    "                    preds = model(image.to(device)).argmax(dim=1)\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(targets.numpy())\n",
    "            \n",
    "            # 클래스별 정확도\n",
    "            fold_class_acc = {}\n",
    "            for c in range(17):\n",
    "                mask = np.array(val_targets) == c\n",
    "                if mask.sum() > 0:\n",
    "                    fold_class_acc[c] = (np.array(val_preds)[mask] == c).mean()\n",
    "    \n",
    "    # 현재 fold 결과 저장\n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset)\n",
    "    })\n",
    "    \n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Best Validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    fold_class_accuracies.append(fold_class_acc) # 각 fold의 클래스별 정확도 저장\n",
    "\n",
    "# K-Fold 결과 요약\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"K-FOLD CROSS VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Best single fold: {max(val_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 3 classes:\n",
      "Class 14: 50.0%\n",
      "Class 7: 60.0%\n",
      "Class 3: 61.0%\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 성능 시각화\n",
    "meta_df = pd.read_csv(\"../data/meta.csv\")\n",
    "avg_acc = {c: np.mean([f.get(c,0) for f in fold_class_accuracies]) for c in range(17)}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "classes = list(avg_acc.keys())\n",
    "accs = [avg_acc[c] * 100 for c in classes]\n",
    "names = [f\"C{c}\" for c in classes]\n",
    "\n",
    "plt.bar(range(17), accs)\n",
    "plt.xticks(range(17), names)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Class-wise Prediction Accuracy')\n",
    "for i, acc in enumerate(accs):\n",
    "    plt.text(i, acc + 1, f'{acc:.1f}%', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Worst 3 classes:\")\n",
    "worst = sorted(avg_acc.items(), key=lambda x: x[1])[:3]\n",
    "for c, acc in worst:\n",
    "    print(f\"Class {c}: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 - 현재 상태 그대로 저장\n",
    "def save_models():\n",
    "    \"\"\"학습한 모델들을 저장\"\"\"\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    save_dir = \"models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"🚨 모델 저장 시작...\")\n",
    "    \n",
    "    # 각 fold별 모델 저장 (fold_models 리스트가 있다고 가정)\n",
    "    try:\n",
    "        for fold in range(5):  # 5-fold라고 가정\n",
    "            model_path = f\"{save_dir}/fold_{fold}_model_{timestamp}.pth\"\n",
    "            \n",
    "            # fold_models[fold]가 존재한다면 저장\n",
    "            if 'fold_models' in globals() and len(fold_models) > fold:\n",
    "                torch.save({\n",
    "                    'model_state_dict': fold_models[fold].state_dict(),\n",
    "                    'fold': fold,\n",
    "                    'timestamp': timestamp,\n",
    "                    'epoch': 'unknown',  # 에포크 정보 모르면 unknown\n",
    "                }, model_path)\n",
    "                print(f\"✅ Fold {fold} 모델 저장 완료: {model_path}\")\n",
    "            \n",
    "            # 또는 best_models 리스트가 있다면\n",
    "            elif 'best_models' in globals() and len(best_models) > fold:\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_models[fold].state_dict(),\n",
    "                    'fold': fold,\n",
    "                    'timestamp': timestamp,\n",
    "                    'epoch': 'unknown',\n",
    "                }, model_path)\n",
    "                print(f\"✅ Fold {fold} best 모델 저장 완료: {model_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fold별 저장 실패: {e}\")\n",
    "    \n",
    "    # 전체 변수 상태 저장 (혹시 모르니까)\n",
    "    try:\n",
    "        state_path = f\"{save_dir}/full_state_{timestamp}.pth\"\n",
    "        \n",
    "        # 현재 글로벌 변수에서 모델 관련 객체들 찾아서 저장\n",
    "        save_dict = {}\n",
    "        \n",
    "        # 가능한 모델 변수명들 체크\n",
    "        possible_model_vars = ['model', 'models', 'fold_models', 'best_models', \n",
    "                              'tta_models', 'ensemble_models']\n",
    "        \n",
    "        for var_name in possible_model_vars:\n",
    "            if var_name in globals():\n",
    "                save_dict[var_name] = globals()[var_name]\n",
    "                print(f\"✅ {var_name} 변수 포함됨\")\n",
    "        \n",
    "        if save_dict:\n",
    "            torch.save(save_dict, state_path)\n",
    "            print(f\"✅ 전체 상태 저장 완료: {state_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 전체 상태 저장 실패: {e}\")\n",
    "    \n",
    "    print(f\"🎉 저장 완료! 저장 위치: {save_dir}/\")\n",
    "    print(f\"📁 파일 목록:\")\n",
    "    for file in os.listdir(save_dir):\n",
    "        print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 모델 저장 시작...\n",
      "❌ Fold별 저장 실패: 'collections.OrderedDict' object has no attribute 'state_dict'\n",
      "✅ model 변수 포함됨\n",
      "✅ fold_models 변수 포함됨\n",
      "✅ ensemble_models 변수 포함됨\n",
      "✅ 전체 상태 저장 완료: models/full_state_20250909_160737.pth\n",
      "🎉 저장 완료! 저장 위치: models/\n",
      "📁 파일 목록:\n",
      "   - full_state_20250909_160737.pth\n"
     ]
    }
   ],
   "source": [
    "save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "오답 데이터 분석 시작...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'collections.OrderedDict' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 90\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m오답 데이터 분석 시작...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     89\u001b[0m \u001b[38;5;66;03m# 현재 best 모델로 오답 분석 (Fold 1 기준)\u001b[39;00m\n\u001b[0;32m---> 90\u001b[0m wrong_preds \u001b[38;5;241m=\u001b[39m \u001b[43manalyze_wrong_predictions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;66;03m# 오답 요약 출력\u001b[39;00m\n\u001b[1;32m     93\u001b[0m print_wrong_class_summary(wrong_preds)\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36manalyze_wrong_predictions\u001b[0;34m(model, val_loader, device, num_samples)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21manalyze_wrong_predictions\u001b[39m(model, val_loader, device, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m):\n\u001b[1;32m      7\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"오답 데이터 분석 및 시각화\"\"\"\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m()\n\u001b[1;32m      9\u001b[0m     wrong_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'collections.OrderedDict' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "# # 오답 데이터 분석 및 시각화\n",
    "# import matplotlib.pyplot as plt\n",
    "# from PIL import Image\n",
    "# import numpy as np\n",
    "\n",
    "# def analyze_wrong_predictions(model, val_loader, device, num_samples=20):\n",
    "#     \"\"\"오답 데이터 분석 및 시각화\"\"\"\n",
    "#     model.eval()\n",
    "#     wrong_predictions = []\n",
    "    \n",
    "#     with torch.no_grad():\n",
    "#         for images, targets in val_loader:\n",
    "#             images, targets = images.to(device), targets.to(device)\n",
    "#             outputs = model(images)\n",
    "#             _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "#             # 오답인 샘플들 찾기\n",
    "#             wrong_mask = (preds != targets)\n",
    "#             wrong_indices = torch.where(wrong_mask)[0]\n",
    "            \n",
    "#             for idx in wrong_indices:\n",
    "#                 wrong_predictions.append({\n",
    "#                     'image': images[idx].cpu(),\n",
    "#                     'true_class': targets[idx].cpu().item(),\n",
    "#                     'pred_class': preds[idx].cpu().item(),\n",
    "#                     'confidence': torch.softmax(outputs[idx], 0).max().cpu().item()\n",
    "#                 })\n",
    "                \n",
    "#                 if len(wrong_predictions) >= num_samples:\n",
    "#                     break\n",
    "            \n",
    "#             if len(wrong_predictions) >= num_samples:\n",
    "#                 break\n",
    "    \n",
    "#     return wrong_predictions\n",
    "\n",
    "# def visualize_wrong_predictions(wrong_predictions, class_names, rows=4, cols=5):\n",
    "#     \"\"\"오답 데이터 시각화\"\"\"\n",
    "#     fig, axes = plt.subplots(rows, cols, figsize=(20, 16))\n",
    "#     axes = axes.flatten()\n",
    "    \n",
    "#     for i, wrong_pred in enumerate(wrong_predictions[:rows*cols]):\n",
    "#         if i >= len(axes):\n",
    "#             break\n",
    "            \n",
    "#         # 이미지 전처리 (정규화 해제)\n",
    "#         img = wrong_pred['image'].permute(1, 2, 0)\n",
    "#         img = img * torch.tensor([0.229, 0.224, 0.225]) + torch.tensor([0.485, 0.456, 0.406])\n",
    "#         img = torch.clamp(img, 0, 1)\n",
    "        \n",
    "#         axes[i].imshow(img)\n",
    "#         axes[i].set_title(f\"True: {wrong_pred['true_class']} | \"\n",
    "#                          f\"Pred: {wrong_pred['pred_class']}\\n\"\n",
    "#                          f\"Conf: {wrong_pred['confidence']:.3f}\", \n",
    "#                          fontsize=10)\n",
    "#         axes[i].axis('off')\n",
    "    \n",
    "#     # 빈 subplot 제거\n",
    "#     for i in range(len(wrong_predictions), len(axes)):\n",
    "#         axes[i].remove()\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.savefig('wrong_predictions_analysis.png', dpi=300, bbox_inches='tight')\n",
    "#     plt.show()\n",
    "\n",
    "# def print_wrong_class_summary(wrong_predictions):\n",
    "#     \"\"\"오답 클래스 요약 출력\"\"\"\n",
    "#     from collections import Counter\n",
    "    \n",
    "#     true_classes = [wp['true_class'] for wp in wrong_predictions]\n",
    "#     pred_classes = [wp['pred_class'] for wp in wrong_predictions]\n",
    "    \n",
    "#     print(\"=== 오답 분석 요약 ===\")\n",
    "#     print(\"실제 클래스별 오답 빈도:\")\n",
    "#     true_counter = Counter(true_classes)\n",
    "#     for class_id, count in sorted(true_counter.items()):\n",
    "#         print(f\"  클래스 {class_id}: {count}개 오답\")\n",
    "    \n",
    "#     print(\"\\n예측 클래스별 오답 빈도:\")\n",
    "#     pred_counter = Counter(pred_classes)\n",
    "#     for class_id, count in sorted(pred_counter.items()):\n",
    "#         print(f\"  클래스 {class_id}로 오예측: {count}개\")\n",
    "    \n",
    "#     print(f\"\\n총 분석된 오답 수: {len(wrong_predictions)}개\")\n",
    "\n",
    "# # 실행 코드\n",
    "# print(\"오답 데이터 분석 시작...\")\n",
    "\n",
    "# # 현재 best 모델로 오답 분석 (Fold 1 기준)\n",
    "# wrong_preds = analyze_wrong_predictions(best_model, val_loader, device, num_samples=20)\n",
    "\n",
    "# # 오답 요약 출력\n",
    "# print_wrong_class_summary(wrong_preds)\n",
    "\n",
    "# # 오답 이미지 시각화\n",
    "# visualize_wrong_predictions(wrong_preds, class_names=None, rows=4, cols=5)\n",
    "\n",
    "# print(\"오답 분석 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ensemble of all 5 fold models for inference\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold 앙상블 모델 준비\n",
    "ensemble_models = []\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "print(f\"Using ensemble of all {len(ensemble_models)} fold models for inference\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling 클래스 정의\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_tta_transforms = [\n",
    "    # 원본\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90도 회전들\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 밝기 개선\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 추론을 위한 Dataset 클래스\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # 여러 transform을 리스트로 받음\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 모든 transform을 적용한 결과를 리스트로 반환\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# TTA Dataset 생성\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"../data/sample_submission.csv\",\n",
    "    \"../data/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (배치 크기를 줄여서 메모리 절약)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTA는 메모리를 많이 사용하므로 배치 크기 줄임\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"TTA Dataset size: {len(tta_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_tta_inference(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold 모델 앙상블 + TTA 추론\"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # 각 fold 모델별 예측\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                # 각 TTA 변형별 예측\n",
    "                for images in images_list:\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ensemble TTA inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA: 100%|██████████| 50/50 [17:13<00:00, 20.67s/it]\n"
     ]
    }
   ],
   "source": [
    "# 앙상블 TTA 실행\n",
    "print(\"Starting Ensemble TTA inference...\")\n",
    "tta_predictions = ensemble_tta_inference(\n",
    "    models=ensemble_models, \n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 결과로 submission 파일 생성\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 기존 submission과 동일한 순서인지 확인\n",
    "sample_submission_df = pd.read_csv(\"../data/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA predictions saved\n",
      "TTA Prediction sample:\n"
     ]
    }
   ],
   "source": [
    "# TTA 결과 저장\n",
    "tta_pred_df.to_csv(\"../submission/choice.csv\", index=False)\n",
    "print(\"TTA predictions saved\")\n",
    "\n",
    "print(\"TTA Prediction sample:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "executionInfo": {
     "elapsed": 353,
     "status": "ok",
     "timestamp": 1700315247734,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "9yMO8s6GqAwZ",
    "outputId": "9a30616f-f0ea-439f-a906-dd806737ce00"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0008fdb22ddce0ce.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>00091bffdffd83de.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00396fbc1f6cc21d.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00471f8038d9c4b6.jpg</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00901f504008d884.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     ID  target\n",
       "0  0008fdb22ddce0ce.jpg       2\n",
       "1  00091bffdffd83de.jpg      12\n",
       "2  00396fbc1f6cc21d.jpg       5\n",
       "3  00471f8038d9c4b6.jpg      12\n",
       "4  00901f504008d884.jpg       2"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tta_pred_df.head()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
