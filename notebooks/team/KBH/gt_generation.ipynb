{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c869e453",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üöÄ GT Generation ÎÖ∏Ìä∏Î∂Å ÏãúÏûë!\")\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c00062b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 1: ÌôòÍ≤Ω ÏÑ§Ï†ï & Îç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "# =============================================================================\n",
    "\n",
    "# Í≤ΩÎ°ú ÏÑ§Ï†ï (Ïã§Ï†ú Í≤ΩÎ°úÎ°ú ÏàòÏ†ï ÌïÑÏöî)\n",
    "TEST_PATH = \"./data/raw/test\"  # test Ïù¥ÎØ∏ÏßÄ Ìè¥Îçî Í≤ΩÎ°ú\n",
    "META_PATH = \"./data/raw/meta.csv\"  # Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÌååÏùº Í≤ΩÎ°ú\n",
    "\n",
    "# Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ Î°úÎìú\n",
    "meta_df = pd.read_csv(META_PATH)\n",
    "print(\"üìã ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥:\")\n",
    "for idx, row in meta_df.iterrows():\n",
    "    print(f\"  {row['target']:2d}: {row['class_name']}\")\n",
    "\n",
    "print(f\"\\nüìÅ Test Ïù¥ÎØ∏ÏßÄ Í≤ΩÎ°ú: {TEST_PATH}\")\n",
    "test_files = [f for f in os.listdir(TEST_PATH) if f.endswith('.jpg')]\n",
    "print(f\"üìä Ï¥ù Test Ïù¥ÎØ∏ÏßÄ Ïàò: {len(test_files)}Ïû•\")\n",
    "\n",
    "# Ï∑®ÏïΩ ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "VULNERABLE_CLASSES = [3, 4, 7, 14]\n",
    "vuln_names = meta_df[meta_df['target'].isin(VULNERABLE_CLASSES)]['class_name'].tolist()\n",
    "print(f\"\\nüéØ Ï∑®ÏïΩ ÌÅ¥ÎûòÏä§ ({VULNERABLE_CLASSES}): {vuln_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a35603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 2: Ïù¥ÎØ∏ÏßÄ ÌíàÏßà Ï∏°Ï†ï Ìï®Ïàò\n",
    "# =============================================================================\n",
    "\n",
    "def assess_image_quality(image_path):\n",
    "    \"\"\"\n",
    "    Ïù¥ÎØ∏ÏßÄ ÌíàÏßàÏùÑ 0-1 Ï†êÏàòÎ°ú ÌèâÍ∞Ä (ÎÇÆÏùÑÏàòÎ°ù Ïò§ÏóºÎê®)\n",
    "    Î∏îÎü¨, ÎåÄÎπÑ, Î∞ùÍ∏∞ Î∂ÑÌè¨Î•º Ï¢ÖÌï©ÌïòÏó¨ Í≥ÑÏÇ∞\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return 0.0\n",
    "            \n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # 1) Î∏îÎü¨ Ï∏°Ï†ï (Laplacian variance)\n",
    "        # ÎÜíÏùÑÏàòÎ°ù ÏÑ†Î™ÖÌï®\n",
    "        blur_score = cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "        blur_normalized = min(blur_score / 1000.0, 1.0)  # 1000ÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "        \n",
    "        # 2) ÎåÄÎπÑ Ï∏°Ï†ï (ÌëúÏ§ÄÌé∏Ï∞®)\n",
    "        # ÎÜíÏùÑÏàòÎ°ù ÎåÄÎπÑÍ∞Ä Ï¢ãÏùå\n",
    "        contrast_score = gray.std()\n",
    "        contrast_normalized = min(contrast_score / 80.0, 1.0)  # 80ÏúºÎ°ú Ï†ïÍ∑úÌôî\n",
    "        \n",
    "        # 3) Î∞ùÍ∏∞ Î∂ÑÌè¨ Ï∏°Ï†ï (ÌûàÏä§ÌÜ†Í∑∏Îû® ÏóîÌä∏Î°úÌîº)\n",
    "        # ÎÜíÏùÑÏàòÎ°ù Î∞ùÍ∏∞ Î∂ÑÌè¨Í∞Ä Îã§ÏñëÌï®\n",
    "        hist = cv2.calcHist([gray], [0], None, [256], [0, 256])\n",
    "        hist = hist.flatten()\n",
    "        hist = hist[hist > 0]  # 0Ïù∏ Í∞í Ï†úÍ±∞\n",
    "        if len(hist) > 1:\n",
    "            prob = hist / hist.sum()\n",
    "            brightness_score = -np.sum(prob * np.log2(prob + 1e-8))\n",
    "            brightness_normalized = brightness_score / 8.0  # 8Î°ú Ï†ïÍ∑úÌôî (log2(256))\n",
    "        else:\n",
    "            brightness_normalized = 0.0\n",
    "            \n",
    "        # Í∞ÄÏ§ë ÌèâÍ∑†ÏúºÎ°ú ÏµúÏ¢Ö Ï†êÏàò Í≥ÑÏÇ∞\n",
    "        quality_score = (\n",
    "            0.5 * blur_normalized +      # Î∏îÎü¨Í∞Ä Í∞ÄÏû• Ï§ëÏöî\n",
    "            0.3 * contrast_normalized +   # ÎåÄÎπÑ\n",
    "            0.2 * brightness_normalized   # Î∞ùÍ∏∞ Î∂ÑÌè¨\n",
    "        )\n",
    "        \n",
    "        return min(quality_score, 1.0)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå ÌíàÏßà Ï∏°Ï†ï Ïã§Ìå® ({image_path}): {e}\")\n",
    "        return 0.0\n",
    "\n",
    "# ÌÖåÏä§Ìä∏ Ìï®Ïàò\n",
    "print(\"\\nüß™ ÌíàÏßà Ï∏°Ï†ï Ìï®Ïàò ÌÖåÏä§Ìä∏:\")\n",
    "if test_files:\n",
    "    sample_path = os.path.join(TEST_PATH, test_files[0])\n",
    "    sample_score = assess_image_quality(sample_path)\n",
    "    print(f\"  ÏÉòÌîå Ïù¥ÎØ∏ÏßÄ ({test_files[0]}): ÌíàÏßàÏ†êÏàò {sample_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a79dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adaptive_preprocessing(image_path, quality_threshold=0.4):\n",
    "    \"\"\"Î¨∏ÏÑú Ïù¥ÎØ∏ÏßÄ Ï†ÑÎ¨∏Í∞Ä ÏõåÌÅ¨ÌîåÎ°úÏö∞ Í∏∞Î∞ò Ï†ÑÏ≤òÎ¶¨\"\"\"\n",
    "    try:\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            return None, 0.0, 0.0\n",
    "        \n",
    "        original_quality = assess_image_quality(image_path)\n",
    "        if original_quality > quality_threshold:\n",
    "            return img, original_quality, original_quality\n",
    "        \n",
    "        # 1) ÌíàÏßà ÏßÑÎã®\n",
    "        processed_img = img.copy()\n",
    "        gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        g_std = gray.std()  # ÎåÄÎπÑ\n",
    "        lap_var = cv2.Laplacian(gray, cv2.CV_64F).var()  # Î∏îÎü¨\n",
    "        mean_bright = gray.mean()  # Î∞ùÍ∏∞\n",
    "        \n",
    "        # Ïä§ÌÅê Í∞ÅÎèÑ Ï∏°Ï†ï\n",
    "        edges = cv2.Canny(gray, 50, 150)\n",
    "        lines = cv2.HoughLines(edges, 1, np.pi/180, threshold=100)\n",
    "        skew_angle = 0\n",
    "        if lines is not None and len(lines) > 5:\n",
    "            angles = []\n",
    "            for line in lines[:20]:\n",
    "                if len(line[0]) >= 2:\n",
    "                    rho, theta = line[0]\n",
    "                    angle = theta * 180 / np.pi - 90\n",
    "                    if abs(angle) < 45:\n",
    "                        angles.append(angle)\n",
    "            if angles:\n",
    "                skew_angle = abs(np.median(angles))\n",
    "        \n",
    "        # 2) ÎùºÏö∞ÌåÖ Í≤∞Ï†ï\n",
    "        needs_deskew = skew_angle >= 8\n",
    "        low_contrast = g_std < 35\n",
    "        is_blurry = 50 <= lap_var < 150\n",
    "        very_blurry = lap_var < 50\n",
    "        too_bright = mean_bright > 180\n",
    "        has_noise = lap_var < 100  # ÎÖ∏Ïù¥Ï¶à Ï∂îÏ†ï\n",
    "        \n",
    "        # 3) ÌîÑÎ¶¨ÏÖã ÏÑ†ÌÉù\n",
    "        if very_blurry and too_bright and has_noise:\n",
    "            preset = \"HEAVY\"\n",
    "        elif needs_deskew or skew_angle >= 5:\n",
    "            preset = \"MEDIUM\"  \n",
    "        else:\n",
    "            preset = \"LIGHT\"\n",
    "        \n",
    "        # 4) Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö© (Í∂åÏû• ÏàúÏÑúÎåÄÎ°ú)\n",
    "        \n",
    "        # Îã®Í≥Ñ 0: ÌîåÎ¶Ω Í∞êÏßÄ Î∞è Î≥¥Ï†ï (Î™®Îì† ÌîÑÎ¶¨ÏÖãÏóê Ï†ÅÏö©)\n",
    "        flipped = cv2.flip(processed_img, 1)\n",
    "        gray_current = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "        gray_flipped = cv2.cvtColor(flipped, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        def get_text_density(image):\n",
    "            binary = cv2.adaptiveThreshold(image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, \n",
    "                                         cv2.THRESH_BINARY, 15, 10)\n",
    "            return np.sum(binary == 0) / binary.size\n",
    "        \n",
    "        original_density = get_text_density(gray_current)\n",
    "        flipped_density = get_text_density(gray_flipped)\n",
    "        \n",
    "        if flipped_density > original_density * 1.1:  # 10% Ïù¥ÏÉÅ Ï∞®Ïù¥\n",
    "            processed_img = flipped\n",
    "        \n",
    "        # Îã®Í≥Ñ 1: Deskew (ÌïÑÏöîÏãú)\n",
    "        if needs_deskew and preset in [\"MEDIUM\", \"HEAVY\"]:\n",
    "            h, w = processed_img.shape[:2]\n",
    "            center = (w//2, h//2)\n",
    "            \n",
    "            # Í∞ÅÎèÑ Í≥ÑÏÇ∞ Î°úÏßÅ ÏàòÏ†ï\n",
    "            valid_angles = []\n",
    "            if lines is not None:\n",
    "                for line in lines[:10]:\n",
    "                    if len(line[0]) >= 2:\n",
    "                        rho, theta = line[0]\n",
    "                        angle = theta * 180 / np.pi - 90\n",
    "                        if abs(angle) < 15:\n",
    "                            valid_angles.append(angle)\n",
    "            \n",
    "            if len(valid_angles) > 0:\n",
    "                rotation_angle = np.median(valid_angles)\n",
    "                if abs(rotation_angle) >= 3:\n",
    "                    M = cv2.getRotationMatrix2D(center, rotation_angle, 1.0)\n",
    "                    processed_img = cv2.warpAffine(processed_img, M, (w, h), \n",
    "                                                borderMode=cv2.BORDER_CONSTANT, \n",
    "                                                borderValue=(255, 255, 255))\n",
    "        \n",
    "        # Îã®Í≥Ñ 2: Î∞ùÍ∏∞ Ï°∞Ï†ï (HEAVYÎßå)\n",
    "        if too_bright and preset == \"HEAVY\":\n",
    "            processed_img = np.power(processed_img/255.0, 0.92) * 255\n",
    "            processed_img = processed_img.astype(np.uint8)\n",
    "        \n",
    "        # Îã®Í≥Ñ 3: NLM ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞ (HEAVYÎßå)\n",
    "        if has_noise and preset == \"HEAVY\":\n",
    "            processed_img = cv2.fastNlMeansDenoisingColored(processed_img, None, 3, 3, 7, 21)\n",
    "        \n",
    "        # Îã®Í≥Ñ 4: CLAHE (Ï†ÄÎåÄÎπÑÏù∏ Í≤ΩÏö∞)\n",
    "        if low_contrast:\n",
    "            lab = cv2.cvtColor(processed_img, cv2.COLOR_BGR2LAB)\n",
    "            l_channel = lab[:, :, 0]\n",
    "            clahe = cv2.createCLAHE(clipLimit=2.5, tileGridSize=(8, 8))\n",
    "            l_channel = clahe.apply(l_channel)\n",
    "            lab[:, :, 0] = l_channel\n",
    "            processed_img = cv2.cvtColor(lab, cv2.COLOR_LAB2BGR)\n",
    "        \n",
    "        # Îã®Í≥Ñ 5: Unsharp (Î∏îÎü¨Ïù∏ Í≤ΩÏö∞)\n",
    "        if is_blurry or very_blurry:\n",
    "            gray = cv2.cvtColor(processed_img, cv2.COLOR_BGR2GRAY)\n",
    "            gaussian = cv2.GaussianBlur(gray, (0, 0), 1.0)\n",
    "            unsharp = cv2.addWeighted(gray, 1.4, gaussian, -0.4, 0)\n",
    "            unsharp = np.clip(unsharp, 0, 255).astype(np.uint8)\n",
    "            \n",
    "            # HSVÏóêÏÑú VÏ±ÑÎÑêÎßå ÍµêÏ≤¥\n",
    "            hsv = cv2.cvtColor(processed_img, cv2.COLOR_BGR2HSV)\n",
    "            hsv[:, :, 2] = unsharp\n",
    "            processed_img = cv2.cvtColor(hsv, cv2.COLOR_HSV2BGR)\n",
    "        \n",
    "        # ÌíàÏßà Í≤ÄÏÇ¨\n",
    "        temp_path = image_path.replace('.jpg', '_temp_processed.jpg')\n",
    "        cv2.imwrite(temp_path, processed_img)\n",
    "        processed_quality = assess_image_quality(temp_path)\n",
    "        os.remove(temp_path)\n",
    "        \n",
    "        if processed_quality < original_quality * 0.8:\n",
    "            return img, original_quality, original_quality\n",
    "        \n",
    "        return processed_img, original_quality, processed_quality\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Ï†ÑÏ≤òÎ¶¨ Ïã§Ìå® ({image_path}): {e}\")\n",
    "        img = cv2.imread(image_path)\n",
    "        original_quality = assess_image_quality(image_path) if img is not None else 0.0\n",
    "        return img, original_quality, original_quality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf1a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Í≤ÄÏ¶ù Ïã§Ìñâ\n",
    "validation_results = test_preprocessing_effects(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e84bf36c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def before_after_visualization(selected_images, sample_count=20):\n",
    "    \"\"\"\n",
    "    Ï†ÑÏ≤òÎ¶¨ Ï†ÑÌõÑ ÎπÑÍµê ÏãúÍ∞ÅÌôî (20Í∞ú ÎûúÎç§ ÏÉòÌîå)\n",
    "    \"\"\"\n",
    "    print(f\"\\nÏ†ÑÏ≤òÎ¶¨ Ìö®Í≥º ÏãúÍ∞ÅÌôî - {sample_count}Í∞ú ÎûúÎç§ ÏÉòÌîå\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # ÎûúÎç§ ÏÉòÌîå ÏÑ†ÌÉù\n",
    "    import random\n",
    "    random.seed(42)  # Ïû¨ÌòÑ Í∞ÄÎä•Ìïú ÎûúÎç§\n",
    "    sample_images = random.sample(selected_images, min(sample_count, len(selected_images)))\n",
    "    \n",
    "    # Ï†ÑÏ≤òÎ¶¨ Í≤∞Í≥º Ï†ÄÏû•Ïö©\n",
    "    results = []\n",
    "    \n",
    "    for i, img_info in enumerate(sample_images):\n",
    "        img_path = os.path.join(TEST_PATH, img_info['filename'])\n",
    "        \n",
    "        # Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©\n",
    "        processed_img, orig_quality, new_quality = adaptive_preprocessing(img_path)\n",
    "        \n",
    "        if processed_img is not None:\n",
    "            results.append({\n",
    "                'filename': img_info['filename'],\n",
    "                'original_quality': orig_quality,\n",
    "                'processed_quality': new_quality,\n",
    "                'improvement': new_quality - orig_quality,\n",
    "                'processed_img': processed_img\n",
    "            })\n",
    "            \n",
    "            print(f\"{i+1:2d}. {img_info['filename'][:20]:20s} | \"\n",
    "                  f\"ÌíàÏßà: {orig_quality:.3f} ‚Üí {new_quality:.3f} | \"\n",
    "                  f\"Í∞úÏÑ†: {new_quality - orig_quality:+.3f}\")\n",
    "    \n",
    "    # ÏãúÍ∞ÅÌôî (4x5 Í∑∏Î¶¨ÎìúÎ°ú 20Í∞ú)\n",
    "    fig, axes = plt.subplots(4, 5, figsize=(25, 20))\n",
    "    fig.suptitle(f'Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º ÎπÑÍµê - {len(results)}Í∞ú ÏÉòÌîå', fontsize=16)\n",
    "    \n",
    "    for i, result in enumerate(results):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        \n",
    "        # ÏõêÎ≥∏ Ïù¥ÎØ∏ÏßÄ Î°úÎìú\n",
    "        original_img = cv2.imread(os.path.join(TEST_PATH, result['filename']))\n",
    "        original_img = cv2.cvtColor(original_img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ï†ÑÏ≤òÎ¶¨Îêú Ïù¥ÎØ∏ÏßÄ Î≥ÄÌôò\n",
    "        processed_img = cv2.cvtColor(result['processed_img'], cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Ï¢åÏö∞ ÎπÑÍµê Ïù¥ÎØ∏ÏßÄ ÏÉùÏÑ±\n",
    "        combined = np.hstack([original_img, processed_img])\n",
    "        \n",
    "        axes[row, col].imshow(combined)\n",
    "        axes[row, col].set_title(f\"{result['filename'][:15]}\\n\"\n",
    "                                f\"ÌíàÏßà: {result['original_quality']:.3f} ‚Üí \"\n",
    "                                f\"{result['processed_quality']:.3f}\\n\"\n",
    "                                f\"Í∞úÏÑ†: {result['improvement']:+.3f}\",\n",
    "                                fontsize=8)\n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67def156",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_improvement_analysis(preprocessing_results):\n",
    "    \"\"\"\n",
    "    Ï†ÑÏ≤òÎ¶¨ ÌíàÏßà Í∞úÏÑ† ÏàòÏπò Î∂ÑÏÑù\n",
    "    \"\"\"\n",
    "    print(\"\\nÌíàÏßà Í∞úÏÑ† Î∂ÑÏÑù Í≤∞Í≥º\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    if not preprocessing_results:\n",
    "        print(\"Î∂ÑÏÑùÌï† Îç∞Ïù¥ÌÑ∞Í∞Ä ÏóÜÏäµÎãàÎã§.\")\n",
    "        return\n",
    "    \n",
    "    # Í∞úÏÑ†ÎèÑ Í≥ÑÏÇ∞\n",
    "    improvements = [r['improvement'] for r in preprocessing_results]\n",
    "    original_qualities = [r['original_quality'] for r in preprocessing_results]\n",
    "    processed_qualities = [r['processed_quality'] for r in preprocessing_results]\n",
    "    \n",
    "    # ÌÜµÍ≥Ñ Í≥ÑÏÇ∞\n",
    "    avg_improvement = np.mean(improvements)\n",
    "    positive_count = sum(1 for imp in improvements if imp > 0)\n",
    "    significant_count = sum(1 for imp in improvements if imp > 0.05)  # 5% Ïù¥ÏÉÅ Í∞úÏÑ†\n",
    "    \n",
    "    print(f\"üìä Ï†ÑÏ≤¥ ÏÉòÌîå Ïàò: {len(preprocessing_results)}Í∞ú\")\n",
    "    print(f\"üìà ÌèâÍ∑† ÌíàÏßà Í∞úÏÑ†: {avg_improvement:+.3f}\")\n",
    "    print(f\"üìà ÌíàÏßà Ìñ•ÏÉÅ Ïù¥ÎØ∏ÏßÄ: {positive_count}Í∞ú ({positive_count/len(preprocessing_results)*100:.1f}%)\")\n",
    "    print(f\"üìà Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ† (5%+): {significant_count}Í∞ú ({significant_count/len(preprocessing_results)*100:.1f}%)\")\n",
    "    print()\n",
    "    print(f\"üìä ÏõêÎ≥∏ ÌíàÏßà Ï†êÏàò: {np.mean(original_qualities):.3f} ¬± {np.std(original_qualities):.3f}\")\n",
    "    print(f\"üìä Ï†ÑÏ≤òÎ¶¨ ÌõÑ ÌíàÏßà: {np.mean(processed_qualities):.3f} ¬± {np.std(processed_qualities):.3f}\")\n",
    "    print()\n",
    "    \n",
    "    # Í∞úÏÑ† Ìö®Í≥ºÎ≥Ñ Î∂ÑÎ•ò\n",
    "    excellent = [r for r in preprocessing_results if r['improvement'] > 0.1]  # 10% Ïù¥ÏÉÅ Í∞úÏÑ†\n",
    "    good = [r for r in preprocessing_results if 0.05 <= r['improvement'] <= 0.1]  # 5-10% Í∞úÏÑ†\n",
    "    moderate = [r for r in preprocessing_results if 0.02 <= r['improvement'] < 0.05]  # 2-5% Í∞úÏÑ†\n",
    "    minimal = [r for r in preprocessing_results if 0 < r['improvement'] < 0.02]  # ÎØ∏ÎØ∏Ìïú Í∞úÏÑ†\n",
    "    no_effect = [r for r in preprocessing_results if r['improvement'] <= 0]  # Í∞úÏÑ† ÏóÜÏùå/ÏïÖÌôî\n",
    "    \n",
    "    print(\"üìã Í∞úÏÑ† Ìö®Í≥ºÎ≥Ñ Î∂ÑÎ•ò:\")\n",
    "    print(f\"   üåü Îõ∞Ïñ¥ÎÇ® (10%+): {len(excellent)}Í∞ú\")\n",
    "    print(f\"   ‚úÖ ÏñëÌò∏ (5-10%): {len(good)}Í∞ú\") \n",
    "    print(f\"   üìä Î≥¥ÌÜµ (2-5%): {len(moderate)}Í∞ú\")\n",
    "    print(f\"   üìâ ÎØ∏ÎØ∏ (0-2%): {len(minimal)}Í∞ú\")\n",
    "    print(f\"   ‚ùå Ìö®Í≥ºÏóÜÏùå/ÏïÖÌôî: {len(no_effect)}Í∞ú\")\n",
    "    \n",
    "    # Í∞ÄÏû• Í∞úÏÑ†Îêú ÏºÄÏù¥Ïä§ Ï∂úÎ†•\n",
    "    if excellent:\n",
    "        best_case = max(excellent, key=lambda x: x['improvement'])\n",
    "        print(f\"\\nüèÜ ÏµúÍ≥† Í∞úÏÑ† ÏÇ¨Î°Ä:\")\n",
    "        print(f\"   ÌååÏùº: {best_case['filename']}\")\n",
    "        print(f\"   ÌíàÏßà: {best_case['original_quality']:.3f} ‚Üí {best_case['processed_quality']:.3f}\")\n",
    "        print(f\"   Í∞úÏÑ†: +{best_case['improvement']:.3f} ({best_case['improvement']/best_case['original_quality']*100:+.1f}%)\")\n",
    "    \n",
    "    # Í≤∞Î°†\n",
    "    print(\"\\nüéØ Í≤∞Î°†:\")\n",
    "    if avg_improvement > 0.05:\n",
    "        print(\"   ‚úÖ Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Îõ∞Ïñ¥ÎÇ® - Ïã§Ï†ú Ï∂îÎ°†Ïóê Ï†ÅÏö© Í∂åÏû•\")\n",
    "    elif avg_improvement > 0.02:\n",
    "        print(\"   üìä Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Î≥¥ÌÜµ - Ï∂îÍ∞Ä Ïã§Ìóò ÌõÑ Í≤∞Ï†ï\")\n",
    "    else:\n",
    "        print(\"   ‚ùå Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º ÎØ∏ÎØ∏ - Îã§Î•∏ Î∞©Î≤ï Î™®ÏÉâ ÌïÑÏöî\")\n",
    "    \n",
    "    return {\n",
    "        'avg_improvement': avg_improvement,\n",
    "        'positive_ratio': positive_count / len(preprocessing_results),\n",
    "        'significant_ratio': significant_count / len(preprocessing_results),\n",
    "        'excellent_cases': len(excellent),\n",
    "        'total_samples': len(preprocessing_results)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Ï†ÑÏ≤òÎ¶¨ Ìï®ÏàòÎì§Ïù¥ Ï†ïÏùòÎêòÏóàÏäµÎãàÎã§!\")\n",
    "print(\"   - adaptive_preprocessing(): ÌíàÏßà Í∏∞Î∞ò Ï†ÅÏùëÌòï Ï†ÑÏ≤òÎ¶¨\")  \n",
    "print(\"   - before_after_visualization(): Ï†ÑÌõÑ ÎπÑÍµê ÏãúÍ∞ÅÌôî\")\n",
    "print(\"   - quality_improvement_analysis(): ÏàòÏπò Î∂ÑÏÑù\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf885270",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 3: Î©îÏù∏Î™®Îç∏ Ï∂îÎ°†ÏùÑ ÏúÑÌïú Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§\n",
    "# =============================================================================\n",
    "\n",
    "class SimpleTestDataset(Dataset):\n",
    "    \"\"\"Î©îÏù∏Î™®Îç∏ Ï∂îÎ°†Ïö© Í∞ÑÎã®Ìïú Îç∞Ïù¥ÌÑ∞ÏÖã\"\"\"\n",
    "    def __init__(self, image_files, test_path, img_size=384):\n",
    "        self.image_files = image_files\n",
    "        self.test_path = test_path\n",
    "        self.img_size = img_size\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.test_path, self.image_files[idx])\n",
    "        \n",
    "        try:        \n",
    "            # Ïù¥ÎØ∏ÏßÄ Î°úÎìú Î∞è Ï†ÑÏ≤òÎ¶¨\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize((self.img_size, self.img_size))\n",
    "            img = np.array(img, dtype=np.float32) / 255.0\n",
    "            \n",
    "            # Ï†ïÍ∑úÌôî (ImageNet ÌÜµÍ≥Ñ)\n",
    "            mean = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
    "            std = np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "            img = (img - mean) / std\n",
    "            \n",
    "            # CHW ÌòïÌÉúÎ°ú Î≥ÄÌôò\n",
    "            img = torch.from_numpy(img).permute(2, 0, 1).float()\n",
    "            \n",
    "            return img, self.image_files[idx]\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Ïù¥ÎØ∏ÏßÄ Î°úÎìú Ïã§Ìå® ({self.image_files[idx]}): {e}\")\n",
    "            # ÏóêÎü¨ Ïãú ÎçîÎØ∏ ÌÖêÏÑú Î∞òÌôò (float32)\n",
    "            dummy_tensor = torch.zeros(3, self.img_size, self.img_size, dtype=torch.float32)\n",
    "            return dummy_tensor, self.image_files[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e3f87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 4: Î©îÏù∏Î™®Îç∏ Î°úÎìú Î∞è Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ Ï∂îÏ∂ú\n",
    "# =============================================================================\n",
    "\n",
    "# def extract_vulnerable_candidates():\n",
    "#     \"\"\"Î©îÏù∏Î™®Îç∏Î°ú Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Ï∂îÏ∂ú\"\"\"\n",
    "#     print(\"\\nüîÑ Î©îÏù∏Î™®Îç∏Î°ú Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ Ï∂îÏ∂ú Ï§ë...\")\n",
    "    \n",
    "#     # TODO: Ïã§Ï†ú Î™®Îç∏ Í≤ΩÎ°úÎ°ú ÏàòÏ†ï ÌïÑÏöî\n",
    "#     # Ïó¨Í∏∞ÏÑúÎäî ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î°ú ÎåÄÏ≤¥\n",
    "#     print(\"‚ö†Ô∏è  Ïã§Ï†ú Íµ¨ÌòÑ Ïãú Î™®Îç∏ Î°úÎìú ÏΩîÎìúÎ°ú ÍµêÏ≤¥ ÌïÑÏöî\")\n",
    "#     print(\"   ÌòÑÏû¨Îäî ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î°ú ÏßÑÌñâ\")\n",
    "    \n",
    "#     # ÎçîÎØ∏ Ï∂îÎ°† Í≤∞Í≥º ÏÉùÏÑ± (Ïã§Ï†úÎ°úÎäî Î™®Îç∏ Ï∂îÎ°† Í≤∞Í≥º ÏÇ¨Ïö©)\n",
    "#     # Ï∑®ÏïΩÌÅ¥ÎûòÏä§Î°ú ÏòàÏ∏°Îê† Í∞ÄÎä•ÏÑ±Ïù¥ ÎÜíÏùÄ Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ ÏûÑÏùòÎ°ú ÏÑ†ÌÉù\n",
    "#     np.random.seed(42)\n",
    "    \n",
    "#     # Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄ Ï§ë 20-30% Ï†ïÎèÑÍ∞Ä Ï∑®ÏïΩÌÅ¥ÎûòÏä§Î°ú ÏòàÏ∏°ÎêúÎã§Í≥† Í∞ÄÏ†ï\n",
    "#     n_candidates = min(int(len(test_files) * 0.25), 1200)\n",
    "#     candidate_indices = np.random.choice(len(test_files), n_candidates, replace=False)\n",
    "    \n",
    "#     # ÌõÑÎ≥¥ Ïù¥ÎØ∏ÏßÄ ÌååÏùºÎ™ÖÍ≥º ÏòàÏ∏° ÌÅ¥ÎûòÏä§ ÏÉùÏÑ±\n",
    "#     candidates = []\n",
    "#     for idx in candidate_indices:\n",
    "#         filename = test_files[idx]\n",
    "#         predicted_class = np.random.choice(VULNERABLE_CLASSES)\n",
    "#         candidates.append({\n",
    "#             'filename': filename,\n",
    "#             'predicted_class': predicted_class,\n",
    "#             'confidence': np.random.uniform(0.3, 0.9)  # ÎçîÎØ∏ Ïã†Î¢∞ÎèÑ\n",
    "#         })\n",
    "    \n",
    "#     print(f\"‚úÖ Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ {len(candidates)}Ïû• Ï∂îÏ∂ú ÏôÑÎ£å\")\n",
    "#     print(f\"   ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨: {Counter(c['predicted_class'] for c in candidates)}\")\n",
    "    \n",
    "#     return candidates\n",
    "\n",
    "def load_ensemble_models(model_paths):\n",
    "    \"\"\"5-fold ÏïôÏÉÅÎ∏î Î™®Îç∏Îì§ÏùÑ Î°úÎìú\"\"\"\n",
    "    models = []\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    for i, model_path in enumerate(model_paths):\n",
    "        print(f\"   üìÇ Fold {i+1} Î™®Îç∏ Î°úÎî©: {model_path}\")\n",
    "        \n",
    "        # ConvNeXt Base Î™®Îç∏ ÏÉùÏÑ± (Ïã§Ï†ú Î™®Îç∏Ïóê ÎßûÍ≤å ÏàòÏ†ï)\n",
    "        model = timm.create_model('convnext_base_384_in22ft1k', \n",
    "                                  pretrained=False, \n",
    "                                  num_classes=17)\n",
    "        \n",
    "        # Ï†ÄÏû•Îêú Í∞ÄÏ§ëÏπò Î°úÎìú\n",
    "        checkpoint = torch.load(model_path, map_location=device)\n",
    "        \n",
    "        # state_dict ÌÇ§ Ï≤òÎ¶¨ (Ï†ÄÏû• Î∞©ÏãùÏóê Îî∞Îùº Ï°∞Ï†ï ÌïÑÏöî)\n",
    "        if 'model_state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        elif 'state_dict' in checkpoint:\n",
    "            model.load_state_dict(checkpoint['state_dict'])\n",
    "        else:\n",
    "            model.load_state_dict(checkpoint)\n",
    "        \n",
    "        model.to(device)\n",
    "        model.eval()\n",
    "        models.append(model)\n",
    "        print(f\"   ‚úÖ Fold {i+1} Î™®Îç∏ Î°úÎìú ÏôÑÎ£å\")\n",
    "    \n",
    "    return models, device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1863cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_vulnerable_candidates():\n",
    "    \"\"\"Î©îÏù∏Î™®Îç∏Î°ú Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ Ï∂îÏ∂ú\"\"\"\n",
    "    print(\"\\nüîÑ 5-fold ÏïôÏÉÅÎ∏î Î™®Îç∏Î°ú Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ Ï∂îÏ∂ú Ï§ë...\")\n",
    "\n",
    "    MODEL_PATHS = [\n",
    "        # ÏòàÏãú Í≤ΩÎ°ú - Ïã§Ï†ú ÌååÏùº Í≤ΩÎ°úÎ°ú Î≥ÄÍ≤Ω ÌïÑÏöî\n",
    "        \"./notebooks/team/KBH/models/fold_1_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_2_best.pth\", \n",
    "        \"./notebooks/team/KBH/models/fold_3_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_4_best.pth\",\n",
    "        \"./notebooks/team/KBH/models/fold_5_best.pth\"\n",
    "    ]\n",
    "    \n",
    "    # Ïã§Ï†ú ÌååÏùº Ï°¥Ïû¨ ÌôïÏù∏\n",
    "    missing_files = [path for path in MODEL_PATHS if not os.path.exists(path)]\n",
    "    if missing_files:\n",
    "        print(\"‚ùå Îã§Ïùå Î™®Îç∏ ÌååÏùºÎì§Ïù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§:\")\n",
    "        for missing in missing_files:\n",
    "            print(f\"   {missing}\")\n",
    "        print(\"\\nüí° Ìï¥Í≤∞ Î∞©Î≤ï:\")\n",
    "        print(\"   1) MODEL_PATHSÎ•º Ïã§Ï†ú Î™®Îç∏ ÌååÏùº Í≤ΩÎ°úÎ°ú ÏàòÏ†ï\")\n",
    "        print(\"   2) ÎòêÎäî ÎçîÎØ∏ Î™®ÎìúÎ°ú Ïã§ÌñâÌïòÎ†§Î©¥ return Î¨∏ Îí§Ïùò ÎçîÎØ∏ ÏΩîÎìú ÏÇ¨Ïö©\")\n",
    "        print(\"   3) main.ipynbÏóêÏÑú Î™®Îç∏ ÌååÏùºÎì§ÏùÑ ÌôïÏù∏ ÌõÑ Í≤ΩÎ°ú Î≥µÏÇ¨\")\n",
    "        \n",
    "        # ÎçîÎØ∏ Î™®ÎìúÎ°ú ÎåÄÏ≤¥\n",
    "        print(\"\\nüîÑ ÎçîÎØ∏ Î™®ÎìúÎ°ú ÏßÑÌñâ...\")\n",
    "        return extract_vulnerable_candidates_dummy()\n",
    "    \n",
    "    try:\n",
    "        # Î™®Îç∏ Î°úÎìú\n",
    "        models, device = load_ensemble_models(MODEL_PATHS)\n",
    "        \n",
    "        # ÌÖåÏä§Ìä∏ Îç∞Ïù¥ÌÑ∞ÏÖã ÏÉùÏÑ±\n",
    "        test_dataset = SimpleTestDataset(test_files, TEST_PATH, img_size=384)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "        \n",
    "        print(f\"üìä Ï†ÑÏ≤¥ {len(test_files)}Ïû• Ïù¥ÎØ∏ÏßÄ Ï∂îÎ°† ÏãúÏûë...\")\n",
    "        \n",
    "        # 5-fold ÏïôÏÉÅÎ∏î Ï∂îÎ°†\n",
    "        all_predictions = []\n",
    "        all_confidences = []\n",
    "        all_filenames = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images, filenames) in enumerate(test_loader):\n",
    "                if batch_idx % 20 == 0:\n",
    "                    progress = (batch_idx * test_loader.batch_size) / len(test_files) * 100\n",
    "                    print(f\"   ÏßÑÌñâÎ•†: {progress:.1f}% ({batch_idx * test_loader.batch_size}/{len(test_files)})\")\n",
    "                \n",
    "                images = images.to(device)\n",
    "                \n",
    "                # 5Í∞ú Î™®Îç∏Ïùò ÏòàÏ∏° Í≤∞Í≥ºÎ•º ÌèâÍ∑†\n",
    "                ensemble_outputs = []\n",
    "                for model in models:\n",
    "                    outputs = model(images)\n",
    "                    probabilities = torch.softmax(outputs, dim=1)\n",
    "                    ensemble_outputs.append(probabilities)\n",
    "                \n",
    "                # ÏïôÏÉÅÎ∏î ÌèâÍ∑†\n",
    "                ensemble_probs = torch.stack(ensemble_outputs).mean(dim=0)\n",
    "                predicted_classes = ensemble_probs.argmax(dim=1)\n",
    "                max_confidences = ensemble_probs.max(dim=1)[0]\n",
    "                \n",
    "                # Í≤∞Í≥º Ï†ÄÏû•\n",
    "                for i in range(len(filenames)):\n",
    "                    pred_class = predicted_classes[i].cpu().item()\n",
    "                    confidence = max_confidences[i].cpu().item()\n",
    "                    filename = filenames[i]\n",
    "                    \n",
    "                    all_predictions.append(pred_class)\n",
    "                    all_confidences.append(confidence)\n",
    "                    all_filenames.append(filename)\n",
    "        \n",
    "        # Ï∑®ÏïΩÌÅ¥ÎûòÏä§Î°ú ÏòàÏ∏°Îêú Ïù¥ÎØ∏ÏßÄÎì§Îßå ÌïÑÌÑ∞ÎßÅ\n",
    "        candidates = []\n",
    "        for i, (filename, pred_class, confidence) in enumerate(zip(all_filenames, all_predictions, all_confidences)):\n",
    "            if pred_class in VULNERABLE_CLASSES:\n",
    "                candidates.append({\n",
    "                    'filename': filename,\n",
    "                    'predicted_class': pred_class,\n",
    "                    'confidence': confidence\n",
    "                })\n",
    "        \n",
    "        print(f\"‚úÖ Ïã§Ï†ú Î™®Îç∏ Ï∂îÎ°† ÏôÑÎ£å!\")\n",
    "        print(f\"   Ï†ÑÏ≤¥ Ïù¥ÎØ∏ÏßÄ: {len(test_files)}Ïû•\")\n",
    "        print(f\"   Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥: {len(candidates)}Ïû• ({len(candidates)/len(test_files)*100:.1f}%)\")\n",
    "        print(f\"   ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨: {Counter(c['predicted_class'] for c in candidates)}\")\n",
    "        \n",
    "        # GPU Î©îÎ™®Î¶¨ Ï†ïÎ¶¨\n",
    "        del models\n",
    "        torch.cuda.empty_cache() if torch.cuda.is_available() else None\n",
    "        \n",
    "        return candidates\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Î™®Îç∏ Ï∂îÎ°† Ï§ë ÏóêÎü¨ Î∞úÏÉù: {e}\")\n",
    "        print(\"üîÑ ÎçîÎØ∏ Î™®ÎìúÎ°ú ÎåÄÏ≤¥...\")\n",
    "        return extract_vulnerable_candidates_dummy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1184ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def extract_vulnerable_candidates_dummy():\n",
    "#     \"\"\"ÎçîÎØ∏ Îç∞Ïù¥ÌÑ∞Î°ú ÌõÑÎ≥¥ Ï∂îÏ∂ú (Î™®Îç∏ Î°úÎìú Ïã§Ìå®Ïãú ÎåÄÏ≤¥Ïö©)\"\"\"\n",
    "#     print(\"‚ö†Ô∏è  ÎçîÎØ∏ Î™®ÎìúÎ°ú Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ ÏÉùÏÑ±\")\n",
    "    \n",
    "#     np.random.seed(42)\n",
    "#     n_candidates = min(int(len(test_files) * 0.25), 1200)\n",
    "#     candidate_indices = np.random.choice(len(test_files), n_candidates, replace=False)\n",
    "    \n",
    "#     candidates = []\n",
    "#     for idx in candidate_indices:\n",
    "#         filename = test_files[idx]\n",
    "#         predicted_class = np.random.choice(VULNERABLE_CLASSES)\n",
    "#         candidates.append({\n",
    "#             'filename': filename,\n",
    "#             'predicted_class': predicted_class,\n",
    "#             'confidence': np.random.uniform(0.3, 0.9)\n",
    "#         })\n",
    "    \n",
    "#     print(f\"‚úÖ ÎçîÎØ∏ Ï∑®ÏïΩÌÅ¥ÎûòÏä§ ÌõÑÎ≥¥ {len(candidates)}Ïû• ÏÉùÏÑ± ÏôÑÎ£å\")\n",
    "#     return candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb3c49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÌõÑÎ≥¥ Ï∂îÏ∂ú Ïã§Ìñâ\n",
    "vulnerable_candidates = extract_vulnerable_candidates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84620efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5: Ïò§ÏóºÎèÑ Í∏∞Î∞ò Ïù¥ÎØ∏ÏßÄ ÏÑ†Î≥Ñ\n",
    "# =============================================================================\n",
    "\n",
    "def select_most_corrupted(candidates, target_count=500):\n",
    "    \"\"\"Ïò§ÏóºÎèÑÍ∞Ä ÎÜíÏùÄ Ïù¥ÎØ∏ÏßÄÎì§ÏùÑ ÏÑ†Î≥Ñ\"\"\"\n",
    "    print(f\"\\nüìä Ïò§ÏóºÎèÑ Í∏∞Î∞ò ÏÉÅÏúÑ {target_count}Ïû• ÏÑ†Î≥Ñ Ï§ë...\")\n",
    "    \n",
    "    # Í∞Å ÌõÑÎ≥¥ Ïù¥ÎØ∏ÏßÄÏùò ÌíàÏßàÏ†êÏàò Í≥ÑÏÇ∞\n",
    "    quality_scores = []\n",
    "    for i, candidate in enumerate(candidates):\n",
    "        if i % 100 == 0:\n",
    "            print(f\"   ÏßÑÌñâÎ•†: {i}/{len(candidates)} ({i/len(candidates)*100:.1f}%)\")\n",
    "        \n",
    "        img_path = os.path.join(TEST_PATH, candidate['filename'])\n",
    "        quality_score = assess_image_quality(img_path)\n",
    "        \n",
    "        quality_scores.append({\n",
    "            'filename': candidate['filename'],\n",
    "            'predicted_class': candidate['predicted_class'],\n",
    "            'confidence': candidate['confidence'],\n",
    "            'quality_score': quality_score\n",
    "        })\n",
    "    \n",
    "    # ÌíàÏßàÏ†êÏàò ÎÇÆÏùÄ ÏàúÏúºÎ°ú Ï†ïÎ†¨ (Ïò§ÏóºÎèÑ ÎÜíÏùÄ Ïàú)\n",
    "    quality_scores.sort(key=lambda x: x['quality_score'])\n",
    "    \n",
    "    # ÏÉÅÏúÑ target_countÍ∞ú ÏÑ†ÌÉù\n",
    "    selected = quality_scores[:target_count]\n",
    "    \n",
    "    print(f\"‚úÖ ÏµúÏ¢Ö {len(selected)}Ïû• ÏÑ†Î≥Ñ ÏôÑÎ£å\")\n",
    "    print(f\"   ÌíàÏßàÏ†êÏàò Î≤îÏúÑ: {selected[0]['quality_score']:.3f} ~ {selected[-1]['quality_score']:.3f}\")\n",
    "    \n",
    "    # CSVÎ°ú Ï†ÄÏû•\n",
    "    selected_df = pd.DataFrame(selected)\n",
    "    selected_df.to_csv('selected_images_500.csv', index=False)\n",
    "    print(\"üíæ selected_images_500.csv ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å\")\n",
    "    \n",
    "    return selected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8985a28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ïò§ÏóºÎèÑ Í∏∞Î∞ò ÏÑ†Î≥Ñ Ïã§Ìñâ\n",
    "selected_images = select_most_corrupted(vulnerable_candidates, target_count=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fca689a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 5.5: Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Í≤ÄÏ¶ù (STEP 5 Îã§ÏùåÏóê ÏÇΩÏûÖ)  \n",
    "# =============================================================================\n",
    "\n",
    "def test_preprocessing_effects(selected_images):\n",
    "    \"\"\"\n",
    "    ÏÑ†Î≥ÑÎêú Ïò§Ïóº Îç∞Ïù¥ÌÑ∞Î°ú Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º ÌÖåÏä§Ìä∏\n",
    "    \"\"\"\n",
    "    print(\"\\nüß™ Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Í≤ÄÏ¶ù ÏãúÏûë!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üìã Í≤ÄÏ¶ù Îã®Í≥Ñ:\")\n",
    "    print(\"   1Ô∏è‚É£ 20Í∞ú ÎûúÎç§ ÏÉòÌîå Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö©\")  \n",
    "    print(\"   2Ô∏è‚É£ Ï†ÑÌõÑ ÎπÑÍµê ÏãúÍ∞ÅÌôî\")\n",
    "    print(\"   3Ô∏è‚É£ ÌíàÏßà Í∞úÏÑ† ÏàòÏπò Î∂ÑÏÑù\")\n",
    "    print(\"   4Ô∏è‚É£ Ìö®Í≥º ÌåêÏ†ï Î∞è Í∂åÏû•ÏÇ¨Ìï≠\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # STEP 1: Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö© Î∞è ÏãúÍ∞ÅÌôî\n",
    "    print(\"\\n1Ô∏è‚É£ Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö© Î∞è ÏãúÍ∞ÅÌôî ÏßÑÌñâÏ§ë...\")\n",
    "    preprocessing_results = before_after_visualization(selected_images, sample_count=20)\n",
    "    \n",
    "    # STEP 2: ÏàòÏπò Î∂ÑÏÑù\n",
    "    print(\"\\n2Ô∏è‚É£ ÌíàÏßà Í∞úÏÑ† ÏàòÏπò Î∂ÑÏÑù ÏßÑÌñâÏ§ë...\")\n",
    "    analysis_results = quality_improvement_analysis(preprocessing_results)\n",
    "    \n",
    "    # STEP 3: Ìö®Í≥º ÌåêÏ†ï\n",
    "    print(\"\\n3Ô∏è‚É£ ÏµúÏ¢Ö Ìö®Í≥º ÌåêÏ†ï\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    avg_improvement = analysis_results['avg_improvement']\n",
    "    positive_ratio = analysis_results['positive_ratio'] \n",
    "    significant_ratio = analysis_results['significant_ratio']\n",
    "    \n",
    "    # ÌåêÏ†ï Í∏∞Ï§Ä\n",
    "    if avg_improvement >= 0.05 and significant_ratio >= 0.4:\n",
    "        recommendation = \"üåü Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö© Í∞ïÎ†• Í∂åÏû•\"\n",
    "        action = \"inf_new.ipynbÏóê Ï†ÑÏ≤òÎ¶¨ Î°úÏßÅ ÌÜµÌï© ÌõÑ Ï†ÑÏ≤¥ Ï∂îÎ°† Ïã§Ìñâ\"\n",
    "        confidence = \"ÎÜíÏùå\"\n",
    "    elif avg_improvement >= 0.02 and positive_ratio >= 0.6:\n",
    "        recommendation = \"‚úÖ Ï†ÑÏ≤òÎ¶¨ Ï†ÅÏö© Í∂åÏû•\" \n",
    "        action = \"Ï∂îÍ∞Ä ÏÉòÌîåÎ°ú Í≤ÄÏ¶ù ÌõÑ Ï†ÅÏö© Ïó¨Î∂Ä Í≤∞Ï†ï\"\n",
    "        confidence = \"Î≥¥ÌÜµ\"\n",
    "    else:\n",
    "        recommendation = \"‚ùå Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Î∂ÄÏ°±\"\n",
    "        action = \"Îã§Î•∏ Ï†ÑÏ≤òÎ¶¨ Î∞©Î≤ï ÎòêÎäî Ï¶ùÍ∞ï Í∏∞Î≤ï Î™®ÏÉâ\"\n",
    "        confidence = \"ÎÇÆÏùå\"\n",
    "    \n",
    "    print(f\"üìä ÌåêÏ†ï Í≤∞Í≥º: {recommendation}\")\n",
    "    print(f\"üéØ Í∂åÏû• Ï°∞Ïπò: {action}\")  \n",
    "    print(f\"üîç Ïã†Î¢∞ÎèÑ: {confidence}\")\n",
    "    \n",
    "    # ÏÉÅÏÑ∏ ÏàòÏπò\n",
    "    print(\"\\nüìà ÏÉÅÏÑ∏ ÏßÄÌëú:\")\n",
    "    print(f\"   ‚Ä¢ ÌèâÍ∑† ÌíàÏßà Í∞úÏÑ†: {avg_improvement:+.3f}\")\n",
    "    print(f\"   ‚Ä¢ Í∞úÏÑ†Îêú Ïù¥ÎØ∏ÏßÄ ÎπÑÏú®: {positive_ratio*100:.1f}%\")\n",
    "    print(f\"   ‚Ä¢ Ïú†ÏùòÎØ∏Ìïú Í∞úÏÑ† ÎπÑÏú®: {significant_ratio*100:.1f}%\")  \n",
    "    print(f\"   ‚Ä¢ Îõ∞Ïñ¥ÎÇú Í∞úÏÑ† ÏÇ¨Î°Ä: {analysis_results['excellent_cases']}Í∞ú\")\n",
    "    \n",
    "    # Îã§Ïùå Îã®Í≥Ñ Í∞ÄÏù¥Îìú\n",
    "    print(\"\\nüöÄ Îã§Ïùå Îã®Í≥Ñ:\")\n",
    "    if avg_improvement >= 0.02:\n",
    "        print(\"   1. inf_new.ipynb ÏΩîÎìúÏóê Ï†ÑÏ≤òÎ¶¨ Ìï®Ïàò ÌÜµÌï©\")\n",
    "        print(\"   2. TTAImageDataset.__getitem__Ïóê adaptive_preprocessing Ï∂îÍ∞Ä\")\n",
    "        print(\"   3. Ï†ÑÏ≤¥ 3140Í∞ú Ïù¥ÎØ∏ÏßÄÎ°ú Ï∂îÎ°† Ïã§Ìñâ\")\n",
    "        print(\"   4. Ï†úÏ∂ú Ï†êÏàò ÎπÑÍµê (Ï†ÑÏ≤òÎ¶¨ Ï†Ñ vs ÌõÑ)\")\n",
    "    else:\n",
    "        print(\"   1. Îã§Î•∏ Ï†ÑÏ≤òÎ¶¨ Í∏∞Î≤ï ÏãúÎèÑ:\")\n",
    "        print(\"      - Îçî Í∞ïÌïú ÎÖ∏Ïù¥Ï¶à Ï†úÍ±∞\")\n",
    "        print(\"      - ÌöåÏ†Ñ Í∞ÅÎèÑ Î≥¥Ï†ï\")  \n",
    "        print(\"      - ÍπäÏù¥ ÌïôÏäµ Í∏∞Î∞ò super-resolution\")\n",
    "        print(\"   2. ÎòêÎäî Îç∞Ïù¥ÌÑ∞ Ï¶ùÍ∞ï Í∞ïÌôî\")\n",
    "        print(\"   3. ÎòêÎäî Î™®Îç∏ ÏïôÏÉÅÎ∏î ÏµúÏ†ÅÌôî\")\n",
    "    \n",
    "    return {\n",
    "        'recommendation': recommendation,\n",
    "        'action': action,\n",
    "        'confidence': confidence,\n",
    "        'should_apply': avg_improvement >= 0.02,\n",
    "        'preprocessing_results': preprocessing_results,\n",
    "        'analysis_results': analysis_results\n",
    "    }\n",
    "\n",
    "# Ïã§Ìñâ Í∞ÄÏù¥Îìú\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Í≤ÄÏ¶ù Ïã§Ìñâ Í∞ÄÏù¥Îìú\")  \n",
    "print(\"=\"*60)\n",
    "print(\"üìù Îã§Ïùå ÏÖÄÏóêÏÑú Ïã§ÌñâÌïòÏÑ∏Ïöî:\")\n",
    "print(\"   validation_results = test_preprocessing_effects(selected_images)\")\n",
    "print()\n",
    "print(\"üîç Ïù¥ÎØ∏ selected_images_500.csv ÌååÏùºÏù¥ ÏûàÎã§Î©¥:\")\n",
    "print(\"   selected_images = pd.read_csv('selected_images_500.csv').to_dict('records')\")\n",
    "print(\"   validation_results = test_preprocessing_effects(selected_images)\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0206afb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selected_images_500.csvÍ∞Ä Ïù¥ÎØ∏ ÏûàÎã§Î©¥:\n",
    "selected_images = pd.read_csv('selected_images_500.csv').to_dict('records')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac43852b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ï†ÑÏ≤òÎ¶¨ Ìö®Í≥º Í≤ÄÏ¶ù Ïã§Ìñâ\n",
    "validation_results = test_preprocessing_effects(selected_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08a124dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 6: ÏàòÎèô ÎùºÎ≤®ÎßÅ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§\n",
    "# =============================================================================\n",
    "\n",
    "def display_images_grid(image_list, start_idx=0, grid_size=10):\n",
    "    \"\"\"Ïù¥ÎØ∏ÏßÄÎ•º 5x2 Í≤©ÏûêÎ°ú Ï∂úÎ†•\"\"\"\n",
    "    fig, axes = plt.subplots(2, 5, figsize=(30, 12))\n",
    "    fig.suptitle(f'Images {start_idx+1}-{start_idx+grid_size}', fontsize=16)\n",
    "    \n",
    "    for i in range(grid_size):\n",
    "        row = i // 5\n",
    "        col = i % 5\n",
    "        \n",
    "        if start_idx + i < len(image_list):\n",
    "            img_info = image_list[start_idx + i]\n",
    "            img_path = os.path.join(TEST_PATH, img_info['filename'])\n",
    "            \n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                axes[row, col].imshow(img)\n",
    "                axes[row, col].set_title(f\"{start_idx+i+1}: {img_info['filename']}\\n\"\n",
    "                                       f\"ÏòàÏ∏°: {img_info['predicted_class']} \"\n",
    "                                       f\"(Ïã†Î¢∞ÎèÑ: {img_info['confidence']:.2f})\\n\"\n",
    "                                       f\"ÌíàÏßà: {img_info['quality_score']:.3f}\",\n",
    "                                       fontsize=10)\n",
    "            except Exception as e:\n",
    "                axes[row, col].text(0.5, 0.5, f\"Error\\n{e}\", ha='center', va='center')\n",
    "        else:\n",
    "            axes[row, col].set_visible(False)\n",
    "        \n",
    "        axes[row, col].axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26479a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def manual_labeling_interface(selected_images, start_idx=0):\n",
    "    \"\"\"ÏàòÎèô ÎùºÎ≤®ÎßÅ Ïù∏ÌÑ∞ÌéòÏù¥Ïä§\"\"\"\n",
    "    print(\"\\nüéØ ÏàòÎèô ÎùºÎ≤®ÎßÅ ÏãúÏûë!\")\n",
    "    print(\"=\" * 50)\n",
    "    print(\"üìã ÌÅ¥ÎûòÏä§ Ï†ïÎ≥¥ (Ï∞∏Í≥†Ïö©):\")\n",
    "    for idx, row in meta_df.iterrows():\n",
    "        print(f\"  {row['target']:2d}: {row['class_name']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Í≤∞Í≥º Ï†ÄÏû•Ïö© Î¶¨Ïä§Ìä∏\n",
    "    manual_labels = []\n",
    "    \n",
    "    # Ïù¥Ï†Ñ ÏûëÏóÖ Î°úÎìú (ÏûàÎã§Î©¥)\n",
    "    progress_file = 'labeling_progress.csv'\n",
    "    if os.path.exists(progress_file):\n",
    "        print(f\"üìÇ Ïù¥Ï†Ñ ÏûëÏóÖ ÏßÑÌñâÏÉÅÌô© Î°úÎìú: {progress_file}\")\n",
    "        prev_df = pd.read_csv(progress_file)\n",
    "        manual_labels = prev_df.to_dict('records')\n",
    "        start_idx = len(manual_labels)\n",
    "        print(f\"   Ïù¥Ï†Ñ ÏûëÏóÖ: {len(manual_labels)}Ïû• ÏôÑÎ£å\")\n",
    "    \n",
    "    current_idx = start_idx\n",
    "    \n",
    "    while current_idx < len(selected_images):\n",
    "        # 10Ïû•Ïî© Î≥¥Ïó¨Ï£ºÍ∏∞\n",
    "        display_images_grid(selected_images, current_idx, min(10, len(selected_images) - current_idx))\n",
    "        \n",
    "        # Í∞Å Ïù¥ÎØ∏ÏßÄÏóê ÎåÄÌï¥ ÎùºÎ≤® ÏûÖÎ†•Î∞õÍ∏∞\n",
    "        batch_end = min(current_idx + 10, len(selected_images))\n",
    "        for i in range(current_idx, batch_end):\n",
    "            img_info = selected_images[i]\n",
    "            print(f\"\\n[{i+1}/{len(selected_images)}] {img_info['filename']}\")\n",
    "            print(f\"  ÏòàÏ∏°ÌÅ¥ÎûòÏä§: {img_info['predicted_class']}, ÌíàÏßàÏ†êÏàò: {img_info['quality_score']:.3f}\")\n",
    "            \n",
    "            while True:\n",
    "                try:\n",
    "                    user_input = input(f\"  Ï†ïÎãµ ÌÅ¥ÎûòÏä§ ÏûÖÎ†• (0-16, s=Í±¥ÎÑàÎõ∞Í∏∞, q=Ï¢ÖÎ£å): \").strip()\n",
    "                    \n",
    "                    if user_input.lower() == 'q':\n",
    "                        print(\"üõë ÎùºÎ≤®ÎßÅ Ï§ëÎã®\")\n",
    "                        return manual_labels\n",
    "                    elif user_input.lower() == 's':\n",
    "                        print(\"  ‚è≠Ô∏è  Í±¥ÎÑàÎõ∞Í∏∞\")\n",
    "                        break\n",
    "                    else:\n",
    "                        label = int(user_input)\n",
    "                        if 0 <= label <= 16:\n",
    "                            manual_labels.append({\n",
    "                                'filename': img_info['filename'],\n",
    "                                'true_label': label,\n",
    "                                'predicted_class': img_info['predicted_class'],\n",
    "                                'quality_score': img_info['quality_score']\n",
    "                            })\n",
    "                            print(f\"  ‚úÖ ÎùºÎ≤® {label} Ï†ÄÏû•\")\n",
    "                            break\n",
    "                        else:\n",
    "                            print(\"  ‚ùå 0-16 Î≤îÏúÑÏùò Ïà´ÏûêÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî\")\n",
    "                except ValueError:\n",
    "                    print(\"  ‚ùå Ïò¨Î∞îÎ•∏ Ïà´ÏûêÎ•º ÏûÖÎ†•ÌïòÏÑ∏Ïöî\")\n",
    "        \n",
    "        current_idx = batch_end\n",
    "        \n",
    "        # Ï§ëÍ∞Ñ Ï†ÄÏû• (50Ïû•ÎßàÎã§)\n",
    "        if len(manual_labels) % 50 == 0 and len(manual_labels) > 0:\n",
    "            temp_df = pd.DataFrame(manual_labels)\n",
    "            temp_df.to_csv(progress_file, index=False)\n",
    "            print(f\"üíæ ÏßÑÌñâÏÉÅÌô© Ï†ÄÏû•: {len(manual_labels)}Ïû• ÏôÑÎ£å\")\n",
    "        \n",
    "        # Í≥ÑÏÜçÌï†ÏßÄ ÌôïÏù∏\n",
    "        if current_idx < len(selected_images):\n",
    "            continue_input = input(f\"\\nÍ≥ÑÏÜç ÏßÑÌñâÌïòÏãúÍ≤†ÏäµÎãàÍπå? (y/n, ÌòÑÏû¨ {len(manual_labels)}Ïû• ÏôÑÎ£å): \")\n",
    "            if continue_input.lower() != 'y':\n",
    "                break\n",
    "    \n",
    "    return manual_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da334651",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# STEP 7: ÏµúÏ¢Ö GT ÌååÏùº ÏÉùÏÑ±\n",
    "# =============================================================================\n",
    "\n",
    "def save_final_gt(manual_labels):\n",
    "    \"\"\"ÏµúÏ¢Ö GT ÌååÏùº ÏÉùÏÑ± Î∞è Í≤ÄÏ¶ù\"\"\"\n",
    "    print(f\"\\nüíæ ÏµúÏ¢Ö GT ÌååÏùº ÏÉùÏÑ± Ï§ë... (Ï¥ù {len(manual_labels)}Ïû•)\")\n",
    "    \n",
    "    # Îç∞Ïù¥ÌÑ∞ÌîÑÎ†àÏûÑ ÏÉùÏÑ±\n",
    "    gt_df = pd.DataFrame(manual_labels)\n",
    "    \n",
    "    # ÌååÏùºÎ™ÖÏóêÏÑú ID Ï∂îÏ∂ú (ÌôïÏû•Ïûê Ï†úÍ±∞)\n",
    "    gt_df['ID'] = gt_df['filename'].str.replace('.jpg', '')\n",
    "    \n",
    "    # ÏµúÏ¢Ö GT ÌòïÏãùÏúºÎ°ú Î≥ÄÌôò\n",
    "    final_gt = gt_df[['ID', 'true_label']].copy()\n",
    "    final_gt.columns = ['ID', 'target']\n",
    "    \n",
    "    # GT ÌååÏùº Ï†ÄÏû•\n",
    "    final_gt.to_csv('mini_gt_500.csv', index=False)\n",
    "    \n",
    "    # Î∞±ÏóÖ Ï†ÄÏû•\n",
    "    import datetime\n",
    "    timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    backup_filename = f'mini_gt_500_backup_{timestamp}.csv'\n",
    "    final_gt.to_csv(backup_filename, index=False)\n",
    "    \n",
    "    # ÌÜµÍ≥Ñ Ï∂úÎ†•\n",
    "    print(\"üìä ÏµúÏ¢Ö GT ÌÜµÍ≥Ñ:\")\n",
    "    print(f\"   Ï¥ù ÎùºÎ≤®ÎßÅÎêú Ïù¥ÎØ∏ÏßÄ: {len(final_gt)}Ïû•\")\n",
    "    print(\"\\n   ÌÅ¥ÎûòÏä§Î≥Ñ Î∂ÑÌè¨:\")\n",
    "    class_counts = final_gt['target'].value_counts().sort_index()\n",
    "    for class_id, count in class_counts.items():\n",
    "        class_name = meta_df[meta_df['target'] == class_id]['class_name'].iloc[0]\n",
    "        print(f\"     {class_id:2d} ({class_name}): {count:3d}Ïû•\")\n",
    "    \n",
    "    # ÌíàÏßàÏ†êÏàò Î∂ÑÌè¨\n",
    "    if 'quality_score' in gt_df.columns:\n",
    "        print(f\"\\n   ÌíàÏßàÏ†êÏàò Î∂ÑÌè¨:\")\n",
    "        print(f\"     ÌèâÍ∑†: {gt_df['quality_score'].mean():.3f}\")\n",
    "        print(f\"     Î≤îÏúÑ: {gt_df['quality_score'].min():.3f} ~ {gt_df['quality_score'].max():.3f}\")\n",
    "    \n",
    "    print(f\"\\n‚úÖ GT ÌååÏùº Ï†ÄÏû• ÏôÑÎ£å:\")\n",
    "    print(f\"   üìÅ main: mini_gt_500.csv\")\n",
    "    print(f\"   üìÅ backup: {backup_filename}\")\n",
    "    \n",
    "    return final_gt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "334b765e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Ïã§Ìñâ Í∞ÄÏù¥Îìú\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üéØ ÏàòÎèô ÎùºÎ≤®ÎßÅ Ïã§Ìñâ Í∞ÄÏù¥Îìú\")\n",
    "print(\"=\"*60)\n",
    "print(\"1Ô∏è‚É£  manual_labels = manual_labeling_interface(selected_images)\")\n",
    "print(\"2Ô∏è‚É£  final_gt = save_final_gt(manual_labels)\")\n",
    "print(\"3Ô∏è‚É£  ÌåÄÏõêÍ≥º ÏûëÏóÖ Î∂ÑÎã¥ Ïãú: Î¶¨Ïä§Ìä∏Î•º Ï†àÎ∞òÏúºÎ°ú ÎÇòÎàÑÏñ¥ ÏßÑÌñâ\")\n",
    "print(\"4Ô∏è‚É£  ÏôÑÎ£å ÌõÑ: mini_gt_500.csv ÌååÏùºÏùÑ Îã§Ïùå Îã®Í≥ÑÏóêÏÑú ÏÇ¨Ïö©\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60323f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT ÏûëÏóÖ ÏãúÏûë\n",
    "manual_labels = manual_labeling_interface(selected_images, start_idx=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7745d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT ÏûëÏóÖ ÏÑ∏Ïù¥Î∏å\n",
    "final_gt = save_final_gt(manual_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843c77b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dd8a634",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
