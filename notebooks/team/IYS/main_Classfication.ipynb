{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zkH9T_86lDSS"
   },
   "source": [
    "## 1. Prepare Environments\n",
    "\n",
    "* 데이터 로드를 위한 구글 드라이브를 마운트합니다.\n",
    "* 필요한 라이브러리를 설치합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 8489,
     "status": "ok",
     "timestamp": 1700314558888,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "NC8V-D393wY4",
    "outputId": "e9927325-26c4-4b89-9c51-c1d6541388d6"
   },
   "outputs": [],
   "source": [
    "# # 필요한 라이브러리를 설치합니다.\n",
    "# !pip install timm\n",
    "# !pip install matplotlib\n",
    "# !pip install seaborn\n",
    "# !pip install optuna\n",
    "# !apt install -y libgl1-mesa-glx\n",
    "# !pip install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 작업 디렉토리: /home/ieyeppo/AI_Lab/computer-vision-competition-1SEN\n",
      "✅ GPU 사용 가능: NVIDIA GeForce RTX 4090\n",
      "✅ 나눔고딕 폰트 로드 성공\n",
      "📝 노트북 작업 시작: main_Classfication\n",
      "📝 로그 디렉토리: notebooks/team/IYS/main_Classfication/20250912_035158\n",
      "✅ 환경 설정 및 로거 초기화 완료\n"
     ]
    }
   ],
   "source": [
    "# [1] 프로젝트 루트 디렉토리 이동 및 환경 설정\n",
    "import os\n",
    "os.chdir(\"../../../\")  # 프로젝트 루트로 이동\n",
    "print(\"현재 작업 디렉토리:\", os.getcwd())\n",
    "\n",
    "# GPU 체크\n",
    "import torch\n",
    "if torch.cuda.is_available():\n",
    "    print(f'✅ GPU 사용 가능: {torch.cuda.get_device_name(0)}')\n",
    "else:\n",
    "    print('⚠️ GPU 사용 불가, CPU로 실행됩니다')\n",
    "\n",
    "# 경고 억제 설정\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 한글 폰트 적용 및 시각화 환경 설정\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "# 나눔고딕 폰트 경로 및 설정\n",
    "font_path = './font/NanumGothic.ttf'\n",
    "fontprop = fm.FontProperties(fname=font_path)\n",
    "\n",
    "# 폰트 등록 및 설정 (한글 텍스트 표시를 위함)\n",
    "fe = fm.FontEntry(fname=font_path, name='NanumGothic')\n",
    "fm.fontManager.ttflist.insert(0, fe)\n",
    "plt.rcParams['font.family'] = 'NanumGothic'      # 기본 폰트를 나눔고딕으로 설정\n",
    "plt.rcParams['font.size'] = 10                   # 기본 글자 크기 설정\n",
    "plt.rcParams['axes.unicode_minus'] = False       # 마이너스 기호 깨짐 방지\n",
    "\n",
    "# 글자 겹침 방지를 위한 레이아웃 설정\n",
    "plt.rcParams['figure.autolayout'] = True         # 자동 레이아웃 조정\n",
    "plt.rcParams['axes.titlepad'] = 20               # 제목과 축 사이 여백\n",
    "\n",
    "# 폰트 로드 확인\n",
    "try:\n",
    "    test_font = fm.FontProperties(fname=font_path)\n",
    "    print(\"✅ 나눔고딕 폰트 로드 성공\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ 폰트 로드 실패: {e}\")\n",
    "\n",
    "# 노트북 로거 생성\n",
    "from src.logging.notebook_logger import create_notebook_logger\n",
    "\n",
    "logger = create_notebook_logger(\n",
    "    base_log_dir=\"team\",\n",
    "    folder_name=\"IYS\",\n",
    "    file_name=\"main_Classfication\"\n",
    ")\n",
    "\n",
    "print(\"✅ 환경 설정 및 로거 초기화 완료\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "PXa_FPM73R9f"
   },
   "source": [
    "## 2. Import Library & Define Functions\n",
    "* 학습 및 추론에 필요한 라이브러리를 로드합니다.\n",
    "* 학습 및 추론에 필요한 함수와 클래스를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 9396,
     "status": "ok",
     "timestamp": 1700314592802,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "3BaoIkv5Xwa0"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import copy\n",
    "\n",
    "import optuna, math\n",
    "import timm\n",
    "import torch\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "from torch.optim import Adam, AdamW\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.cuda.amp import autocast, GradScaler  # Mixed Precision용\n",
    "\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# import wandb\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 시드를 고정합니다.\n",
    "SEED = 42\n",
    "os.environ['PYTHONHASHSEED'] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.benchmark = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 241,
     "status": "ok",
     "timestamp": 1700314772722,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "Hyl8oAy6TZAu"
   },
   "outputs": [],
   "source": [
    "def mixup_data(x, y, alpha=1.0):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).cuda()\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "# 계층적 모델 훈련 함수들 추가\n",
    "def train_weak_detector_fold(train_df, val_df, fold, device, data_path):\n",
    "    \"\"\"취약 클래스 탐지기 훈련 (1개 fold용)\"\"\"\n",
    "    print(f\"🔍 Training Weak Detector - Fold {fold+1}\")\n",
    "    \n",
    "    # 간단한 변환 (빠른 훈련을 위해)\n",
    "    train_transform = A.Compose([\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224, border_mode=0, value=0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.2, contrast_limit=0.2, p=0.7),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = WeakClassDataset(train_df, data_path, train_transform)\n",
    "    val_dataset = WeakClassDataset(val_df, data_path, val_transform)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 클래스 가중치 (불균형 처리)\n",
    "    class_counts = train_dataset.df['binary_target'].value_counts()\n",
    "    weight_ratio = float(class_counts[0] / class_counts[1]) if 1 in class_counts else 1.0  # float() 추가\n",
    "    class_weights = torch.tensor([1.0, weight_ratio], dtype=torch.float32).to(device)      # dtype 명시\n",
    "    \n",
    "    model = WeakClassDetector().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.01)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # 간단한 5 에포크 훈련\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        for images, targets in train_loader:\n",
    "            images, targets = images.to(device), targets.long().to(device)  # .long() 추가\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.long()  # .long() 추가\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_targets.extend(targets.numpy())\n",
    "        \n",
    "        val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    return best_model, best_f1\n",
    "\n",
    "def train_weak_specializer_fold(train_df, val_df, fold, device, data_path):\n",
    "    \"\"\"취약 클래스 전용 분류기 훈련 (1개 fold용)\"\"\"\n",
    "    print(f\"🎯 Training Weak Specializer - Fold {fold+1}\")\n",
    "    \n",
    "    train_transform = A.Compose([\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224, border_mode=0, value=0),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=10, p=0.7),\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    val_transform = A.Compose([\n",
    "        A.LongestMaxSize(max_size=224),\n",
    "        A.PadIfNeeded(min_height=224, min_width=224, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "    \n",
    "    train_dataset = WeakClassSpecializerDataset(train_df, data_path, train_transform)\n",
    "    val_dataset = WeakClassSpecializerDataset(val_df, data_path, val_transform)\n",
    "    \n",
    "    if len(train_dataset) == 0 or len(val_dataset) == 0:\n",
    "        print(f\"⚠️ No weak class samples in fold {fold+1}, skipping specializer training\")\n",
    "        return None, 0.0\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True, num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=16, shuffle=False, num_workers=4)\n",
    "    \n",
    "    # 클래스 가중치\n",
    "    class_dist = train_dataset.df['mapped_target'].value_counts().sort_index()\n",
    "    total_samples = len(train_dataset)\n",
    "    class_weights = []\n",
    "    for i in range(4):\n",
    "        if i in class_dist.index:\n",
    "            weight = float(total_samples / (4 * class_dist[i]))  # float() 추가\n",
    "            class_weights.append(weight)\n",
    "        else:\n",
    "            class_weights.append(1.0)\n",
    "    class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)  # dtype 명시\n",
    "    \n",
    "    model = WeakClassSpecializer().to(device)\n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights, label_smoothing=0.1)\n",
    "    optimizer = AdamW(model.parameters(), lr=2e-4, weight_decay=0.01)\n",
    "    \n",
    "    best_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # 8 에포크 훈련\n",
    "    for epoch in range(8):\n",
    "        model.train()\n",
    "        for images, targets in train_loader:\n",
    "            images, targets = images.to(device), targets.long().to(device)  # .long() 추가\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # 검증\n",
    "        model.eval()\n",
    "        val_preds, val_targets = [], []\n",
    "        with torch.no_grad():\n",
    "            for images, targets in val_loader:\n",
    "                images = images.to(device)\n",
    "                targets = targets.long()  # .long() 추가\n",
    "                outputs = model(images)\n",
    "                preds = torch.argmax(outputs, dim=1)\n",
    "                val_preds.extend(preds.cpu().numpy())\n",
    "                val_targets.extend(targets.numpy())\n",
    "        \n",
    "        if len(val_targets) > 0:\n",
    "            val_f1 = f1_score(val_targets, val_preds, average='macro')\n",
    "            if val_f1 > best_f1:\n",
    "                best_f1 = val_f1\n",
    "                best_model = copy.deepcopy(model.state_dict())\n",
    "    \n",
    "    return best_model, best_f1\n",
    "\n",
    "# 계층적 분류를 위한 모델 클래스들\n",
    "class WeakClassDetector(nn.Module):\n",
    "    \"\"\"취약 클래스(3,4,7,14) 탐지를 위한 이진 분류기\"\"\"\n",
    "    def __init__(self, model_name='convnext_tiny', img_size=224):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name, \n",
    "            pretrained=True, \n",
    "            num_classes=2,  # 취약클래스 vs 일반클래스\n",
    "            drop_rate=0.1\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class WeakClassSpecializer(nn.Module):\n",
    "    \"\"\"취약 클래스(3,4,7,14) 전용 4-class 분류기\"\"\"\n",
    "    def __init__(self, model_name='convnext_small', img_size=224):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            model_name,\n",
    "            pretrained=True,\n",
    "            num_classes=4,  # 클래스 3,4,7,14만 구분\n",
    "            drop_rate=0.15\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class HierarchicalDocumentClassifier(nn.Module):\n",
    "    \"\"\"조건부 계층적 문서 분류 시스템\"\"\"\n",
    "    def __init__(self, main_model_name='convnext_base_384_in22ft1k'):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weak_classes = [3, 4, 7, 14]\n",
    "        self.class_mapping = {3: 0, 4: 1, 7: 2, 14: 3}\n",
    "        self.reverse_mapping = {0: 3, 1: 4, 2: 7, 3: 14}\n",
    "        \n",
    "        # 메인 분류기는 기존 모델과 동일\n",
    "        self.main_classifier = timm.create_model(\n",
    "            main_model_name, pretrained=True, num_classes=17\n",
    "        )\n",
    "        self.weak_detector = WeakClassDetector()  \n",
    "        self.weak_specializer = WeakClassSpecializer()\n",
    "        \n",
    "        # 조건부 실행을 위한 임계값들\n",
    "        self.confidence_threshold = 0.8  # 메인 분류기 신뢰도 임계값\n",
    "        self.weak_class_threshold = 0.6  # 탐지기 임계값\n",
    "        self.main_weak_confidence_threshold = 0.7  # 메인이 취약클래스 예측시 임계값\n",
    "        \n",
    "    def forward(self, x, mode='inference'):\n",
    "        if mode == 'main':\n",
    "            return self.main_classifier(x)\n",
    "        elif mode == 'detector':\n",
    "            return self.weak_detector(x)\n",
    "        elif mode == 'specializer':\n",
    "            return self.weak_specializer(x)\n",
    "        elif mode == 'inference':\n",
    "            return self.conditional_hierarchical_inference(x)\n",
    "            \n",
    "    def conditional_hierarchical_inference(self, x):\n",
    "        \"\"\"조건부 계층적 추론 - 필요할 때만 탐지기 실행\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        final_predictions = torch.zeros(batch_size, 17).to(x.device)\n",
    "        \n",
    "        # 1단계: 메인 분류기만 먼저 실행\n",
    "        main_logits = self.main_classifier(x)\n",
    "        main_probs = torch.softmax(main_logits, dim=1)\n",
    "        main_confidence, main_preds = torch.max(main_probs, dim=1)\n",
    "        \n",
    "        # 2단계: 조건 확인 - 탐지기 실행이 필요한 샘플들 찾기\n",
    "        needs_detection = torch.zeros(batch_size, dtype=torch.bool).to(x.device)\n",
    "        \n",
    "        for i in range(batch_size):\n",
    "            main_pred = main_preds[i].item()\n",
    "            main_conf = main_confidence[i].item()\n",
    "            \n",
    "            # 탐지기 실행 조건들\n",
    "            condition1 = main_pred in self.weak_classes and main_conf < self.main_weak_confidence_threshold\n",
    "            condition2 = main_conf < self.confidence_threshold\n",
    "            \n",
    "            needs_detection[i] = condition1 or condition2\n",
    "        \n",
    "        # 3단계: 조건부 탐지기 실행\n",
    "        detection_indices = torch.where(needs_detection)[0]\n",
    "        \n",
    "        if len(detection_indices) > 0:\n",
    "            # 필요한 샘플들만 탐지기에 입력\n",
    "            suspicious_samples = x[detection_indices]\n",
    "            detector_logits = self.weak_detector(suspicious_samples)\n",
    "            detector_probs = torch.softmax(detector_logits, dim=1)\n",
    "            weak_class_probs = detector_probs[:, 1]  # 취약클래스 확률\n",
    "            \n",
    "            # 4단계: 전용 분류기 사용 여부 결정\n",
    "            for idx_in_batch, sample_idx in enumerate(detection_indices):\n",
    "                main_pred = main_preds[sample_idx].item()\n",
    "                main_conf = main_confidence[sample_idx].item()\n",
    "                weak_prob = weak_class_probs[idx_in_batch].item()\n",
    "                \n",
    "                # 전용 분류기 사용 조건\n",
    "                use_specializer = (\n",
    "                    (main_pred in self.weak_classes and main_conf < self.main_weak_confidence_threshold) or\n",
    "                    (weak_prob > self.weak_class_threshold)\n",
    "                )\n",
    "                \n",
    "                if use_specializer:\n",
    "                    # 전용 분류기 실행 (단일 샘플)\n",
    "                    single_sample = x[sample_idx:sample_idx+1]\n",
    "                    specializer_logits = self.weak_specializer(single_sample)\n",
    "                    specializer_probs = torch.softmax(specializer_logits, dim=1)\n",
    "                    spec_pred = torch.argmax(specializer_probs, dim=1).item()\n",
    "                    final_class = self.reverse_mapping[spec_pred]\n",
    "                    final_predictions[sample_idx, final_class] = 1.0\n",
    "                else:\n",
    "                    # 메인 분류기 결과 사용\n",
    "                    final_predictions[sample_idx] = main_probs[sample_idx]\n",
    "        \n",
    "        # 5단계: 탐지가 필요없던 샘플들은 메인 결과 사용\n",
    "        no_detection_indices = torch.where(~needs_detection)[0]\n",
    "        for sample_idx in no_detection_indices:\n",
    "            final_predictions[sample_idx] = main_probs[sample_idx]\n",
    "                \n",
    "        return final_predictions\n",
    "    \n",
    "    def get_inference_stats(self, x):\n",
    "        \"\"\"추론 과정 통계 반환 (디버깅용)\"\"\"\n",
    "        batch_size = x.size(0)\n",
    "        \n",
    "        # 메인 분류기 실행\n",
    "        main_logits = self.main_classifier(x)\n",
    "        main_probs = torch.softmax(main_logits, dim=1)\n",
    "        main_confidence, main_preds = torch.max(main_probs, dim=1)\n",
    "        \n",
    "        # 조건 확인\n",
    "        needs_detection = torch.zeros(batch_size, dtype=torch.bool).to(x.device)\n",
    "        for i in range(batch_size):\n",
    "            main_pred = main_preds[i].item()\n",
    "            main_conf = main_confidence[i].item()\n",
    "            \n",
    "            condition1 = main_pred in self.weak_classes and main_conf < self.main_weak_confidence_threshold\n",
    "            condition2 = main_conf < self.confidence_threshold\n",
    "            needs_detection[i] = condition1 or condition2\n",
    "        \n",
    "        detection_count = needs_detection.sum().item()\n",
    "        \n",
    "        stats = {\n",
    "            'total_samples': batch_size,\n",
    "            'detection_needed': detection_count,\n",
    "            'detection_ratio': detection_count / batch_size if batch_size > 0 else 0,\n",
    "            'main_only_samples': batch_size - detection_count,\n",
    "            'avg_main_confidence': main_confidence.mean().item(),\n",
    "            'weak_class_predictions': sum(1 for pred in main_preds if pred.item() in self.weak_classes)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "\n",
    "# 취약 클래스 탐지용 이진 분류 데이터셋\n",
    "class WeakClassDataset(Dataset):\n",
    "    \"\"\"취약 클래스 탐지용 이진 분류 데이터셋\"\"\"\n",
    "    def __init__(self, data, path, transform=None, weak_classes=[3, 4, 7, 14]):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data)\n",
    "        else:\n",
    "            self.df = data.copy()\n",
    "        \n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.weak_classes = weak_classes\n",
    "        \n",
    "        # 이진 레이블 생성 (취약클래스: 1, 나머지: 0)\n",
    "        self.df['binary_target'] = self.df['target'].apply(\n",
    "            lambda x: 1 if x in weak_classes else 0\n",
    "        ).astype(int)  # int 타입으로 명시적 변환\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.path, row['ID'])\n",
    "        img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "            \n",
    "        return img, int(row['binary_target'])  # int()로 확실히 정수 타입으로 변환\n",
    "\n",
    "class WeakClassSpecializerDataset(Dataset):\n",
    "    \"\"\"취약 클래스 전용 4-class 데이터셋\"\"\"\n",
    "    def __init__(self, data, path, transform=None, weak_classes=[3, 4, 7, 14]):\n",
    "        if isinstance(data, str):\n",
    "            df = pd.read_csv(data)\n",
    "        else:\n",
    "            df = data.copy()\n",
    "        \n",
    "        # 취약 클래스만 필터링\n",
    "        self.df = df[df['target'].isin(weak_classes)].reset_index(drop=True)\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        \n",
    "        # 클래스 매핑 (3->0, 4->1, 7->2, 14->3)\n",
    "        self.class_mapping = {3: 0, 4: 1, 7: 2, 14: 3}\n",
    "        self.df['mapped_target'] = self.df['target'].map(self.class_mapping).astype(int)  # int 타입으로 명시\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        img_path = os.path.join(self.path, row['ID'])\n",
    "        img = np.array(Image.open(img_path).convert('RGB'))\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(image=img)['image']\n",
    "            \n",
    "        return img, int(row['mapped_target'])\n",
    "\n",
    "# 데이터셋 클래스를 정의합니다. (Hard Augmentation 포함)\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, data, path, epoch=0, total_epochs=10, is_train=True):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.epoch = epoch\n",
    "        self.total_epochs = total_epochs\n",
    "        self.is_train = is_train\n",
    "        \n",
    "        # Hard augmentation 확률 계산\n",
    "        self.p_hard = 0.2 + 0.3 * (epoch / total_epochs) if is_train else 0\n",
    "        \n",
    "        # Normal augmentation\n",
    "        self.normal_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "            ], p=0.6),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.3, contrast_limit=0.3, p=0.8),\n",
    "            A.GaussNoise(var_limit=(30.0, 100.0), p=0.7),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "        \n",
    "        # Hard augmentation\n",
    "        self.hard_aug = A.Compose([\n",
    "            A.LongestMaxSize(max_size=img_size),\n",
    "            A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=[90,90], p=1.0),\n",
    "                A.Rotate(limit=[180,180], p=1.0),\n",
    "                A.Rotate(limit=[270,270], p=1.0),\n",
    "                A.Rotate(limit=[-15,15], p=1.0),\n",
    "            ], p=0.8),\n",
    "            A.OneOf([\n",
    "                A.MotionBlur(blur_limit=15, p=1.0),\n",
    "                A.GaussianBlur(blur_limit=15, p=1.0),\n",
    "            ], p=0.95),\n",
    "            A.RandomBrightnessContrast(brightness_limit=0.5, contrast_limit=0.5, p=0.9),\n",
    "            A.GaussNoise(var_limit=(50.0, 150.0), p=0.8),\n",
    "            A.JpegCompression(quality_lower=70, quality_upper=100, p=0.5),\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 배치별 증강 선택\n",
    "        if self.is_train and random.random() < self.p_hard:\n",
    "            img = self.hard_aug(image=img)['image']\n",
    "        else:\n",
    "            img = self.normal_aug(image=img)['image']\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 255,
     "status": "ok",
     "timestamp": 1700315066028,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "kTECBJfVTbdl"
   },
   "outputs": [],
   "source": [
    "# one epoch 학습을 위한 함수입니다.\n",
    "def train_one_epoch(loader, model, optimizer, loss_fn, device):\n",
    "    scaler = GradScaler()  # Mixed Precision용\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "\n",
    "    pbar = tqdm(loader)\n",
    "    for image, targets in pbar:\n",
    "        image = image.to(device)\n",
    "        targets = targets.to(device)\n",
    "        \n",
    "        # Cutmix/Mixup 적용 (30% 확률)\n",
    "        if random.random() < 0.3:\n",
    "            mixed_x, y_a, y_b, lam = mixup_data(image, targets, alpha=1.0)\n",
    "            with autocast(): preds = model(mixed_x)\n",
    "            loss = lam * loss_fn(preds, y_a) + (1 - lam) * loss_fn(preds, y_b)\n",
    "        else:\n",
    "            with autocast(): preds = model(image)\n",
    "            loss = loss_fn(preds, targets)\n",
    "\n",
    "        model.zero_grad(set_to_none=True)\n",
    "\n",
    "        scaler.scale(loss).backward()  # Mixed Precision용\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        scaler.step(optimizer); scaler.update()  # Mixed Precision용\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())\n",
    "        targets_list.extend(targets.detach().cpu().numpy())\n",
    "\n",
    "        pbar.set_description(f\"Loss: {loss.item():.4f}\")\n",
    "\n",
    "    train_loss /= len(loader)\n",
    "    train_acc = accuracy_score(targets_list, preds_list)\n",
    "    train_f1 = f1_score(targets_list, preds_list, average='macro')\n",
    "\n",
    "    ret = {\n",
    "        \"train_loss\": train_loss,\n",
    "        \"train_acc\": train_acc,\n",
    "        \"train_f1\": train_f1,\n",
    "    }\n",
    "\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation을 위한 함수 추가\n",
    "def validate_one_epoch(loader, model, loss_fn, device):\n",
    "    \"\"\"\n",
    "    한 에폭 검증을 수행하는 함수\n",
    "    - model.eval()로 모델을 평가 모드로 전환\n",
    "    - torch.no_grad()로 gradient 계산 비활성화하여 메모리 절약\n",
    "    - 검증 데이터에 대한 loss, accuracy, f1 score 계산\n",
    "    \"\"\"\n",
    "    model.eval()  # 모델을 평가 모드로 전환 (dropout, batchnorm 비활성화)\n",
    "    val_loss = 0\n",
    "    preds_list = []\n",
    "    targets_list = []\n",
    "    \n",
    "    with torch.no_grad():  # gradient 계산 비활성화로 메모리 절약\n",
    "        pbar = tqdm(loader, desc=\"Validating\")\n",
    "        for image, targets in pbar:\n",
    "            image = image.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            preds = model(image)  # 모델 예측\n",
    "            loss = loss_fn(preds, targets)  # 손실 계산\n",
    "            \n",
    "            val_loss += loss.item()\n",
    "            preds_list.extend(preds.argmax(dim=1).detach().cpu().numpy())  # 예측 클래스 저장\n",
    "            targets_list.extend(targets.detach().cpu().numpy())  # 실제 클래스 저장\n",
    "            \n",
    "            pbar.set_description(f\"Val Loss: {loss.item():.4f}\")\n",
    "    \n",
    "    val_loss /= len(loader)  # 평균 손실 계산\n",
    "    val_acc = accuracy_score(targets_list, preds_list)  # 정확도 계산\n",
    "    val_f1 = f1_score(targets_list, preds_list, average='macro')  # Macro F1 계산 (대회 평가지표)\n",
    "    \n",
    "    ret = {\n",
    "        \"val_loss\": val_loss,\n",
    "        \"val_acc\": val_acc,\n",
    "        \"val_f1\": val_f1,\n",
    "    }\n",
    "    \n",
    "    return ret"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Wjom43UvoXcx"
   },
   "source": [
    "## 3. Hyper-parameters\n",
    "* 학습 및 추론에 필요한 하이퍼파라미터들을 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 436,
     "status": "ok",
     "timestamp": 1700315112439,
     "user": {
      "displayName": "Ynot(송원호)",
      "userId": "16271863862696372773"
     },
     "user_tz": -540
    },
    "id": "KByfAeRmXwYk"
   },
   "outputs": [],
   "source": [
    "# device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# data config\n",
    "data_path = './data/raw/'\n",
    "\n",
    "# model config\n",
    "# model_name = 'tf_efficientnetv2_b3' # 'resnet50' 'efficientnet-b0', ...\n",
    "# model_name = 'swin_base_patch4_window12_384_in22k'\n",
    "model_name = 'convnext_base_384_in22ft1k'\n",
    "# model_name = 'convnextv2_base.fcmae_ft_in22k_in1k_384'\n",
    "# model_name = 'vit_base_patch16_clip_384.laion2b_ft_in12k_in1k' # openclip\n",
    "# model_name = 'vit_base_patch16_384.augreg_in1k' # augreg\n",
    "# model_name = 'eva02_enormous_patch14_plus_clip_224.laion2b_s9b_b144k' # eva-02 멀티모달\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "# model_name = 'vit_base_patch14_reg4_dinov2.lvd142m' # dinov2 reg4\n",
    "\n",
    "# model_name = 'eva02_large_patch14_448.mim_in22k_ft_in1k' #448 테스트용\n",
    "\n",
    "# training config\n",
    "img_size = 512\n",
    "LR = 2e-4\n",
    "# EPOCHS = 64\n",
    "EPOCHS = 1\n",
    "BATCH_SIZE = 10\n",
    "num_workers = 8\n",
    "EMA = True  # Exponential Moving Average 사용 여부"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna를 사용한 하이퍼파라미터 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    def objective(trial):\n",
    "        lr = trial.suggest_loguniform('lr', 1e-5, 1e-2)\n",
    "        batch_size = trial.suggest_categorical('batch_size', [32, 64, 128])\n",
    "        \n",
    "        # 간단한 3-fold CV로 빠른 평가\n",
    "        skf_simple = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "        fold_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(skf_simple.split(train_df, train_df['target'])):\n",
    "            # 모델 생성\n",
    "            model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "            optimizer = Adam(model.parameters(), lr=lr)\n",
    "            loss_fn = nn.CrossEntropyLoss()\n",
    "            \n",
    "            # 간단한 2 epoch 학습\n",
    "            for epoch in range(2):\n",
    "                train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "            \n",
    "            val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "            fold_scores.append(val_ret['val_f1'])\n",
    "        \n",
    "        return np.mean(fold_scores)\n",
    "    \n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(objective, n_trials=10)\n",
    "    \n",
    "    # 최적 파라미터 적용\n",
    "    LR = study.best_params['lr']\n",
    "    BATCH_SIZE = study.best_params['batch_size']\n",
    "    print(f\"Best params: {study.best_params}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "amum-FlIojc6"
   },
   "source": [
    "## 4. Load Data\n",
    "* 학습, 테스트 데이터셋과 로더를 정의합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna 튜닝 (선택적 실행)\n",
    "USE_OPTUNA = False  # True로 바꾸면 튜닝 실행\n",
    "\n",
    "if USE_OPTUNA:\n",
    "    # 위의 objective 함수와 study 코드\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold 적용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 5-Fold Cross Validation...\n",
      "\n",
      "==================================================\n",
      "FOLD 1/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.2764: 100%|██████████| 126/126 [00:21<00:00,  5.94it/s]\n",
      "Val Loss: 1.4231: 100%|██████████| 32/32 [00:03<00:00,  8.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.7735 | Train F1: 0.4406 | Val Loss: 0.8967 | Val F1: 0.7872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🎯 Training hierarchical models for Fold 1\n",
      "🔍 Training Weak Detector - Fold 1\n",
      "🎯 Training Weak Specializer - Fold 1\n",
      "Fold 1 - Main F1: 0.7872, Detector F1: 1.0000, Specializer F1: 0.1111\n",
      "Fold 1 Best Validation F1: 0.7872\n",
      "\n",
      "==================================================\n",
      "FOLD 2/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.5889: 100%|██████████| 126/126 [00:13<00:00,  9.56it/s]\n",
      "Val Loss: 1.4689: 100%|██████████| 32/32 [00:02<00:00, 13.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.6288 | Train F1: 0.4650 | Val Loss: 0.8794 | Val F1: 0.7179\n",
      "\n",
      "🎯 Training hierarchical models for Fold 2\n",
      "🔍 Training Weak Detector - Fold 2\n",
      "🎯 Training Weak Specializer - Fold 2\n",
      "Fold 2 - Main F1: 0.7179, Detector F1: 0.9909, Specializer F1: 0.1111\n",
      "Fold 2 Best Validation F1: 0.7179\n",
      "\n",
      "==================================================\n",
      "FOLD 3/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.4297: 100%|██████████| 126/126 [00:12<00:00,  9.90it/s]\n",
      "Val Loss: 0.3590: 100%|██████████| 32/32 [00:02<00:00, 12.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.5241 | Train F1: 0.5050 | Val Loss: 0.7463 | Val F1: 0.8114\n",
      "\n",
      "🎯 Training hierarchical models for Fold 3\n",
      "🔍 Training Weak Detector - Fold 3\n",
      "🎯 Training Weak Specializer - Fold 3\n",
      "Fold 3 - Main F1: 0.8114, Detector F1: 1.0000, Specializer F1: 0.1111\n",
      "Fold 3 Best Validation F1: 0.8114\n",
      "\n",
      "==================================================\n",
      "FOLD 4/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 1.1230: 100%|██████████| 126/126 [00:12<00:00,  9.90it/s]\n",
      "Val Loss: 0.7444: 100%|██████████| 32/32 [00:02<00:00, 13.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.5712 | Train F1: 0.5256 | Val Loss: 0.8776 | Val F1: 0.7344\n",
      "\n",
      "🎯 Training hierarchical models for Fold 4\n",
      "🔍 Training Weak Detector - Fold 4\n",
      "🎯 Training Weak Specializer - Fold 4\n",
      "Fold 4 - Main F1: 0.7344, Detector F1: 0.9909, Specializer F1: 0.1111\n",
      "Fold 4 Best Validation F1: 0.7344\n",
      "\n",
      "==================================================\n",
      "FOLD 5/5\n",
      "==================================================\n",
      "Train samples: 1256, Validation samples: 314\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loss: 0.7485: 100%|██████████| 126/126 [00:13<00:00,  9.25it/s]\n",
      "Val Loss: 1.1925: 100%|██████████| 32/32 [00:02<00:00, 13.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1 | Train Loss: 1.7117 | Train F1: 0.4544 | Val Loss: 0.8344 | Val F1: 0.7791\n",
      "\n",
      "🎯 Training hierarchical models for Fold 5\n",
      "🔍 Training Weak Detector - Fold 5\n",
      "🎯 Training Weak Specializer - Fold 5\n",
      "Fold 5 - Main F1: 0.7791, Detector F1: 1.0000, Specializer F1: 0.1111\n",
      "Fold 5 Best Validation F1: 0.7791\n"
     ]
    }
   ],
   "source": [
    "# K-Fold 설정\n",
    "N_FOLDS = 5  # 5-fold로 설정 (데이터가 적으므로)\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "# 클래스별 최소 샘플 보장 확인\n",
    "# for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "#     assert len(np.unique(train_df.iloc[val_idx]['target'])) == 17\n",
    "\n",
    "# 전체 학습 데이터 로드\n",
    "train_df = pd.read_csv(\"./data/raw/train.csv\")\n",
    "\n",
    "# K-Fold 결과를 저장할 리스트\n",
    "fold_results = []\n",
    "fold_models = []  # 각 fold의 최고 성능 모델을 저장\n",
    "fold_class_accuracies = [] # 각 fold의 클래스별 정확도 저장\n",
    "detector_models = []      #  각 fold의 탐지기 모델 저장\n",
    "specializer_models = []   #  각 fold의 전용 분류기 모델 저장\n",
    "\n",
    "print(f\"Starting {N_FOLDS}-Fold Cross Validation...\")\n",
    "\n",
    "# LR = best_params['lr']\n",
    "# BATCH_SIZE = best_params['batch_size']\n",
    "\n",
    "# K-Fold Cross Validation 시작\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(train_df, train_df['target'])):\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"FOLD {fold + 1}/{N_FOLDS}\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    current_model = model_name\n",
    "    \n",
    "    # 현재 fold의 train/validation 데이터 분할\n",
    "    train_fold_df = train_df.iloc[train_idx].reset_index(drop=True)\n",
    "    val_fold_df = train_df.iloc[val_idx].reset_index(drop=True)\n",
    "    \n",
    "    # 현재 fold의 Dataset 생성\n",
    "    trn_dataset = ImageDataset(\n",
    "        train_fold_df,\n",
    "        \"./data/raw/train/\",\n",
    "        # transform=trn_transform\n",
    "        epoch=0,  # 현재 epoch 전달\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=True\n",
    "    )\n",
    "    \n",
    "    val_dataset = ImageDataset(\n",
    "        val_fold_df,\n",
    "        \"./data/raw/train/\",\n",
    "        # transform=tst_transform  # 검증에는 증강 적용 안함\n",
    "        epoch=0,  # validation은 epoch 관계없음\n",
    "        total_epochs=EPOCHS,\n",
    "        is_train=False  # validation이므로 hard augmentation 비활성화\n",
    "    )\n",
    "    \n",
    "    # 현재 fold의 DataLoader 생성\n",
    "    trn_loader = DataLoader(\n",
    "        trn_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True,\n",
    "        drop_last=False\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(\n",
    "        val_dataset,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    print(f\"Train samples: {len(trn_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "    \n",
    "    # 모델 초기화 (각 fold마다 새로운 모델)\n",
    "    model = timm.create_model(\n",
    "        current_model,\n",
    "        pretrained=True,\n",
    "        num_classes=17\n",
    "    ).to(device)\n",
    "    \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.05)  # Label Smoothing 적용\n",
    "    optimizer = Adam(model.parameters(), lr=LR)\n",
    "    \n",
    "    # Learning Rate Scheduler 추가\n",
    "    scheduler = CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "    \n",
    "    # 현재 fold의 최고 성능 추적\n",
    "    best_val_f1 = 0.0\n",
    "    best_model = None\n",
    "    \n",
    "    # 현재 fold 학습\n",
    "    for epoch in range(EPOCHS):\n",
    "        # Training\n",
    "        train_ret = train_one_epoch(trn_loader, model, optimizer, loss_fn, device)\n",
    "        \n",
    "        # Validation\n",
    "        val_ret = validate_one_epoch(val_loader, model, loss_fn, device)\n",
    "        \n",
    "        # Scheduler step 추가\n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch+1:2d} | \"\n",
    "              f\"Train Loss: {train_ret['train_loss']:.4f} | \"\n",
    "              f\"Train F1: {train_ret['train_f1']:.4f} | \"\n",
    "              f\"Val Loss: {val_ret['val_loss']:.4f} | \"\n",
    "              f\"Val F1: {val_ret['val_f1']:.4f}\")\n",
    "        \n",
    "        # 최고 성능 모델 저장\n",
    "        if val_ret['val_f1'] > best_val_f1:\n",
    "            best_val_f1 = val_ret['val_f1']\n",
    "            best_model = copy.deepcopy(model.state_dict())\n",
    "            \n",
    "            # Best 모델 분석\n",
    "            model.eval()\n",
    "            val_preds, val_targets = [], []\n",
    "            with torch.no_grad():\n",
    "                for image, targets in val_loader:\n",
    "                    preds = model(image.to(device)).argmax(dim=1)\n",
    "                    val_preds.extend(preds.cpu().numpy())\n",
    "                    val_targets.extend(targets.numpy())\n",
    "            \n",
    "            # 클래스별 정확도\n",
    "            fold_class_acc = {}\n",
    "            for c in range(17):\n",
    "                mask = np.array(val_targets) == c\n",
    "                if mask.sum() > 0:\n",
    "                    fold_class_acc[c] = (np.array(val_preds)[mask] == c).mean()\n",
    "    \n",
    "    print(f\"\\n🎯 Training hierarchical models for Fold {fold + 1}\")\n",
    "    \n",
    "    # 취약 클래스 탐지기 훈련\n",
    "    detector_state, detector_f1 = train_weak_detector_fold(\n",
    "        train_fold_df, val_fold_df, fold, device, \"./data/raw/train/\"\n",
    "    )\n",
    "\n",
    "    # 취약 클래스 전용 분류기 훈련  \n",
    "    specializer_state, specializer_f1 = train_weak_specializer_fold(\n",
    "        train_fold_df, val_fold_df, fold, device, \"./data/raw/train/\"\n",
    "    )\n",
    "    \n",
    "    # 계층적 모델 결과 저장\n",
    "    detector_models.append(detector_state)\n",
    "    specializer_models.append(specializer_state)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} - Main F1: {best_val_f1:.4f}, Detector F1: {detector_f1:.4f}, Specializer F1: {specializer_f1:.4f}\")\n",
    "\n",
    "    # 현재 fold 결과 저장 \n",
    "    fold_results.append({\n",
    "        'fold': fold + 1,\n",
    "        'best_val_f1': best_val_f1,\n",
    "        'detector_f1': detector_f1,        # 새로 추가\n",
    "        'specializer_f1': specializer_f1,  # 새로 추가\n",
    "        'train_samples': len(trn_dataset),\n",
    "        'val_samples': len(val_dataset)\n",
    "    })\n",
    "    \n",
    "    fold_models.append(best_model)\n",
    "    \n",
    "    print(f\"Fold {fold + 1} Best Validation F1: {best_val_f1:.4f}\")\n",
    "    \n",
    "    fold_class_accuracies.append(fold_class_acc) # 각 fold의 클래스별 정확도 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "K-FOLD CROSS VALIDATION RESULTS\n",
      "============================================================\n",
      "Fold 1: 0.7872\n",
      "Fold 2: 0.7179\n",
      "Fold 3: 0.8114\n",
      "Fold 4: 0.7344\n",
      "Fold 5: 0.7791\n",
      "\n",
      "Mean CV F1: 0.7660 ± 0.0346\n",
      "Best single fold: 0.8114\n"
     ]
    }
   ],
   "source": [
    "# K-Fold 결과 요약\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"K-FOLD CROSS VALIDATION RESULTS\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "val_f1_scores = [result['best_val_f1'] for result in fold_results]\n",
    "mean_f1 = np.mean(val_f1_scores)\n",
    "std_f1 = np.std(val_f1_scores)\n",
    "\n",
    "for result in fold_results:\n",
    "    print(f\"Fold {result['fold']}: {result['best_val_f1']:.4f}\")\n",
    "\n",
    "print(f\"\\nMean CV F1: {mean_f1:.4f} ± {std_f1:.4f}\")\n",
    "print(f\"Best single fold: {max(val_f1_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1500x800 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worst 3 classes:\n",
      "Class 14: 14.0%\n",
      "Class 3: 33.0%\n",
      "Class 7: 37.0%\n"
     ]
    }
   ],
   "source": [
    "# 클래스별 성능 시각화\n",
    "meta_df = pd.read_csv(\"./data/raw/meta.csv\")\n",
    "avg_acc = {c: np.mean([f.get(c,0) for f in fold_class_accuracies]) for c in range(17)}\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "classes = list(avg_acc.keys())\n",
    "accs = [avg_acc[c] * 100 for c in classes]\n",
    "names = [f\"C{c}\" for c in classes]\n",
    "\n",
    "plt.bar(range(17), accs)\n",
    "plt.xticks(range(17), names)\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Class-wise Prediction Accuracy')\n",
    "for i, acc in enumerate(accs):\n",
    "    plt.text(i, acc + 1, f'{acc:.1f}%', ha='center', fontsize=8)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Worst 3 classes:\")\n",
    "worst = sorted(avg_acc.items(), key=lambda x: x[1])[:3]\n",
    "for c, acc in worst:\n",
    "    print(f\"Class {c}: {acc*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 저장 - 현재 상태 그대로 저장\n",
    "def save_models():\n",
    "    \"\"\"학습한 모델들을 저장\"\"\"\n",
    "    \n",
    "    # 저장 디렉토리 생성\n",
    "    save_dir = \"models\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    print(\"🚨 모델 저장 시작...\")\n",
    "    \n",
    "    # 각 fold별 모델 저장 (fold_models 리스트가 있다고 가정)\n",
    "    try:\n",
    "        for fold in range(5):  # 5-fold라고 가정\n",
    "            model_path = f\"{save_dir}/fold_{fold}_model_{timestamp}.pth\"\n",
    "            \n",
    "            # fold_models[fold]가 존재한다면 저장\n",
    "            if 'fold_models' in globals() and len(fold_models) > fold:\n",
    "                torch.save({\n",
    "                    'model_state_dict': fold_models[fold].state_dict(),\n",
    "                    'fold': fold,\n",
    "                    'timestamp': timestamp,\n",
    "                    'epoch': 'unknown',  # 에포크 정보 모르면 unknown\n",
    "                }, model_path)\n",
    "                print(f\"✅ Fold {fold} 모델 저장 완료: {model_path}\")\n",
    "            \n",
    "            # 또는 best_models 리스트가 있다면\n",
    "            elif 'best_models' in globals() and len(best_models) > fold:\n",
    "                torch.save({\n",
    "                    'model_state_dict': best_models[fold].state_dict(),\n",
    "                    'fold': fold,\n",
    "                    'timestamp': timestamp,\n",
    "                    'epoch': 'unknown',\n",
    "                }, model_path)\n",
    "                print(f\"✅ Fold {fold} best 모델 저장 완료: {model_path}\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Fold별 저장 실패: {e}\")\n",
    "    \n",
    "    # 전체 변수 상태 저장 (혹시 모르니까)\n",
    "    try:\n",
    "        state_path = f\"{save_dir}/full_state_{timestamp}.pth\"\n",
    "        \n",
    "        # 현재 글로벌 변수에서 모델 관련 객체들 찾아서 저장\n",
    "        save_dict = {}\n",
    "        \n",
    "        # 가능한 모델 변수명들 체크\n",
    "        possible_model_vars = ['model', 'models', 'fold_models', 'best_models', \n",
    "                              'tta_models', 'ensemble_models']\n",
    "        \n",
    "        for var_name in possible_model_vars:\n",
    "            if var_name in globals():\n",
    "                save_dict[var_name] = globals()[var_name]\n",
    "                print(f\"✅ {var_name} 변수 포함됨\")\n",
    "        \n",
    "        if save_dict:\n",
    "            torch.save(save_dict, state_path)\n",
    "            print(f\"✅ 전체 상태 저장 완료: {state_path}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 전체 상태 저장 실패: {e}\")\n",
    "    \n",
    "    print(f\"🎉 저장 완료! 저장 위치: {save_dir}/\")\n",
    "    print(f\"📁 파일 목록:\")\n",
    "    for file in os.listdir(save_dir):\n",
    "        print(f\"   - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚨 모델 저장 시작...\n",
      "❌ Fold별 저장 실패: 'collections.OrderedDict' object has no attribute 'state_dict'\n",
      "✅ model 변수 포함됨\n",
      "✅ fold_models 변수 포함됨\n",
      "✅ 전체 상태 저장 완료: models/full_state_20250912_035640.pth\n",
      "🎉 저장 완료! 저장 위치: models/\n",
      "📁 파일 목록:\n",
      "   - fold_2_best.pth\n",
      "   - full_state_20250912_034648.pth\n",
      "   - full_state_20250912_035640.pth\n",
      "   - fold_1_best.pth\n"
     ]
    }
   ],
   "source": [
    "save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using ensemble of all 5 fold models for inference\n"
     ]
    }
   ],
   "source": [
    "# 5-Fold 앙상블 모델 준비\n",
    "ensemble_models = []\n",
    "for i, state_dict in enumerate(fold_models):\n",
    "    fold_model = timm.create_model(model_name, pretrained=True, num_classes=17).to(device)\n",
    "    fold_model.load_state_dict(state_dict)\n",
    "    fold_model.eval()\n",
    "    ensemble_models.append(fold_model)\n",
    "print(f\"Using ensemble of all {len(ensemble_models)} fold models for inference\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "Nmm5h3J-pXNV"
   },
   "source": [
    "## 5. Train Model\n",
    "* 모델을 로드하고, 학습을 진행합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lkwxRXoBpbaX"
   },
   "source": [
    "# 6. Inference & Save File\n",
    "* 테스트 이미지에 대한 추론을 진행하고, 결과 파일을 저장합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Scaling 클래스 정의\n",
    "class TemperatureScaling(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
    "    \n",
    "    def forward(self, logits):\n",
    "        return logits / self.temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "essential_tta_transforms = [\n",
    "    # 원본\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 90도 회전들\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[90, 90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[180, 180], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.Rotate(limit=[-90, -90], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "    # 밝기 개선\n",
    "    A.Compose([\n",
    "        A.LongestMaxSize(max_size=img_size),\n",
    "        A.PadIfNeeded(min_height=img_size, min_width=img_size, border_mode=0, value=0),\n",
    "        A.RandomBrightnessContrast(brightness_limit=[0.3, 0.3], contrast_limit=[0.3, 0.3], p=1.0),\n",
    "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "        ToTensorV2(),\n",
    "    ]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TTA 추론을 위한 Dataset 클래스\n",
    "class TTAImageDataset(Dataset):\n",
    "    def __init__(self, data, path, transforms):\n",
    "        if isinstance(data, str):\n",
    "            self.df = pd.read_csv(data).values\n",
    "        else:\n",
    "            self.df = data.values\n",
    "        self.path = path\n",
    "        self.transforms = transforms  # 여러 transform을 리스트로 받음\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        name, target = self.df[idx]\n",
    "        img = np.array(Image.open(os.path.join(self.path, name)).convert('RGB'))\n",
    "        \n",
    "        # 모든 transform을 적용한 결과를 리스트로 반환\n",
    "        augmented_images = []\n",
    "        for transform in self.transforms:\n",
    "            aug_img = transform(image=img)['image']\n",
    "            augmented_images.append(aug_img)\n",
    "        \n",
    "        return augmented_images, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TTA Dataset size: 3140\n"
     ]
    }
   ],
   "source": [
    "# TTA Dataset 생성\n",
    "tta_dataset = TTAImageDataset(\n",
    "    \"./data/raw/sample_submission.csv\",\n",
    "    \"./data/raw/test/\",\n",
    "    essential_tta_transforms\n",
    ")\n",
    "\n",
    "# TTA DataLoader (배치 크기를 줄여서 메모리 절약)\n",
    "tta_loader = DataLoader(\n",
    "    tta_dataset,\n",
    "    batch_size=64,  # TTA는 메모리를 많이 사용하므로 배치 크기 줄임\n",
    "    shuffle=False,\n",
    "    num_workers=num_workers,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"TTA Dataset size: {len(tta_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_tta_inference(models, loader, transforms, confidence_threshold=0.9):\n",
    "    \"\"\"5-Fold 모델 앙상블 + TTA 추론\"\"\"\n",
    "    all_predictions = []\n",
    "    \n",
    "    for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=\"Ensemble TTA\")):\n",
    "        batch_size = images_list[0].size(0)\n",
    "        ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "        \n",
    "        # 각 fold 모델별 예측\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                # 각 TTA 변형별 예측\n",
    "                for images in images_list:\n",
    "                    images = images.to(device)\n",
    "                    preds = model(images)\n",
    "                    probs = torch.softmax(preds, dim=1)\n",
    "                    ensemble_probs += probs / (len(models) * len(images_list))\n",
    "        \n",
    "        final_preds = torch.argmax(ensemble_probs, dim=1)\n",
    "        all_predictions.extend(final_preds.cpu().numpy())\n",
    "    \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 계층적 TTA 추론 함수\n",
    "def hierarchical_tta_inference(main_models, detector_models, specializer_models, loader, device):\n",
    "    \"\"\"계층적 모델들을 사용한 TTA 추론\"\"\"\n",
    "    print(\"🎯 Starting Hierarchical TTA Inference...\")\n",
    "    \n",
    "    n_folds = len(main_models)\n",
    "    fold_predictions = []\n",
    "    total_stats = {\n",
    "        'total_samples': 0,\n",
    "        'total_detection_needed': 0,\n",
    "        'total_specializer_used': 0\n",
    "    }\n",
    "    \n",
    "    for fold in range(n_folds):\n",
    "        print(f\"Processing Hierarchical Fold {fold + 1}/{n_folds}...\")\n",
    "        \n",
    "        # 계층적 분류기 구성\n",
    "        hierarchical_model = HierarchicalDocumentClassifier(main_model_name=model_name)\n",
    "        \n",
    "        # 메인 모델 로드 (기존 fold_models의 state_dict 사용)\n",
    "        hierarchical_model.main_classifier.load_state_dict(main_models[fold])\n",
    "        \n",
    "        # 탐지기와 전용 분류기 로드\n",
    "        if detector_models[fold] is not None:\n",
    "            try:\n",
    "                hierarchical_model.weak_detector.load_state_dict(detector_models[fold])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load detector for fold {fold}: {e}\")\n",
    "                # 백업 방법: 새 모델 생성해서 로드\n",
    "                temp_detector = WeakClassDetector().to(device)\n",
    "                temp_detector.load_state_dict(detector_models[fold])\n",
    "                hierarchical_model.weak_detector = temp_detector\n",
    "                \n",
    "        if specializer_models[fold] is not None:\n",
    "            try:\n",
    "                hierarchical_model.weak_specializer.load_state_dict(specializer_models[fold])\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Failed to load specializer for fold {fold}: {e}\")\n",
    "                # 백업 방법: 새 모델 생성해서 로드\n",
    "                temp_specializer = WeakClassSpecializer().to(device)\n",
    "                temp_specializer.load_state_dict(specializer_models[fold])\n",
    "                hierarchical_model.weak_specializer = temp_specializer\n",
    "            \n",
    "        hierarchical_model.eval()\n",
    "        hierarchical_model.to(device)\n",
    "        \n",
    "        fold_preds = []\n",
    "        fold_stats = {'detection_needed': 0, 'total': 0}\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch_idx, (images_list, _) in enumerate(tqdm(loader, desc=f\"Hierarchical Fold {fold+1}\")):\n",
    "                batch_size = images_list[0].size(0)\n",
    "                batch_ensemble_probs = torch.zeros(batch_size, 17).to(device)\n",
    "                \n",
    "                # 각 TTA 변형에 대해 계층적 추론\n",
    "                for tta_idx, images in enumerate(images_list):\n",
    "                    images = images.to(device)\n",
    "                    \n",
    "                    # 조건부 계층적 추론 사용\n",
    "                    probs = hierarchical_model(images, mode='inference')\n",
    "                    batch_ensemble_probs += probs / len(images_list)\n",
    "                    \n",
    "                    # 통계 수집 (첫 번째 TTA에서만)\n",
    "                    if tta_idx == 0:  # 첫 번째 TTA 변형에서만 통계 수집\n",
    "                        stats = hierarchical_model.get_inference_stats(images)\n",
    "                        fold_stats['detection_needed'] += stats['detection_needed']\n",
    "                        fold_stats['total'] += stats['total_samples']\n",
    "                \n",
    "                final_preds = torch.argmax(batch_ensemble_probs, dim=1)\n",
    "                fold_preds.extend(final_preds.cpu().numpy())\n",
    "        \n",
    "        # 폴드 통계 출력- ZeroDivisionError 방지\n",
    "        if fold_stats['total'] > 0:\n",
    "            detection_ratio = fold_stats['detection_needed'] / fold_stats['total']\n",
    "            print(f\"  Fold {fold+1} - Detection needed: {fold_stats['detection_needed']}/{fold_stats['total']} ({detection_ratio:.1%})\")\n",
    "        else:\n",
    "            print(f\"  Fold {fold+1} - No samples processed (possible issue with stats collection)\")\n",
    "        \n",
    "        total_stats['total_samples'] += fold_stats['total']\n",
    "        total_stats['total_detection_needed'] += fold_stats['detection_needed']\n",
    "        \n",
    "        fold_predictions.append(fold_preds)\n",
    "    \n",
    "    # 전체 통계 출력\n",
    "    overall_detection_ratio = total_stats['total_detection_needed'] / total_stats['total_samples']\n",
    "    print(f\"\\n📊 Overall Detection Statistics:\")\n",
    "    print(f\"  Total samples: {total_stats['total_samples']}\")\n",
    "    print(f\"  Detection needed: {total_stats['total_detection_needed']} ({overall_detection_ratio:.1%})\")\n",
    "    print(f\"  Efficiency gain: {1-overall_detection_ratio:.1%} samples skipped detection\")\n",
    "    \n",
    "    # Fold 앙상블 (다수결)\n",
    "    final_predictions = []\n",
    "    for i in range(len(fold_predictions[0])):\n",
    "        votes = [fold_preds[i] for fold_preds in fold_predictions]\n",
    "        final_pred = Counter(votes).most_common(1)[0][0]\n",
    "        final_predictions.append(final_pred)\n",
    "    \n",
    "    return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Original Ensemble TTA inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ensemble TTA:   0%|          | 0/50 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# 앙상블 TTA 실행 (기존 방법)\n",
    "print(\"Starting Original Ensemble TTA inference...\")\n",
    "tta_predictions = ensemble_tta_inference(\n",
    "    models=ensemble_models,\n",
    "    loader=tta_loader, \n",
    "    transforms=essential_tta_transforms,\n",
    "    confidence_threshold=0.9\n",
    ")\n",
    "\n",
    "# 계층적 앙상블 TTA 추가 실행\n",
    "print(\"\\nStarting Hierarchical Ensemble TTA inference...\")\n",
    "\n",
    "# 기존 모델들의 state_dict 추출\n",
    "main_model_states = [model.state_dict() for model in ensemble_models]\n",
    "\n",
    "hierarchical_predictions = hierarchical_tta_inference(\n",
    "    main_models=main_model_states,\n",
    "    detector_models=detector_models,\n",
    "    specializer_models=specializer_models,\n",
    "    loader=tta_loader,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과 비교 분석\n",
    "print(f\"\\n📊 Prediction Comparison:\")\n",
    "print(f\"Original unique predictions: {len(set(tta_predictions))}\")\n",
    "print(f\"Hierarchical unique predictions: {len(set(hierarchical_predictions))}\")\n",
    "\n",
    "# 예측 일치도 분석\n",
    "agreement_count = sum(1 for i, j in zip(tta_predictions, hierarchical_predictions) if i == j)\n",
    "total_count = len(tta_predictions)\n",
    "agreement_ratio = agreement_count / total_count\n",
    "\n",
    "print(f\"Prediction agreement: {agreement_count}/{total_count} ({agreement_ratio:.1%})\")\n",
    "\n",
    "# 취약 클래스 예측 분석\n",
    "weak_classes = [3, 4, 7, 14]\n",
    "original_weak_count = sum(1 for p in tta_predictions if p in weak_classes)\n",
    "hierarchical_weak_count = sum(1 for p in hierarchical_predictions if p in weak_classes)\n",
    "\n",
    "print(f\"Original weak class predictions: {original_weak_count}\")\n",
    "print(f\"Hierarchical weak class predictions: {hierarchical_weak_count}\")\n",
    "\n",
    "# 차이점 분석\n",
    "disagreement_indices = [i for i, (orig, hier) in enumerate(zip(tta_predictions, hierarchical_predictions)) if orig != hier]\n",
    "print(f\"Disagreement count: {len(disagreement_indices)} samples\")\n",
    "\n",
    "if len(disagreement_indices) > 0:\n",
    "    print(\"Disagreement examples (first 10):\")\n",
    "    for i in disagreement_indices[:10]:\n",
    "        print(f\"  Sample {i}: Original={tta_predictions[i]}, Hierarchical={hierarchical_predictions[i]}\")\n",
    "\n",
    "# 원본 TTA 결과로 submission 파일 생성\n",
    "tta_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "tta_pred_df['target'] = tta_predictions\n",
    "\n",
    "# 계층적 TTA 결과로 submission 파일 생성\n",
    "hierarchical_pred_df = pd.DataFrame(tta_dataset.df, columns=['ID', 'target'])\n",
    "hierarchical_pred_df['target'] = hierarchical_predictions\n",
    "\n",
    "# 기존 submission과 동일한 순서인지 확인\n",
    "sample_submission_df = pd.read_csv(\"./data/raw/sample_submission.csv\")\n",
    "assert (sample_submission_df['ID'] == tta_pred_df['ID']).all()\n",
    "assert (sample_submission_df['ID'] == hierarchical_pred_df['ID']).all()\n",
    "\n",
    "# 두 결과 모두 저장\n",
    "tta_pred_df.to_csv(\"./notebooks/team/IYS/submissions/original_choice.csv\", index=False)\n",
    "logger.save_dataframe(tta_pred_df, 'original_choice', '기존 방법')\n",
    "hierarchical_pred_df.to_csv(\"./notebooks/team/IYS/submissions/hierarchical_choice.csv\", index=False)\n",
    "logger.save_dataframe(hierarchical_pred_df, 'hierarchical_choice', '계층적 방법')\n",
    "\n",
    "print(\"✅ Both submissions saved:\")\n",
    "print(\"   - original_choice.csv (기존 방법)\")\n",
    "print(\"   - hierarchical_choice.csv (계층적 방법)\")\n",
    "\n",
    "# 최종 분석\n",
    "print(f\"\\n📊 Final Analysis:\")\n",
    "print(f\"Total test samples: {len(hierarchical_predictions)}\")\n",
    "\n",
    "# 클래스별 예측 분포 비교\n",
    "original_dist = Counter(tta_predictions)\n",
    "hierarchical_dist = Counter(hierarchical_predictions)\n",
    "\n",
    "print(f\"\\n📋 클래스별 예측 분포 비교:\")\n",
    "for cls in sorted(set(list(original_dist.keys()) + list(hierarchical_dist.keys()))):\n",
    "    orig_count = original_dist.get(cls, 0)\n",
    "    hier_count = hierarchical_dist.get(cls, 0)\n",
    "    diff = hier_count - orig_count\n",
    "    marker = \"🎯\" if cls in [3, 4, 7, 14] else \"📋\"\n",
    "    print(f\"  {marker} Class {cls}: Original={orig_count}, Hierarchical={hier_count}, Diff={diff:+d}\")\n",
    "\n",
    "print(\"\\nHierarchical TTA Prediction sample:\")\n",
    "print(hierarchical_pred_df.head())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "cv_py3_11_9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
