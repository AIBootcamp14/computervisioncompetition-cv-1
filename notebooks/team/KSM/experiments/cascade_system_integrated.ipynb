{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f46cb92",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "\n",
    "# 방법 1: 가중 평균을 통한 모델 파라미터 병합\n",
    "class ModelAveraging:\n",
    "    def __init__(self, model_class):\n",
    "        self.model_class = model_class\n",
    "    \n",
    "    def average_models(self, model_paths, weights=None):\n",
    "        \"\"\"\n",
    "        여러 모델의 파라미터를 가중평균으로 병합\n",
    "        \"\"\"\n",
    "        if weights is None:\n",
    "            weights = [1.0 / len(model_paths)] * len(model_paths)\n",
    "        \n",
    "        # 첫 번째 모델 로드\n",
    "        averaged_model = self.model_class()\n",
    "        first_state_dict = torch.load(model_paths[0], map_location='cpu')\n",
    "        averaged_state_dict = OrderedDict()\n",
    "        \n",
    "        # 모든 파라미터 초기화\n",
    "        for key in first_state_dict.keys():\n",
    "            averaged_state_dict[key] = torch.zeros_like(first_state_dict[key])\n",
    "        \n",
    "        # 가중평균 계산\n",
    "        for i, model_path in enumerate(model_paths):\n",
    "            state_dict = torch.load(model_path, map_location='cpu')\n",
    "            for key in state_dict.keys():\n",
    "                averaged_state_dict[key] += weights[i] * state_dict[key]\n",
    "        \n",
    "        averaged_model.load_state_dict(averaged_state_dict)\n",
    "        return averaged_model\n",
    "\n",
    "# 방법 2: 계층적 앙상블 모델 (추천)\n",
    "class HierarchicalEnsemble(nn.Module):\n",
    "    def __init__(self, model_a_class, model_b_class, num_classes=17, vulnerable_classes=[3, 4, 7, 14]):\n",
    "        super(HierarchicalEnsemble, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.vulnerable_classes = vulnerable_classes\n",
    "        \n",
    "        # 분류기 A 앙상블 (전체 17개 클래스)\n",
    "        self.models_a = nn.ModuleList([model_a_class() for _ in range(5)])\n",
    "        \n",
    "        # 분류기 B 앙상블 (취약클래스 4개)\n",
    "        self.models_b = nn.ModuleList([model_b_class() for _ in range(5)])\n",
    "        \n",
    "        # 최종 융합을 위한 가중치\n",
    "        self.alpha = nn.Parameter(torch.tensor(0.7))  # A의 가중치\n",
    "        self.beta = nn.Parameter(torch.tensor(0.3))   # B의 가중치\n",
    "        \n",
    "    def load_pretrained_models(self, model_a_paths, model_b_paths):\n",
    "        \"\"\"사전 학습된 모델들 로드\"\"\"\n",
    "        for i, path in enumerate(model_a_paths):\n",
    "            self.models_a[i].load_state_dict(torch.load(path, map_location='cpu'))\n",
    "            self.models_a[i].eval()\n",
    "            \n",
    "        for i, path in enumerate(model_b_paths):\n",
    "            self.models_b[i].load_state_dict(torch.load(path, map_location='cpu'))\n",
    "            self.models_b[i].eval()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # 분류기 A 앙상블 예측\n",
    "        preds_a = []\n",
    "        for model in self.models_a:\n",
    "            pred = F.softmax(model(x), dim=1)\n",
    "            preds_a.append(pred)\n",
    "        ensemble_a = torch.mean(torch.stack(preds_a), dim=0)\n",
    "        \n",
    "        # 분류기 B 앙상블 예측\n",
    "        preds_b = []\n",
    "        for model in self.models_b:\n",
    "            pred = F.softmax(model(x), dim=1)\n",
    "            preds_b.append(pred)\n",
    "        ensemble_b = torch.mean(torch.stack(preds_b), dim=0)\n",
    "        \n",
    "        # 최종 예측 결합\n",
    "        final_pred = torch.zeros_like(ensemble_a)\n",
    "        \n",
    "        # 모든 클래스에 대해 분류기 A의 예측 사용\n",
    "        final_pred = self.alpha * ensemble_a\n",
    "        \n",
    "        # 취약클래스에 대해서는 분류기 B의 예측을 추가로 반영\n",
    "        for i, cls_idx in enumerate(self.vulnerable_classes):\n",
    "            final_pred[:, cls_idx] += self.beta * ensemble_b[:, i]\n",
    "        \n",
    "        return final_pred\n",
    "\n",
    "# 방법 3: 지식 증류를 통한 단일 모델 생성\n",
    "class KnowledgeDistillation:\n",
    "    def __init__(self, student_model, teacher_models_a, teacher_models_b, \n",
    "                 vulnerable_classes=[3, 4, 7, 14], temperature=3.0):\n",
    "        self.student = student_model\n",
    "        self.teachers_a = teacher_models_a\n",
    "        self.teachers_b = teacher_models_b\n",
    "        self.vulnerable_classes = vulnerable_classes\n",
    "        self.temperature = temperature\n",
    "        \n",
    "        # 교사 모델들을 평가 모드로 설정\n",
    "        for teacher in self.teachers_a + self.teachers_b:\n",
    "            teacher.eval()\n",
    "    \n",
    "    def get_teacher_predictions(self, x):\n",
    "        \"\"\"모든 교사 모델의 예측을 얻음\"\"\"\n",
    "        with torch.no_grad():\n",
    "            # 분류기 A 예측들\n",
    "            preds_a = []\n",
    "            for teacher in self.teachers_a:\n",
    "                pred = F.softmax(teacher(x) / self.temperature, dim=1)\n",
    "                preds_a.append(pred)\n",
    "            ensemble_a = torch.mean(torch.stack(preds_a), dim=0)\n",
    "            \n",
    "            # 분류기 B 예측들\n",
    "            preds_b = []\n",
    "            for teacher in self.teachers_b:\n",
    "                pred = F.softmax(teacher(x) / self.temperature, dim=1)\n",
    "                preds_b.append(pred)\n",
    "            ensemble_b = torch.mean(torch.stack(preds_b), dim=0)\n",
    "            \n",
    "        return ensemble_a, ensemble_b\n",
    "    \n",
    "    def distillation_loss(self, student_logits, teacher_preds_a, teacher_preds_b, \n",
    "                         true_labels=None, alpha=0.7, beta=0.3):\n",
    "        \"\"\"증류 손실 계산\"\"\"\n",
    "        student_soft = F.softmax(student_logits / self.temperature, dim=1)\n",
    "        \n",
    "        # KL divergence 손실\n",
    "        kl_loss_a = F.kl_div(F.log_softmax(student_logits / self.temperature, dim=1),\n",
    "                            teacher_preds_a, reduction='batchmean')\n",
    "        \n",
    "        # 취약클래스에 대한 추가 증류\n",
    "        vulnerable_mask = torch.zeros_like(student_soft)\n",
    "        for cls_idx in self.vulnerable_classes:\n",
    "            vulnerable_mask[:, cls_idx] = 1.0\n",
    "        \n",
    "        # 취약클래스 KL loss (분류기 B의 해당 인덱스와 매칭)\n",
    "        kl_loss_b = 0\n",
    "        for i, cls_idx in enumerate(self.vulnerable_classes):\n",
    "            if i < teacher_preds_b.size(1):\n",
    "                target_prob = teacher_preds_b[:, i].unsqueeze(1)\n",
    "                student_prob = student_soft[:, cls_idx].unsqueeze(1)\n",
    "                kl_loss_b += F.kl_div(torch.log(student_prob + 1e-8), \n",
    "                                    target_prob, reduction='batchmean')\n",
    "        \n",
    "        total_loss = alpha * kl_loss_a + beta * kl_loss_b\n",
    "        \n",
    "        # 실제 라벨이 있다면 하드 타겟 손실도 추가\n",
    "        if true_labels is not None:\n",
    "            hard_loss = F.cross_entropy(student_logits, true_labels)\n",
    "            total_loss += 0.1 * hard_loss\n",
    "        \n",
    "        return total_loss\n",
    "\n",
    "# 실제 사용 예시\n",
    "def integrate_models():\n",
    "    \"\"\"모델 통합 실행 예시\"\"\"\n",
    "    \n",
    "    # 모델 경로들\n",
    "    model_a_paths = [f'model_a_fold_{i}.pt' for i in range(5)]\n",
    "    model_b_paths = [f'model_b_fold_{i}.pt' for i in range(5)]\n",
    "    \n",
    "    # 방법 1: 단순 가중평균 (분류기 A만)\n",
    "    print(\"방법 1: 가중평균 모델 생성\")\n",
    "    averager = ModelAveraging(YourModelClass)  # YourModelClass를 실제 모델 클래스로 교체\n",
    "    averaged_model_a = averager.average_models(model_a_paths)\n",
    "    torch.save(averaged_model_a.state_dict(), 'averaged_model_a.pt')\n",
    "    \n",
    "    # 방법 2: 계층적 앙상블 (추천)\n",
    "    print(\"방법 2: 계층적 앙상블 모델 생성\")\n",
    "    ensemble_model = HierarchicalEnsemble(YourModelClassA, YourModelClassB)\n",
    "    ensemble_model.load_pretrained_models(model_a_paths, model_b_paths)\n",
    "    torch.save(ensemble_model.state_dict(), 'hierarchical_ensemble.pt')\n",
    "    \n",
    "    # 방법 3: 지식 증류로 단일 모델 생성\n",
    "    print(\"방법 3: 지식 증류 모델 학습 준비\")\n",
    "    # 이 경우 추가 학습 데이터와 학습 루프가 필요\n",
    "    \n",
    "    return ensemble_model\n",
    "\n",
    "# 통합 모델 사용 예시\n",
    "def use_integrated_model():\n",
    "    \"\"\"통합된 모델 사용 예시\"\"\"\n",
    "    # 저장된 모델 로드\n",
    "    model = HierarchicalEnsemble(YourModelClassA, YourModelClassB)\n",
    "    model.load_state_dict(torch.load('hierarchical_ensemble.pt'))\n",
    "    model.eval()\n",
    "    \n",
    "    # 예측\n",
    "    with torch.no_grad():\n",
    "        dummy_input = torch.randn(1, 3, 224, 224)  # 예시 입력\n",
    "        output = model(dummy_input)\n",
    "        predicted_class = torch.argmax(output, dim=1)\n",
    "        \n",
    "    return predicted_class\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 실행\n",
    "    integrated_model = integrate_models()\n",
    "    print(\"모델 통합 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69896733",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 사용 예시\n",
    "model_a_paths = ['model_a_fold_0.pt', 'model_a_fold_1.pt', ...]\n",
    "model_b_paths = ['model_b_fold_0.pt', 'model_b_fold_1.pt', ...]\n",
    "\n",
    "ensemble = HierarchicalEnsemble(YourModelA, YourModelB)\n",
    "ensemble.load_pretrained_models(model_a_paths, model_b_paths)\n",
    "\n",
    "# 단일 파일로 저장\n",
    "torch.save(ensemble.state_dict(), 'final_integrated_model.pt')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
